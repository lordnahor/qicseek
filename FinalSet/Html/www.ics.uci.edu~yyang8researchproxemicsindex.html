<html>
<head>
<title>Proxemics Recognition in Personal Photos - UC Irvine</title>
<style>
body
{
	font-family : Arial;
}
#container
{
	width : 900px;
	margin : 20px auto;
	background-color : #fff;
	padding : 20px;
}
h1 strong
{
	font-size : 40px;
}
h1
{
	margin : 0;
	padding : 0;
	font-size : 30px;
}
code
{
	border : 1px solid #ccc;
	display : block;
	padding : 5px;
	margin : 10px;
}
</style>
<!--<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-17813713-3']);
  _gaq.push(['_setDomainName', '.mit.edu']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>-->
</head>


<body>
<div id="container">

<h1 style="text-align:center;"><strong>P</strong>roxemics <strong>R</strong>ecognition in <strong>P</strong>ersonal <strong>P</strong>hotos</h1>

<p style="text-align:center;"><a href="http://www.ics.uci.edu/~yyang8">Yi Yang</a>, <a href="http://research.microsoft.com/en-us/people/sbaker">Simon Baker</a>, <a href="http://research.microsoft.com/en-us/people/ankannan">Anitha Kannan</a> and <a href="http://www.ics.uci.edu/~dramanan">Deva Ramanan</a></p>

<p style="text-align:center;">Under Construction<p>

<!--<blockquote style="text-align:center;">"These are some of the best HITS I've ever done!" <cite>&mdash; Mechanical Turk worker</cite></blockquote>-->

<div style="margin-top : 30px;"></div>

<div style="text-align : center; margin-left : 0px; margin-right: 0px; border : 2px solid black;">
<img src="figs/models_new.jpg" style="height : 256px;">
</div>

<h2>Abstract</h2>
<p>Proxemics is the study of how people interact. We present a computational formulation of visual proxemics by attempting to label each pair of people in an image with a subset of physically based touch codes. A baseline approach would be to first perform pose estimation and then detect the touch codes based on the estimated joint locations. We found that this sequential approach does not perform well because pose estimation step is too unreliable for images of interacting people, due to difficulties with occlusion and limb ambiguities. Instead, we propose a direct approach where we build an articulated model tuned for each touch code. Each such model contains two people, connected in an appropriate manner for the touch code in question. We fit this model to the image and then base classification on the fitting error. Experiments show that this approach significantly outperforms the sequential baseline as well as other related approches.</p>

<h2>Download Data & Codes</h2>

<p>The latest copy of our code and dataset: <a href="code/proxemic-v1.0.zip">DOWNLOAD</a></p>

<p>Please read the <a href="code/README">README</a> file for proper installation.</p>

<h2>Examples</h2>

<center>
<table align="center" border="0" cellspacing="0" cellpadding="2"

<tr>
<td align="center" width="140"><img src="figs/fig4/model_new_hh1_1.jpg" width="140"></td>
<td align="center" width="140"><img src="figs/fig4/model_new_ss1_1.jpg" width="140"></td>
<td align="center" width="140"><img src="figs/fig4/model_new_hs1_1.jpg" width="140"></td>
<td align="center" width="140"><img src="figs/fig4/model_new_he1_1.jpg" width="140"></td>
<td align="center" width="140"><img src="figs/fig4/model_new_es1_1.jpg" width="140"></td>
<td align="center" width="140"><img src="figs/fig4/model_new_hbt1_1.jpg" width="140"></td>
</tr>
<tr>
<td align="center" width="140"><img src="figs/fig6/LHRH_0043_new.jpg" width="140"></td>
<td align="center" width="140"><img src="figs/fig6/LSRS_0092_new.jpg" width="140"></td>
<td align="center" width="140"><img src="figs/fig6/LHLS_0010_new.jpg" width="140"></td>
<td align="center" width="140"><img src="figs/fig6/LHRE_0013_new.jpg" width="140"></td>
<td align="center" width="140"><img src="figs/fig6/LERS_0016_new.jpg" width="140"></td>
<td align="center" width="140"><img src="figs/fig6/RHBT_0034_new.jpg" width="140"></td>
</tr>

</table>
</center>

<h2>Publications</h2>

<p>If you use our software or our data sets, please cite:</p>

<p>Yi Yang, Simon Baker, Anitha Kannan, Deva Ramanan. "Recognizing Proxemics in Personal Photos". <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Rhode Island, USA, 2012.</em> <a href="proxemic2012.pdf">[Paper]</a><a href="proxemic2012_slides.pptx">[Slides]</a><a href="proxemic2012_poster.pdf">[Poster]</a><a href="video/lecture.htm">[Talk]</a> </p>

<pre><code>@conference{yang2012recognizing,
  title={Recognizing proxemics in personal photos},
  author={Yang, Y. and Baker, S. and Kannan, A. and Ramanan, D.},
  booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition},
  year={2012},
  organization={IEEE}
}</code></pre>

<h2>Acknowledgements &amp; Funding</h2>
<p>The research described was conducted while Yi Yang was an intern at Microsoft Research. Deva Ramanan was supported by NSF Grant 0954083 and ONR-MURI Grant N00014-10-1-0933.</p>

<h2>Links</h2>
<ul>
<li><a href="http://www.cs.brown.edu/~pff/latent/">Object Detection with Discriminatively Trained Part Based Models</a></li>
<li><a href="http://www.ics.uci.edu/~yyang8/projects/pose/">Articulated Pose Estimation with Flexible Mixtures of Parts</a></li>
</ul>

<h2>Update History</h2>
<center>
<table style="align:center; border:1px solid black; border-spacing:0px; width:100%">
<tr>
<th style="border:1px solid black; width:20%"><strong>Date</strong></th>
<th style="border:1px solid black; width:80%"><strong>Description</strong></th>
</tr>
<tr>
<td style="border:1px solid black; width:20%">7/21/2012</td>
<td style="border:1px solid black; width:80%">Hello World</td>
</tr>
</table>
</center>

<h2>License</h2>
<p>Copyright &copy; 2012 Yi Yang, Simon Baker, Anitha Kannan and Deva Ramanan</p>
<p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p>
<ul>
<li>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</li>
</ul>
<p>THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONtrACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>

</div>

</body>
</html>


