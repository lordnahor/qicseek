<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html>
<head>
    <meta name="description" content="Web Crawling Introduction to Information Retrieval CS 221 Donald J. Patterson Content adapted from Hinrich Schütze http://www.informationretrieval.org  Introduction URL Frontier Robust Crawling DNS Overview Web Crawling Outline  Introduction  The basic crawl algorithm Initialize a queue of URLs (“seed” URLs) Repeat Remove a URL from the queue Fetch associated page Parse and analyze page Store representation of page Extract URLs from page and add to queue Introduction  Crawling the web Introduction  Basic Algorithm is not reality... Real web crawling requires multiple machines All steps distributed on different computers Even Non-Adversarial pages pose problems Latency and bandwidth to remote servers vary Webmasters have opinions about crawling their turf How “deep” in a URL should you go? Site mirrors and duplicate pages Politeness Don’t hit a server too often Introduction  Basic Algorithm is not reality... Adversarial Web Pages Spam Pages Spider Traps Introduction  Minimum Characteristics for a Web Crawler Be Polite: Respect implicit and explicit terms on website Crawl pages you’re allowed to Respect “robots.txt” (more on this coming up) Be Robust Handle traps and spam gracefully Introduction  Desired Characteristics for a Web Crawler Be a distributed systems Run on multiple machines Be scalable Adding more machines allows you to crawl faster Be Efficient Fully utilize available processing and bandwidth Focus on “Quality” Pages Crawl good information first Introduction  Desired Characteristics for a Web Crawler Support Continuous Operation Fetch fresh copies of previously crawled pages Be Extensible Be able to adapt to new data formats, protocols, etc. Today it’s AJAX, tomorrow it’s SilverLight, then.... Introduction  Updated Crawling picture URL Frontier  Frontier Queue might have multiple pages from the same host These need to be load balanced (“politeness”) All crawl threads should be kept busy URL Frontier  Politeness? It is easy enough for a website to block a crawler Explicit Politeness “Robots Exclusion Standard” Defined by a “robots.txt” file maintained by a webmaster What portions of the site can be crawled. Irrelevant, private or other data excluded. Voluntary compliance by crawlers. Based on regular expression matching URL Frontier  Politeness? Explicit Politeness “Sitemaps” Introduced by Google, but open standard XML based Allows webmasters to give hints to web crawlers: Location of pages (URL islands) Relative importance of pages Update frequency of pages Sitemap location listed in robots.txt URL Frontier  Politeness? Implicit Politeness Even without specification avoid hitting any site too often It costs bandwidth and computing resources for host. URL Frontier  Politeness? URL Frontier  Politeness? URL Frontier  Politeness? URL Frontier  Robots.txt - Exclusion URL Frontier Protocol for giving spiders (“robots”) limited access to a website Source: http://www.robotstxt.org/wc/norobots.html Website announces what is okay and not okay to crawl: Located at http://www.myurl.com/robots.txt This file holds the restrictions  Robots.txt Example URL Frontier http://www.ics.uci.edu/robots.txt  Sitemaps - Inclusion URL Frontier https://www.google.com/webmasters/tools/docs/en/protocol.html#sitemapXMLExample  Introduction URL Frontier Robust Crawling DNS Overview Web Crawling Outline  A Robust Crawl Architecture Robust Crawling"/>
    <title></title>
    <script type="text/javascript" language="javascript">
//      <![CDATA[
            var images = new Array (24);
            images[0] = "Lecture04_02_Slides_CS221_files/Lecture04_02_Slides_CS221.001.png";
            images[1] = "Lecture04_02_Slides_CS221_files/Lecture04_02_Slides_CS221.002.png";
            images[2] = "Lecture04_02_Slides_CS221_files/Lecture04_02_Slides_CS221.003.png";
            images[3] = "Lecture04_02_Slides_CS221_files/Lecture04_02_Slides_CS221.004.png";
            images[4] = "Lecture04_02_Slides_CS221_files/Lecture04_02_Slides_CS221.005.png";
            images[5] = "Lecture04_02_Slides_CS221_files/Lecture04_02_Slides_CS221.006.png";
            images[6] = "Lecture04_02_Slides_CS221_files/Lecture04_02_Slides_CS221.007.png";
            images[7] = "Lecture04_02_Slides_CS221_files/Lecture04_02_Slides_CS221.008.png";
            images[8] = "Lecture04_02_Slides_CS221_files/Lecture04_02_Slides_CS221.009.png";
            images[9] = "Lecture04_02_Slides_CS221_files/Lecture04_02_Slides_CS221.010.png";
            images[10] = "Lecture04_02_Slides_CS221_files/Lecture04_02_Slides_CS221.011.png";
            images[11] = "Lecture04_02_Slides_CS221_files/Lecture04_02_Slides_CS221.012.png";
            images[12] = "Lecture04_02_Slides_CS221_files/Lecture04_02_Slides_CS221.013.png";
            images[13] = "Lecture04_02_Slides_CS221_files/Lecture04_02_Slides_CS221.014.png";
            images[14] = "Lecture04_02_Slides_CS221_files/Lecture04_02_Slides_CS221.015.png";
            images[15] = "Lecture04_02_Slides_CS221_files/Lecture04_02_Slides_CS221.016.png";
            images[16] = "Lecture04_02_Slides_CS221_files/Lecture04_02_Slides_CS221.017.png";
            images[17] = "Lecture04_02_Slides_CS221_files/Lecture04_02_Slides_CS221.018.png";
            images[18] = "Lecture04_02_Slides_CS221_files/Lecture04_02_Slides_CS221.019.png";
            images[19] = "Lecture04_02_Slides_CS221_files/Lecture04_02_Slides_CS221.020.png";
            images[20] = "Lecture04_02_Slides_CS221_files/Lecture04_02_Slides_CS221.021.png";
            images[21] = "Lecture04_02_Slides_CS221_files/Lecture04_02_Slides_CS221.022.png";
            images[22] = "Lecture04_02_Slides_CS221_files/Lecture04_02_Slides_CS221.023.png";
            images[23] = "Lecture04_02_Slides_CS221_files/Lecture04_02_Slides_CS221.024.png";
            var index = 0;
            function WindowLoaded(evt)
            {
                document.body.onselectstart = function () { return false; };
            }
            function Step(i)
            {
                GoTo(index + i)
            }
            function GoTo(newIndex)
            {
                if(newIndex >= 0 && newIndex < images.length)
                {
                    index = newIndex;
                    document.Slideshow.src = images[index];
                }
            }
//      ]]>
    </script>
</head>
<body bgcolor="black" onload='WindowLoaded(event);'>
    <p align="center">
        <br/>
        <br/>
        <img name="Slideshow" alt="" src="Lecture04_02_Slides_CS221_files/Lecture04_02_Slides_CS221.001.png" onclick="Step(1)"/>
        <br/>
        <br/>
        <input type="image" src="Lecture04_02_Slides_CS221_files/home.png" onclick="GoTo(0)"/>
        &nbsp;&nbsp;&nbsp;
        <input type="image" src="Lecture04_02_Slides_CS221_files/prev.png" onclick="Step(-1)"/>
        <input type="image" src="Lecture04_02_Slides_CS221_files/next.png" onclick="Step(1)"/>
    </p>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-15170336-1");
pageTracker._trackPageview();
} catch(err) {}</script>

</body>
</html>

