<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<HTML>
<HEAD>
   <META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
   <META NAME="Generator" CONTENT="Microsoft Word 97">
   <META NAME="Template" CONTENT="C:\Program Files\Microsoft Office\Office\html.dot">
   <META NAME="GENERATOR" CONTENT="Mozilla/4.06 [en] (WinNT; I) [Netscape]">
   <META NAME="Author" CONTENT="default">
   <TITLE>APPLIED PROBABILITY FOR COMPUTER SCIENTISTS</TITLE>
</HEAD>
<BODY TEXT="#000000" BGCOLOR="#CCFFFF" LINK="#0000FF" VLINK="#800080" ALINK="#FF0000">

<CENTER><B><FONT SIZE=+2>PROBABILISTIC MODELS FOR TIME SERIES AND SEQUENCES</FONT></B>
<P><B><FONT SIZE=+1>ICS 280D, SPRING 2000</FONT></B>
<BR><B><FONT SIZE=+1>Time and Location: Mon/Wed, 2 to 3:20, ICF 101</FONT></B>
<BR><B><FONT COLOR="#000000"><FONT SIZE=+1>Instructor</FONT>: <A HREF="http://www.ics.uci.edu/~smyth">Professor
Padhraic Smyth</A></FONT></B>
<BR><B>email</B>: smyth@ics.uci.edu</CENTER>

<BR>&nbsp;
<H3>
<B>Project Information:</B></H3>
<BR><A HREF="http://www.ics.uci.edu/~smyth/courses/time/projects.html">Instructions for Class Projects</a>


<H3>
<B>Course Goals:</B></H3>
In this course students will learn the general principles of how to build
and analyse probabilistic models for time series and sequence data. The
course will begin with basic concepts such as stationarity, autocorrelation,
Markov properties, likelihood, and so forth. From there a variety of models
will be explored, beginning with relatively simple autoregressive models
(for real-valued time series) and Markov chains (for discrete data), working
up to more complex representations such as hidden and semi-Markov models.
Basic principles of how to construct, analyze, estimate, and evaluate such
models will be emphasized, and applications to a variety of areas such
as speech recognition, financial forecasting, protein modeling, fault detection,
etc., will be discussed where appropriate.
<BR>&nbsp;
<H3>
Text</H3>
For classic time-series models we will use <I>The Analysis of Time Series
</I>, C. Chatfield, Chapman and Hall, 1996, 5th edition. This will be supplemented
by a collection of research papers, mainly tutorial in nature, for discrete-valued
sequences and for the more advanced topics.
<H3>
Prerequisites</H3>
Ideally the student will have some familiarity with basic concepts in learning
from data (e.g., have taken ICS 273 or ICS 278 or some equivalent) as well
as having a good basic understanding of probability (e.g., have taken ICS
274 or the equivalent of an upper division or introductory graduate course
in probability and statistics). However, the course will be reasonably
self-contained so these are not strict pre-requisites. If in doubt please
come and discuss with me before signing up.
<BR>&nbsp;
<BR>&nbsp;
<H3>
Syllabus</H3>

<TABLE BORDER COLS=3 WIDTH="100%" >
<TR>
<TD><B>Week</B></TD>

<TD><B>Topic</B></TD>

<TD><B>Reading</B></TD>
</TR>

<TR>
<TD>April 3, 5</TD>

<TD><B>Introduction</B>: Types of time-series and sequences, basic concepts,
descriptive techniques, visualization, stationarity, trends, seasonality,
autocorrelation</TD>

<TD>Chapters 1 and 2 and 3.1 to 3.3 in the text</TD>
</TR>

<TR>
<TD>April 10, 17</TD>

<TD><B>Markov Chains</B>: Review of basic probability, independence and
conditional independence, definition of likelihood, Markov chains, properties
of Markov chains, classification of states, steady-state conditions.</TD>

<TD>Handout (Chapter 4 from Ross)</TD>
</TR>

<TR>
<TD>April 17, 19</TD>

<TD><B>Graphical Models</B>: Basic principles of directed graphical models,
conditional independence properties, general inference algorithms</TD>

<TD>Two chapters from Jordan and Bishop </TD>
</TR>

<TR>
<TD>April 24, 26</TD>

<TD><B>Probability Models for Time Series</B>: linear autoregressive models,
conditional independence properties, stability, forecasting and applications</TD>

<TD>Chapter 3 in Text</TD>
</TR>

<TR>
<TD>May 1, 3</TD>

<TD><B>Learning and Parameter Estimation: </B>maximum likelihood, parameter
estimation, examples for Markov chains and linear models, Bayesian methods</TD>

<TD>Chapter 4 and 5 in text, Handout</TD>
</TR>

<TR>
<TD>May 8, 10</TD>

<TD><B>Hidden Variable Markov Models</B>: graphical models with hidden
variables, hidden Markov models, Kalman filters, inference and estimation,
outline of EM algorithm. Applications in speech recognition, protein modeling,
and fault detection.</TD>

<TD>Chapter 10 in text, Two chapters from Jordan and Bishop</TD>
</TR>

<TR>
<TD>May 15, 17</TD>

<TD><B>Clustering Sequences</B>: probabilistic model-based approaches,
more on EM, applications to clustering Web data.</TD>

<TD>Cadez et al paper on Web clustering</TD>
</TR>

<TR>
<TD>May 22, 24</TD>

<TD><B>Semi-Markov and Segmental Markov models</B>: Semi-Markov processes,
segmental Markov processes, applications to pattern recognition and change
detection.</TD>

<TD>Application paper by Ge and Smyth
&nbsp;</TD>
</TR>

<TR>
<TD>May 31</TD>

<TD><B>Event Data Models</B>: Poisson and related models, modeling bursts,
different models for event characterization</TD>

<TD>Chapter from text by Ross</TD>
</TR>

<TR>
<TD>June 5, 7</TD>

<TD><B>Advanced Topics</B></TD>

<TD>TBD</TD>
</TR>

<TR>
<TD></TD>

<TD></TD>

<TD></TD>
</TR>
</TABLE>

<H3>
<B>Grading</B></H3>

<LI>
TBD</LI>

<BR>&nbsp;
<BR>&nbsp;
<H3>
Office Hours and Discussion</H3>
Professor Smyth will have office hours every Monday, 3:30 to 5pm, CS
414E.
<BR>&nbsp;
</BODY>
</HTML>

