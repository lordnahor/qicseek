<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<title>Mingming's Research</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Style-Type" content="text/css" />
<link href="style.css" rel="stylesheet" type="text/css" />
<script src="js/rollover.js" type="text/javascript"></script>
<script src="SpryAssets/SpryMenuBar.js" type="text/javascript"></script>
<!--[if lt IE 7]>
	<link href="ie_style.css" rel="stylesheet" type="text/css" />
	<script type="text/javascript" src="js/ie_png.js"></script>
	<script type="text/javascript">
		 ie_png.fix('.png, .box .top, .box .bottom, .box .inner');
	</script>
<![endif]-->
<style type="text/css">
<!--
-->
		h3 {margin-bottom: 0px; margin-top: 0px;}
		p {margin-top: 5px;}
		td {padding: 0px 0px 25px; }
</style>

<link href="SpryAssets/SpryMenuBarHorizontal.css" rel="stylesheet" type="text/css" /></head>

<body onload="MM_preloadImages('images/m1-act.jpg','images/m5-act.jpg','images/bar1-act.JPG','images/Awards_act.png','images/bar3-act.JPG','images/AboutMe2.png','images/Projects.png')">
<div id="main">
<!-- header -->
	<div id="header">
		<div class="row-1">
			<div class="indent"></div>
		</div>
		<div class="row-2">
			<ul class="nav">
				<li><a href="index.html" onmouseout="MM_swapImgRestore()" onmouseover="MM_swapImage('r_1','','images/AboutMe2.png',1)"><img alt="" src="images/AboutMe.png" id="r_1" /></a></li>
				<li><a href="Research.html" onmouseout="MM_swapImgRestore()" onmouseover="MM_swapImage('r_2','','images/Research.jpg',1)"><img src="images/Research2.JPG" alt="" width="160" height="53" id="r_2" /></a></li>
				<li><a href="publication.html" onmouseout="MM_swapImgRestore()" onmouseover="MM_swapImage('r_3','','images/bar3-act.JPG',1)"><img src="images/bar3.JPG" alt="" width="160" height="53" id="r_3" /></a></li>
				<li><a href="awards.html" onmouseout="MM_swapImgRestore()" onmouseover="MM_swapImage('r_4','','images/Awards_act.png',1)"><img src="images/AWARDS.png" alt="" width="120" height="47" id="r_4" /></a></li>
				<li class="last"><a href="fun.html" onmouseout="MM_swapImgRestore()" onmouseover="MM_swapImage('r_5','','images/funproject2.jpg',1)"><img alt="" src="images/funproject.jpg" id="r_5" /></a></li>
			</ul>
	  </div>
	</div><div id="content">
<!-- content-box begin -->
  <div class="content-box">
			<div class="top">
				<div class="bottom">
					<div class="inside">
    
     <div class="boxnew">
<div class="inner">   
    <div>
    <h1 id="motivation">Research Projects</h1>
    <p>&nbsp;</p>
    </div>
    <div id = "menu">
      <ul id="MenuBar1" class="MenuBarHorizontal">
        <li><a class="MenuBarItemSubmenu" href="#algorithm"><strong>Novel Interaction </strong></a>
          <ul>
<li><a href="#AIGR">Cerebral Palsy Prediction</a></li>
<li><a href="#Back_of_Phone"> Back of Phone</a></li>
<li><a href="#Squezzing">Squeezing interface</a></li>
<li><a href="#fusing">Multi-modal Gesture Recognizer</a></li>
            <li><a href="#backtoback">Back-to-Back</a></li>
            <li><a href="#uCam">UCam</a></li>
            <li><a href="#pullandpush">Pull and Push</a></li>
          </ul>
  </li>
        <li><a href="#app"><strong> Interactive Apps</strong></a>
          <ul>
        <li><a href="#grabber">Surprise Grabber</a></li>
        <li><a href="#doodle">Doodle Space</a></li>
        <li><a href="#navigation">3D Space Navigation</a></li>
        <li><a href="#airmouse">Phone Air Mouse</a></li>
        <li><a href="#viewer">Picture Viewer</a></li>
        </ul>
</li>
        <li><a href="#localization"><strong>Indoor Localization</strong></a></li>
        <li><a class="MenuBarItemSubmenu" href="#3dauctionhouse"><strong>Review Analysis</strong></a>
          <ul>
        <li><a href="#Review Analysis"">Online Review Analysis and Visualization</a></li>
        </ul>
        </li>
		<li><a href="#uTouchHub"><strong>Multitouch</strong></a>      
		<ul>
			<li><a href="#utouchhub">Multitouch Table Hub</a></li>
		</ul>
		</li>
      </ul>
    </div>
    <p>&nbsp;</p>
    
    <p>&nbsp;</p>
    <div id="algorithm">
    <h2>Novel Interaction </h2>
    </div>
    <p>&nbsp;</p>
<table border="0">
        	<tr valign="top">
        	  <td height = "175"><div><a name = "GestureHealthCare" href="GestureHealthCare.html"><img src="images/AIGR_baby.jpg" alt="AIGR" width="257" height="238" /></a> <p>Click Picture to Watch related Video</p></div></td>
        	  <td width="502" height="175"><h3 id="AIGR">Gesture Recognition+Machine Learning+Health Informatics</h3>
        	    <p style="text-align:justify;">In this project, we demonstrate a Markov model based technique
        	      for recognizing gestures from accelerometers that explicitly
        	      represents duration. We do this by embedding an
        	      Erlang-Cox state transition model, which has been shown
        	      to accurately represent the first three moments of a general
        	      distribution, within a Dynamic Bayesian Network (DBN).
        	      The transition probabilities in the DBN can be learned via
        	      Expectation-Maximization or by using closed-form solutions.<br />
        	      We test this modeling technique on 10 hours of data collected
        	      from accelerometers worn by babies pre-categorized
        	      as high-risk in the Newborn Intensive Care Unit (NICU) at
        	      UCI. We show that by treating instantaneous machine learning
        	      classification values as observations and explicitly modeling
        	      duration, we improve the recognition of Cramped Synchronized
        	      General Movements, a motion highly correlated
        	      with an eventual diagnosis of Cerebral Palsy.</p>
<li><strong>[1]. Fan, M.</strong>, Gravem, D., Cooper, D., Patterson, D.  <a href="doc/Ubicomp12_MingmingFan.pdf">Augmenting Gesture Recognition with Erlang-Cox Models to Identify Neurological  Disorders in Premature Babies.</a> <em>Intl.  Conf. Ubiquitous Computing</em> <strong><em>(Ubicomp’12)</em></strong>,  p411-420.  </li></td>
      	  </tr>
          
          <tr valign="top">
<td width="264" height = "175">
          	<div align="center"><a name = "Back_of_Phone" href="back_phone_interaction.html" ><img src="images/backphone.png" alt="Back_of_Phone" width="306" height="298"></a> <p>Click Picture to Watch related Video</p> </div></td>
          <td>
          	<h3 id="squeezing">
            Back of Phone Interaction: Figner Motion based 3D interaction at the back of Phone
            </h3>
            
            <p style="text-align:justify;">There is an increasing interest in extending mobile  phone interaction area from touch screen to surrounding 3D space. In this  study, we are researching the role of back space plays in phone interaction via  rear camera on phone in real time. A prototype and three applications were  developed allowing users to interact with their phones by moving fingers in  back space. Two strategies for mapping were also discussed to project finger’s  motor space into interaction space. A user study and follow-up analysis not  only showed the promising future of our prototype but also explained the  interaction issues. </p>
            <p>(<em>Right figure is the phone screen shot of the running app on Android Phone.</em>)</p>
<p><a href="#motivation" align="right">Go Back to Top</a></p>
          </td>
         
    </tr>


	<tr valign="top">
<td width="264" height = "175">
          	<div align="center"><a name = "Fusing different Sensors" href="Fusing camera and accelerometer.html"><img src="images/fusing1.jpg" alt="fusingdifferentsensors" ></a><p>Click Picture to Watch related Video</p>
        	    </div></td>
          <td>
          	<h3 id = "fusing">
            Fusing Camera&amp;Accelerometer for Phone 3D Interaction
            </h3>
            <p style="text-align:justify;">We human being process information from different channels (seeing, hearing, feeling, etc), how about our interaction if it could also sense from different channels?            </p>
<p style="text-align:justify;">Enlightened by this idea, I conduct this research by fusing camera and acceleromter on phone to improve hand's 3D motion detection accuracy.
  The basic idea is through tacking the features detected from the camera's  field of view as well as analyzing changes of accelerometer data, we could distinguish translation and  rotationin real time. </p>
            <ul>
              <li><strong>[1] </strong><strong>Fan,</strong> <strong>M.,</strong> Patterson, D., Shi, Y. <a href="doc/MobileHCI_paper110_MingmingFan.pdf">When Camera  Meets Accelerometer: A Novel Way for 3D Interaction of Mobile Phone (Poster). </a> <strong><em>MobileHCI’ 2012</em></strong>, p131-136.</li>
              <li></li>
              <li></li>
              <li></li>
              <li></li>
              <li></li>
              <li>[2]<strong>   Fan</strong>,M., Patterson, D., Shi, Y. <a href="doc/Distinguishing Translation and Rotation for Real-time Phone Gesture.pdf"><em>Distinguishing Translation and Rotation for  Real-time Phone Gesture Interaction.</em></a> 2011 Workshop on Ubiquitous Computing  Uniting the Californias, San Diego, Nov 11 -12, 2011</li>
              <li></li>
             </ul> 
              <p><a href="#motivation" align="right">Go Back to Top</a></p>  
          </td>
         
    </tr>
    
    
        	<tr valign="top">
<td width="264" height = "213">
          	<div align="center"><a name = "Back-to-Back"></a><img src="images/BacktoBack.jpg" alt="fusingdifferentsensors" width="256" height="161">
        	    </div></td>
          <td>
          	<h3 id="backtoback">
            Back-to-Back: Using relative postion for 3D interaction
            </h3>
            <p>In recent years, mobile phones tend to equip two camera at the front and the back sides, one for video-commnication and the other for photographing.</p>
            <p>Whiling manipulting my phone, I get the idea that the geometric relationship between them could be used to hand motion detection. Back-to-Back is a prototype that by binding two webcameras together back to back and exploting the perfect geometric relationship between them, we can deduce hand's 3D motion  through simple trigonometry algorithm.       
            </p>
            <ul>
            	<li><strong>[1] Mingming Fan</strong>, Yuanchun Shi. <a href="doc/achi_2011_5_10_20177.pdf"> Back-to-Back: A Novel Approach for Real Time 3D Hand Gesture Interaction. <em>To be appeared in The Fourth International Conference on Advances in Computer-Human Interactions (ACHI 2011).</em> </a></li>
             </ul> 
             <p><a href="#motivation" align="right">Go Back to Top</a></p>  
          </td>
          
    </tr> 
    
    
    
    	<tr valign="top">
<td width="264" height = "175">
          	<div align="center"><a name = "UCam" href="UCam.html"><img src="images/research_clip_image002.jpg" alt="UCam" width="256" height="161"></a><p>Click Picture to Watch related Video</p></div></td>
          <td>
          	<h3 id="uCam">
            UCam: 3D <em>U</em>niversal Control by a <em>Handheld Cam</em>era
            </h3>
            <p>This work presents the scheme of a real-time and reliable method for recognizing hand's 3D motion only by one handheld camera. The algorithm is totally based on analyzing geometric characters of features from a single camera in user’s hand. As the camera is used in untrained environment, it’s difficult to distinguish similar movements based on computer vision algorithms(e.g. optical flow). A novel differentiation algorithm by voting from some weak classifiers based on geometric characteristics is proposed. The result is quite promising.
            </p>
            <ul>
            	<li>             
<strong>[1] Mingming Fan</strong>, Liang Zhang, Yuanchun Shi. <a href="doc/VRST08_MingmingFan_Poster.pdf">Hand’s 3D Movement Detection with one Handheld Camera. <em>Proceedings of ACM Symposium on Virtual Reality Software and Technology 2008(ACM VRST’08). </em></a>
                </li>
            	<li></li>
            	<li></li>
                
                <li>
                [2] Liang Zhang, Yuanchun Shi, <strong>Mingming Fan</strong>. <a href="doc/UCam Direct Manipulation using Handheld Camera.pdf">UCam: Direct Manipulation using Handheld Camera for 3D Gesture Interaction. <em>Proceedings of ACM International Conference on Multimedia 2008(ACM MM'08)<em></a>
                </li>
             </ul>   
          <p><a href="#motivation" align="right">Go Back to Top</a></p>
          </td>
    </tr> 
    



        	<tr valign="top">
<td width="264" height = "275">
          	<div align="center"><a name = "Pull and Push"></a><img src="images/pullandpush.jpg" alt="PullandPush" align="center">
        	    </div></td>
          <td>
          	<h3 id="pullandpush">
            Pull and Push: Navigating in 3D space by Handheld Camera
            </h3>
            <p>In the 3D object controlling or virtual space navigation tasks, it is necessary to provide the efficient zoom operation. The common method is using the combination of the mouse and keyboard. However, it requires users to be familiar with the operation which needs much time to practice. This paper presents two methods to recognize the zoom operation by sensing hand's pull and push. People only need to hold a camera in hand and when they pull or push hand, our approach will sense the proximity and translate it into the zoom operation in the tasks. By user studies, we have compared different methods' correct rates and analyzed the factors which might affect the approach's performance.      
            </p>
            <ul>
            	<li>
<strong>[1] Mingming Fan</strong>, Yuanchun Shi. <a href="doc/HCII'09_Pull and Push.pdf">Pull and Push: Proximity-Aware User Interface for Navigating in 3D Space Using a Handheld Camera. <em> Proceedings of HCI International 2009 (HCII'09), LNCS5612,pp133-140</em></a>
                </li>
             </ul> 
              <p><a href="#motivation" align="right">Go Back to Top</a></p>  
          </td>        
    </tr>
 
 
         	<tr valign="top">
<td width="264" height = "175">
          	<div align="center"><a name = "Squezzing" ><img src="images/Squezzing.jpg" alt="Squezzing" width="227" height="243"></a> </div></td>
          <td>
          	<h3 id="squeezing">
            Squeezing you pocket to retrieval information
            </h3>
            <p>Mobile phone tends to push the inforamtion(e.g. phone calls,short messages)without any awareness of the context where users are in, which usually makes people embarrassed.
</p>
            <p>How about we change it? Instead of being pushed with information, we simply leave our phone in our pocket. When we want to check the information, we just need to squeeze the pocket. If there is new message, our phone may vibrate as many times as the number of new messages. So we do not need to take the phone out of our pocket if there is actually no new message.            </p>
            <p><a href="#motivation" align="right">Go Back to Top</a></p>
          </td>
         
    </tr>
 
         
</table>




<p>&nbsp;</p>
<div id = "app">
    <h2>Interactive Applications based on Phone Hand Gesture</h2>
    </div>
    <p>&nbsp;</p>
<table border="0">

        	<tr valign="top">
<td width="305" height = "175">
          	<div align="center"><a name = "Surprise Grabber" href="SurpriseGrabber.html"><img src="images/SurpriseGrabber2.png" alt="SurpriseGrabber"></a><p>Click Picture to Watch related Video</p></div></td>
          <td width="422">
          	<h3 id="grabber">
            Surprise Grabber
            </h3>
            <p>"Surprise Grabber" is a multi-user co-located mobile game. Different from single-user or multi-user on-line mobile games. All players stand in front of a public display where a social atmosphere is formed. The whole system is easy to implement in public places. A small piece of code on the phone detects hand motion, delivers results to the server and provides feedbacks. The Sever PC receives the results from different users via Bluetooth and renders the whole game space. A concept "Cyber Physical Circle" is introduced, which tries to connect the co-located mobile game with players' social network.     
            </p>
            <ul>
              [1] <strong>Mingming  Fan</strong>,  Xin Li, Yu Zhong, Li Tian, Yuanchun Shi, Hao Wang. <a href="doc/CSCW'11-Mingming.pdf"><em>Surprise Grabber: a Co-located Tangible Social Game based on Phone Hand  Gestures. </em>To be appeared in ACM Conference on Computer Supported Cooperated  Work 2011. <em>(CSCW’2011)</em></a>
              <li><em></em>                </li>
             </ul>
              <p><a href="#motivation" align="right">Go Back to Top</a></p>   
          </td>
         
    </tr>


        	<tr valign="top">
<td width="305" height = "175">
          	<div align="center"><a name = "DoodleSpace" href="DoodleSpace.html"><img src="images/DoodleSpace.jpg" alt="DoodleSpace"></a><p>Click Picture to Watch related Video</p></div></td>
          <td>
          	<h3 id="doodle">
            Doodle Space
            </h3>
            <p>In this research, we present a novel interactive application on public displays, Doodle Space, which allows multiple participants collaboratively painting 3D objects on a projected wall using personal cam-phones. An image based algorithm is used to estimate the movement parameters of cam phones. Users control a virtual brush naturally by moving their phones in the air. Participants’ experiences show that it is intuitive, convenient and enjoyable to paint in Doodle Space by our interaction techniques.     
            </p>
            <ul>
            	<li>
[1] Yuanchun Shi, <strong>Mingming Fan</strong>, Chun Yu, etc. <a href="doc/Doodle_Space_Camera_Ready.pdf">Painting in Public Doodle Space with Cam-Phone Brush. <em>Adjuctive Proceedings of Ubicomp, Demo at Ubicomp 2009</em></a>
               </li>
            	<li></li>
               <li>
               [2] Yu Zhong, Xin Li, <strong>Mingming Fan</strong>, Yuanchun Shi. <a href="doc/mm09.pdf">Doodle Space: Painting on A Public Display by Cam-Phone. <em>Proceedings of ACM Multimedia Workshop (AMC'09)</em></a>
               </li>
             </ul>  
             <p><a href="#motivation" align="right">Go Back to Top</a></p> 
          </td>
          
    </tr>



        	<tr valign="top">
<td width="305" height = "175">
          	<div align="center"><a name = "3DNavigation" href="3D scene navigation.html"><img src="images/PullandPushNavigation.jpg" alt="PullandPushNavigation"></a> <p>Click Picture to Watch related Video</p></div></td>
          <td>
          	<h3 id="navigation">
            3D Navigation by UCam
            </h3>
            <p>How do you navigate in your video games? Keyboard and mouse? Probably you do need that, try our UCam.</p>
            <p>In this project, we try to use a camera navigating in the virtual world. Rotating your hand around to change the orientation, pulling andpushing your hand to zooming in and out of the scene. </p>
            <p><a href="#motivation" align="right">Go Back to Top</a></p>
          </td>
         
    </tr>


        	<tr valign="top">
<td width="305" height = "175">
          	<div align="center">
          	  <p><a name = "PhoneAirMouse" href="AirMouse.html"><img src="images/AirMouse2.jpg" alt="AirMouse"></a> </p>
          	  <p>Click Picture to Watch related Video</p>
          	</div></td>
          <td>
          	<h3 id="airmouse">
            Phone Air Mouse
            </h3>
            <p>"AirMouse" enables your mobile phone working as an air mouse. By waving mobile phone, users could control PC cursor's movement. Meanwhile, by pressing left and right navigation keys on the phone, users could easily fullfill clicking operation.      
            </p>
   <p><a href="#motivation" align="right">Go Back to Top</a></p>
          </td>
         
    </tr>


        	<tr valign="top">
<td width="305" height = "175">
          	<div align="center">
          	  <p><a name = "PictureViewer" href="PictureViewer.html"><img src="images/PictureViewerSmall.jpg" alt="PictureViewer"></a></p>
          	  <p><a name = "PictureViewer" href="PictureViewer.html"></a></p>
           <p> Click Picture to Watch related Video </p></div></td>
          <td>
          	<h3 id="viewer">
            Picture Viewer
            </h3>
            <p>"PictureViewer" enables users to observe details of different parts of the picture by moving their phones in the air. Comparing with traditional key-press interaction, my method is more intuitive and without need of any visual markers or other auxiliary devices.     
            </p>
            <p><a href="#motivation" align="right">Go Back to Top</a></p>
          </td>
    </tr>
</table>



    <p>&nbsp;</p>
    <div id="localization">
      <h2>Indoor Localization based on Mobile Phones Sensors</h2>
    </div>
    <table border="0">
      <tr valign="top">
        <td width="258" height = "175"><div align="center"><a href="http://research.microsoft.com/apps/video/default.aspx?id=156557" name = "Squezzing" id="Squezzing" ><img src="images/localization.jpg" alt="Squezzing" width="324" height="210" /></a>Click Picture to Watch related Video</div></td>
        <td width="415"><h3 id="squeezing2">&nbsp;</h3>
          <p><a href="http://research.microsoft.com/en-us/projects/indoorloc/">Indoor localization</a> is a research project at Microsoft Research Asia, which I participated in during two times internships. </p>
          <p>For the recent internship  (06/12 to 09/12), I mainly focus on the question that: how to effectively build  the offline database for fingerprinting localization methods? As is shown in  literature, one significant drawback of  fingerprinting based localization is <strong>building the fingerprinting map before</strong> delivered for  use. The normal way is hiring labors to walk through the space, stand at each  point and scan AP signal strengths. Such manual way requires lots of  pre-planning and more importantly long hours of labeling (Labor literally needs  to stand at each possible location to scan the signal strength in order to get  a better localization coverage). Therefore, <strong>my work</strong> is making such  procedure automatic: building a system that can automatically match the  location with APs' signal strength.</p>
<p>&nbsp;</p>
<p>For the first time  internship (04/11~07/11), my main research focus is how to detect user's  initial location without interference from users.          </p>
<p>Second time internship (06/12~09/12), my main focus is on Algorithm Design to Reduce the effort of training data set building for Mobile Indoor Localization</p>
<p><a href="#motivation" align="right">Go Back to Top</a></p></td>
      </tr>
    </table>




<p>&nbsp;</p>
        <h2>Oneline User-generated Reviews Analysis </h2>  
    <p>&nbsp;</p>
<table border="0">
        	<tr valign="top">
<td width="300" height = "175">
          	<div align="center">
          	  <a name = "Auction House" href="IBMExtremeBlueInternship.html">	<img src="images/ReviewAnalysis.png" width="313" height="235" alt="ReviewAnalysis" /></a> <p>&nbsp;</p>
          	</div></td>
          <td>
          	<h3 id="3dauctionhouse">Oneline User-generated Reviews Analysis</h3>
            <h3>&nbsp;</h3>
            <p>The basic research question we try to explore is "Can we give users brief but deep enough knowledge about a restaurant or hotel before diving into large volumn of reviews".</p>
            <p>Left Figure is the top mentioned nouns with top mentioned adjactives from reivews of one hotel.</p>
<p><a href="#motivation" align="right">Go Back to Top</a></p>
          </td>
    </tr>
</table>
<p>&nbsp;</p>
        <h2>Multitouch Table Hub  </h2>  
        <p>&nbsp;</p>
<table border="0">
        	<tr valign="top">
<td width="300" height = "175">
          	<div align="center">
          	  <p><a name = "Multitouch Table Hub" href=#><img src="images/uTable_Tsinghua.jpg" alt="utable"></a>          	    </p>
          	  <p>&nbsp;</p>
          	</div></td>
          <td>
          	<h3 id="uTouchHub">
            uTouchHub
            </h3>
            <p> Have you ever imagined what a very large scale  multitouch table (bigger than 10 microsoft surfaces) would be like? Led by Professor Yuanchun Shi, our group is turning this idea into reality.</p>
			<p> To achieve high resolution and large scale, we artfully combine 10 small multi-touch tables together to form the large scale multitouch table. I think a lot of problems may appear in your mind. How to adjust the display properly on such large scale surface? How to guarantee multiple users' finger traces continuous through such large scale surface ?</p>
            <p>As one of the research members, I am focusing on the second issue, along with Xiangliang Meng, Yue Shi, to guarantee the touch traces continuous in single unit and crossing different small units.
            The reason why name our project as &quot;Hub&quot; is because that our work is actually receiving signals from low-layer multiple cameras, processing and transmitting the traces informaton to up-layer applications. It is pretty much like how the Hub works. </p>
            <p><a href="#motivation" align="right">Go Back to Top</a></p>
            <p>&nbsp;</p>
            <p>[1] Patent:</p>
            <p align="left">Yuanchun Shi, Yue Shi, <strong>Mingming Fan, </strong>Yongqiang  Qin, Zhouyue Su.<strong> </strong><em>Cascaded  multi-camera based multi-touch traces connection and merging algorithm for  tabletop interaction. <strong>Chinese Patent NO</strong>. 201110164514.7. </em></p>
<p>&nbsp;</p>
          </td>
    </tr>
</table>



</div>
</div>
</div>

</div>
</div>
</div>
</div>
<div id="footer">
	  <div class="indent">copyrights©2010,2011,2012,2013. Mingming Fan
	    <div class="fleft"></div>
	  </div>
	</div>
</div>
<script type="text/javascript">
var MenuBar1 = new Spry.Widget.MenuBar("MenuBar1", {imgDown:"SpryAssets/SpryMenuBarDownHover.gif", imgRight:"SpryAssets/SpryMenuBarRightHover.gif"});
</script>

</body>
</html>
