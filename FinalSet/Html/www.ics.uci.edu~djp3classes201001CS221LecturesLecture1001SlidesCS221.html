<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html>
<head>
    <meta name="description" content="Vector Space Scoring Introduction to Information Retrieval CS 221 Donald J. Patterson Content adapted from Hinrich Schütze http://www.informationretrieval.org  Cosine Similarity Score Vector Space Scoring Define: Euclidean Length   Cosine Similarity Score Vector Space Scoring Define: Euclidean Length   Cosine Similarity Score Vector Space Scoring Example  Exercise Vector Space Scoring Rank the following by decreasing cosine similarity. Assume tf-idf weighting: Two docs that have only frequent words in common (the, a , an, of) Two docs that have no words in common Two docs that have many rare words in common (mocha, volatile, organic, shade-grown)  Exercise Vector Space Scoring  Exercise Vector Space Scoring  Exercise Vector Space Scoring  More of the same  Exercise Vector Space Scoring  More of the same   Exercise Vector Space Scoring  More of the same   Exercise Vector Space Scoring  More of the same   Exercise Vector Space Scoring Rank the following by decreasing cosine similarity. Assume tf-idf weighting: Two docs that have only frequent words in common (the, a , an, of) Two docs that have no words in common Two docs that have many rare words in common (mocha, volatile, organic, shade-grown)  Spamming indices Vector Space Scoring This was invented before spam Consider: Indexing a sensible passive document collection vs. Indexing an active document collection, where people, companies, bots are shaping documents to maximize scores Vector space scoring may not be as useful in this context.  Interaction: vectors and phrases Vector Space Scoring Scoring phrases doesn’t naturally fit into the vector space world: How do we get beyond the “bag of words”? “dark roast” and “pot roast” There is no information on “dark roast” as a phrase in our indices. Biword index can treat some phrases as terms postings for phrases document wide statistics for phrases  Interaction: vectors and phrases Vector Space Scoring Theoretical problem: Axes of our term space are now correlated There is a lot of shared information in “light roast” and “dark roast” rows of our index End-user problem: A user doesn’t know which phrases are indexed and can’t effectively discriminate results.  Multiple queries for phrases and vectors Vector Space Scoring Query: “rising interest rates” Iterative refinement: Run the phrase query vector with 3 words as a term. If not enough results, run 2-phrase queries and fold into results: “rising interest” “interest rates” If still not enough results run query with three words as separate terms.  Vectors and Boolean queries Vector Space Scoring Ranked queries and Boolean queries don’t work very well together In term space ranked queries select based on sector containment - cosine similarity boolean queries select based on rectangle unions and intersections  Vectors and wild cards Vector Space Scoring How could we work with the query, “quick* print*” ? Can we view this as a bag of words? What about expanding each wild-card into the matching set of dictionary terms? Danger: Unlike the boolean case, we now have tfs and idfs to deal with Overall, not a great idea  Vectors and other operators Vector Space Scoring Vector space queries are good for no-syntax, bag-of-words queries Nice mathematical formalism Clear metaphor for similar document queries Doesn’t work well with Boolean, wild-card or positional query operators But ...  Query language vs. Scoring Vector Space Scoring Interfaces to the rescue Free text queries are often separated from operator query language Default is free text query Advanced query operators are available in “advanced query” section of interface Or embedded in free text query with special syntax aka -term -”terma termb”  Alternatives to tf-idf Vector Space Scoring Sublinear tf scaling 20 occurrences of “mole” does not indicate 20 times the relevance This motivated the WTF score.    There are other variants for reducing the impact of repeated terms  TF Normalization Vector Space Scoring : Alternatives to tf-idf Normalize tf weights by maximum tf in that document   alpha is a smoothing term from (0 - 1.0 ) ~0.4 in practice This addresses a length bias. Take one document, repeat it, WTF goes up  TF Normalization Vector Space Scoring : Alternatives to tf-idf Normalize tf weights by maximum tf in that document   a change in the stop word list can change weights drastically - hard to tune still based on bag of words model one outlier word, repeated many times might throw off the algorithmic understanding of the content  Laundry List Vector Space Scoring : Alternatives to tf-idf SMART system of describing your IR vector algorithm ddd.qqq (ddd = document weighting) (qqq = query weighting) first is term weighting, second is document, then normalization lnc.ltc is what?  Efficient Cosine Ranking Vector Space Scoring Find the k docs in the corpus “nearest” to the query the k largest query-doc cosines Efficient ranking means: Computing a single cosine efficiently Computing the k largest cosine values efficiently Can we do this without computing all n cosines? n = number of documents in corpus  Efficient Cosine Ranking Vector Space Scoring Computing a single cosine Use inverted index At query time use an array of accumulators Aj to accumulate component-wise sum Accumulate scores as postings lists are being processed (numerator of similarity score)  Efficient Cosine Ranking Vector Space Scoring For the web an array of accumulators in memory is infeasible so only create accumulators for docs that occur in postings list dynamically create accumulators put the tf_d scores in the postings lists themselves limit docs to non-zero cosines on rare words or non-zero cosines on all words reduces number of accumulators  Efficient Cosine Ranking Vector Space Scoring  Use heap for selecting the top K Scores Vector Space Scoring Binary tree in which each node’s value &gt; the values of children Takes 2N operations to construct then each of k “winners” read off in 2logn steps For n =1M, k=100 this is about 10% of the cost of sorting"/>
    <title></title>
    <script type="text/javascript" language="javascript">
//      <![CDATA[
            var images = new Array (30);
            images[0] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.001.png";
            images[1] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.002.png";
            images[2] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.003.png";
            images[3] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.004.png";
            images[4] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.005.png";
            images[5] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.006.png";
            images[6] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.007.png";
            images[7] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.008.png";
            images[8] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.009.png";
            images[9] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.010.png";
            images[10] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.011.png";
            images[11] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.012.png";
            images[12] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.013.png";
            images[13] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.014.png";
            images[14] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.015.png";
            images[15] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.016.png";
            images[16] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.017.png";
            images[17] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.018.png";
            images[18] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.019.png";
            images[19] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.020.png";
            images[20] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.021.png";
            images[21] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.022.png";
            images[22] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.023.png";
            images[23] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.024.png";
            images[24] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.025.png";
            images[25] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.026.png";
            images[26] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.027.png";
            images[27] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.028.png";
            images[28] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.029.png";
            images[29] = "Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.030.png";
            var index = 0;
            function WindowLoaded(evt)
            {
                document.body.onselectstart = function () { return false; };
            }
            function Step(i)
            {
                GoTo(index + i)
            }
            function GoTo(newIndex)
            {
                if(newIndex >= 0 && newIndex < images.length)
                {
                    index = newIndex;
                    document.Slideshow.src = images[index];
                }
            }
//      ]]>
    </script>
</head>
<body bgcolor="black" onload='WindowLoaded(event);'>
    <p align="center">
        <br/>
        <br/>
        <img name="Slideshow" alt="" src="Lecture10_01_Slides_CS221_files/Lecture10_01_Slides_CS221.001.png" onclick="Step(1)"/>
        <br/>
        <br/>
        <input type="image" src="Lecture10_01_Slides_CS221_files/home.png" onclick="GoTo(0)"/>
        &nbsp;&nbsp;&nbsp;
        <input type="image" src="Lecture10_01_Slides_CS221_files/prev.png" onclick="Step(-1)"/>
        <input type="image" src="Lecture10_01_Slides_CS221_files/next.png" onclick="Step(1)"/>
    </p>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-15170336-1");
pageTracker._trackPageview();
} catch(err) {}</script>

</body>
</html>

