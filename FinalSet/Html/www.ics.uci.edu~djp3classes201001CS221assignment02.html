<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
<title>CS 221, Information Retrieval, Winter 2009-2010 Department of Informatics, UCI</title>
<link href="styleSheet.css" rel="stylesheet" type="text/css" />
<style type="text/css">
<!--
.style13 {font-size: x-large}
.style12 {color: #990000}

-->
</style>
</head>
<body>
<p class="style2 style6 style7">Computer Science 221: Information Retrieval </p>
<p class="style6">Winter 2009-2010</p>
<p class="style6">Department of Informatics</p>
<p class="style6">Donald Bren School of Information and Computer Sciences </p>
<div class="container"> <a href="index.html">Home</a> | <a href="admin.html">Administrative Policies </a> | <a href="structure.html">Course Structure</a> |<a href="materials.html"> Materials</a> | <a href="assignmentSchedule.html#CurrentWeek">Assignment Schedule </a></div>
<div class="container">
  <p><span class="style7 style13">Assignment 02</span></p>
  <div class="indentRight, container">
    <ol>
      <li><span class="style5">Goals </span>
        <ol>
            <li>To teach you how difficult crawling the web is in terms of scale and temporal requirements. The software itself is not the hard part of this assignment, it is managing all the data that comes back. This is only one web domain (wikipedia) after all. </li>
            <li>To teach you how difficult it is to process text that is not written for computers to consume. Again this is a very structured domain as far as text goes.</li>
            <li>To make the point that web-scale web-centric activities do not lend themselves to &quot;completeness&quot;. In some sense you are never done. So thinking about web algorithms in terms of &quot;finishing&quot; doesn't make sense. You have to change your mindset to &quot;best possible&quot; given resources. </li>
        </ol>
      </li>
      <li><span class="style5">Java program   </span> (100%)      
        <ol>
          <li><span class="style5">Write a program to crawl wikipedia </span>          </li>
          <li><span class="style5">Administration</span>              
            <ol>
              <li>You may work in teams of 1, 2.</li>
              <li>If you do not have access to your own resources, run your program on an openlab machine in the ICS unix environment.
                <ol>
                  <li>This program should be able to access the &quot;/extra/grad_space&quot; as per these <a href="http://www.ics.uci.edu/~lab/students/extra.php">instructions</a> </li>
                </ol>
              </li>
            </ol>
          </li>
          <li><span class="style5">Suggested approach . </span>                
      
          
                <ol>
                  <li>Use <a href="http://code.google.com/p/crawler4j/">crawler4j</a> as your crawling engine. It is a java library. </li>
                  <li>Follow the instructions at http://code.google.com/p/crawler4j/ and create your MyCrawler and Controller classes.</li>
                  <li>Remember to set the maximum heap of java to an acceptable value. For example, if your machine has 2GB RAM. You may want to assign 1GB RAM to your crawlers. You can add -Xmx1024M to your java command line parameters.</li>
                  <li>Make sure that you dump your partial results in a permanent storage as you crawl. For example, you can write palindromes longer than 20 characters in a file and after finishing the crawl, report 10 longest one. It is always possible that your crawl crashes in the mean time or you pass the deadline. So, it's a good practice to have your partial results in a file.  Also make sure to flush the file whenever you write in it.</li>
                  <li>If you're crawling on a remote linux machine like openlab machines, make sure to use the nohup command. Otherwise your crawling, which may take several days, will be stopped if your connection is disconnected for even a second. Search the web for how to use this command.</li>
</ol></li>         
                <li><span class="style5">Inputs</span>                
                  <ol>
                    <li>A URL start Page (the seed set)
                      <ol>
                        <li>A regular expression </li>
                        <li>Pages are only crawled if the url matches this regular expression.</li>
                      </ol>
                    </li>
                  </ol>
                </li>
                <li><span class="style5">Outputs</span>
                  <ol>
                    <li>Find the 10 longest Palindromes in  english content pages of wikipedia that are not on a page about palindromes. Present them in a table with the source page from which they came. </li>
                    <li>Find the  10 Rhopalics with the most number of words in  english content pages of wikipedia. Present them in a table with the source page from which they came. </li>
                    <li>Number of Pages Crawled.</li>
                    <li>Number of Links found in these pages.</li>
                    <li>Total Size of the Downloaded Content.</li>
                    <li>Total Size of the Extracted Text of the Pages.</li>
                    <li>What was the docid of the Page with this URL in your crawl: http://en.wikipedia.org/wiki/Barack_Obama</li>
                  </ol>
                </li>
        </ol>
            <li><span class="style5">Submitting your assignment  </span>
              <ol>
                <li>Turn your report into the dropbox created for this assignment on EEE. </li>
                <li>Make the file name &lt;StudentID&gt;-&lt;StudentIID&gt;-Assignment02.pdf</li>
                <li>Please include the full names of all of your group members as appropriate in the document </li>
              </ol>
            </li>
              <li><span class="style5">Evaluation</span>:
                <ol>
                  <li>Produce the palindrome and rhopalic   
                    and source URLs from part 3.
                    <ol>
                        <li> A portion of the grade will be assigned according to the length of the sequence compared to the longest known sequence. </li>
                    </ol>
                  </li>
                  <li>Adhering to administrative requirements</li>
                </ol>
              </li>
              </ol>
      </div>
</div>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-15170336-1");
pageTracker._trackPageview();
} catch(err) {}</script>

</body>
</html>

