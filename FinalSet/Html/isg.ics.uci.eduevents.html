<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" type="text/css" href="isg.css"></link><title>ISG</title></head><body><div id="container"><div id="header"><div id="logosection"><table><tr><td style="padding-left:15px;"><a href="http://www.ics.uci.edu/~rares/isg-logo/"><img src="images/isg-website.png" height="100"></img></a></td><td><h1>ISG</h1><h2><a href="http://isg.ics.uci.edu">Information Systems Group</a></h2><h3><a href="http://www.ics.uci.edu">Bren School of ICS</a></h3><h4><a href="http://www.uci.edu">UC Irvine</a></h4></td></tr></table></div><div id="picture"><img src="images/ics3.jpg" height="80"></img></div></div><div id="navigation"><ul><li class=""><a href="index.html">About</a></li><li class=""><a href="news.html">News</a></li><li class=""><a href="people.html">People</a></li><li class=""><a href="research.html">Research</a></li><li class=""><a href="publications.html">Publications</a></li><li class="selected"><a href="events.html">Events</a></li><li class=""><a href="courses.html">Courses</a></li><li class=""><a href="partnerships.html">Partnerships</a></li></ul></div><div id="subcontent"><div class="box"><ul class="menublock"><li><a href="#seminars">ISG Seminars</a></li><li><a href="#invited">Invited Talks</a><ul><li><a href="#upcomingE">Upcoming</a></li><li><a href="#pastE">Past</a></li></ul></li></ul></div></div><div class="content"><h2 id="seminars">ISG Seminars</h2><h3>Regular ISG seminar </h3>Time: Every Fri afternoon, 3pm - 4pm; Location: Bren Hall 3011
		<p>2009-2010 ISG Scalable Data Management Seminar Series <a href="http://isg.ics.uci.edu/scalable_dm_lectures2009-10.html">[talks]</a></p><p>Support for the ISG Seminar Series from Yahoo! is gratefully acknowledged.
        </p></div><div class="content"><h2 id="invited">Invited Talks</h2><h3 id="upcomingE"> Upcoming Events </h3><table id="tb1"><tr><td><div class="eventDate">Jan 31, 2014 (Special Location)</div></td><td><div class="eventInfor">SPEAKER: Padhraic Smyth (UCI)</div><div class="eventInfor">Statistical Machine Learning with Count Data (Informatics Seminar)</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99264");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99264" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Jan 31, 2014 (Special Location) 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 6011 </td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Padhraic Smyth (UCI)</td></tr><tr><td><b>Title</b></td><td>Statistical Machine Learning with Count Data (Informatics Seminar)</td></tr><tr><td><b>Abstract</b></td><td>	(Regular ISG Seminar attendees are encouraged to attend this week's
very interesting Informatics Seminar - we don't want to conflict with
this talk!)

Data represented in the form of sets of counts is easy to acquire and 
can be surprisingly useful in practice. For example, a simple way to 
represent a set of documents is as a "bag of words" where each document 
is represented just by the counts of words that occur in the document, a 
representation that has been the basis for many successful applications 
of machine learning to text data. In this talk we will review some 
important developments over the past 10 years in modeling data 
represented in the form of counts, combining ideas from statistics and 
machine learning. The talk will describe the general principles involved 
and then illustrate how these ideas can be applied to text documents, 
email communications, and social networks, including recent work in my 
research group. The talk will conclude with some speculative comments on 
future directions.
     </td></tr><tr><td><b>Speaker Bio</b></td><td>Padhraic Smyth is a Professor in the Department of Computer Science 
(with a joint appointment in Statistics) and Director of the Center for 
Machine Learning and Intelligent Systems at the University of 
California, Irvine. His research interests include machine learning, 
data mining, pattern recognition, and applied statistics. He received a 
first class honors degree in Electronic Engineering from University 
College Galway (National University of Ireland) in 1984, and the MSEE 
and PhD degrees from the Electrical Engineering Department at the 
California Institute of Technology in 1985 and 1988 respectively. From 
1988 to 1996 he was a Technical Group Leader at the Jet Propulsion 
Laboratory, Pasadena, and has been on the faculty at UC Irvine since 
1996. Dr. Smyth is an ACM Fellow, a AAAI Fellow, and recieved the ACM 
SIGKDD Innovation Award in 2009. He is co-author of two well-known 
research texts in data mining: Modeling the Internet and the Web: 
Probabilistic Methods and Algorithms (with Pierre Baldi and Paolo 
Frasconi in 2003), and Principles of Data Mining, MIT Press, August 
2001, co-authored with David Hand and Heikki Mannila. He has served in 
editorial positions for journals such as the Journal of the American 
Statistical Association, the IEEE Transactions on Knowledge and Data 
Engineering, and the Journal of Machine Learning Research. His research 
has been funded by a variety of government agencies such as NSF, NIH, 
ONR, DARPA and DOE, as well by companies such as Google, IBM, Microsoft, 
and Yahoo! In addition to his academic research he is also active in 
consulting, working with companies such as Samsung, Netflix, eBay, 
Oracle, Microsoft, Yahoo!, Nokia, and ATT. </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Feb. 7, 2014</div></td><td><div class="eventInfor">SPEAKER:  Inci Cetindil (UCI ISG)</div><div class="eventInfor"> </div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99265");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99265" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Feb. 7, 2014 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td> Inci Cetindil (UCI ISG)</td></tr><tr><td><b>Title</b></td><td> </td></tr><tr><td><b>Abstract</b></td><td>	     </td></tr><tr><td><b>Speaker Bio</b></td><td> </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Feb. 14, 2014</div></td><td><div class="eventInfor">SPEAKER: Siripen Pongpaichet (UCI ISG)</div><div class="eventInfor"> EventShop</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99266");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99266" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Feb. 14, 2014 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Siripen Pongpaichet (UCI ISG)</td></tr><tr><td><b>Title</b></td><td> EventShop</td></tr><tr><td><b>Abstract</b></td><td>	     </td></tr><tr><td><b>Speaker Bio</b></td><td> </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">March. 7, 2014</div></td><td><div class="eventInfor">SPEAKER: Michalis Petropoulos (Pivotal)</div><div class="eventInfor">Hawk: How to Do SQL on Top of HDFS</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99266");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99266" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>March. 7, 2014 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Michalis Petropoulos (Pivotal)</td></tr><tr><td><b>Title</b></td><td>Hawk: How to Do SQL on Top of HDFS</td></tr><tr><td><b>Abstract</b></td><td>	     </td></tr><tr><td><b>Speaker Bio</b></td><td> </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">April. 18, 2014</div></td><td><div class="eventInfor">SPEAKER: Pekka Kostamaa (Teradata)</div><div class="eventInfor">Big Data at Teradata</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99272");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99272" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>April. 18, 2014 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Pekka Kostamaa (Teradata)</td></tr><tr><td><b>Title</b></td><td>Big Data at Teradata</td></tr><tr><td><b>Abstract</b></td><td>	     </td></tr><tr><td><b>Speaker Bio</b></td><td> </td></tr></table></div><div style="height: 5px;"></div></td></tr></table></div><div class="content"><h3 id="pastE"> Past Events </h3><table id="tb2"><tr><td><div class="eventDate">Jan. 24, 2014</div></td><td><div class="eventInfor">SPEAKER: Inna Giguere (Data Architect, Disney Interactive Media BI)</div><div class="eventInfor">Web Analytics at the happiest place on earth</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99260");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99260" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Jan. 24, 2014 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Inna Giguere (Data Architect, Disney Interactive Media BI)</td></tr><tr><td><b>Title</b></td><td>Web Analytics at the happiest place on earth</td></tr><tr><td><b>Abstract</b></td><td>
Business analytics requirements at Disney Interactive have pushed the limits of the Omniture reporting systems that has been used for the past decade into building an internal tracking and data warehouse solution. Consequently, we have built a data warehouse and enabled Video and Game Producers to fine-tune new content in near real-time, as well as provide an exhaustive platform for Data Scientists to build recommendation systems.
The presentation will focus on current data pipeline architecture at Disney Interactive and cover specific steps and challenges.  I will discuss how we (BI team) were able to leverage Hadoop’s map/reduce processing capabilities and Vertica MPP engine to load data continuously from multiple sources. However, one of our biggest challenges remains handling memory intensive hash joins in Vertica without sacrificing performance. 
	     </td></tr><tr><td><b>Speaker Bio</b></td><td>Inna Giguere is Data Architect at Disney Interactive Media Business Intelligence group. For the last 2 years she has been leading the architecture design and implementation of the Analytics Data Warehouse utilizing Hadoop, Vertica, and Scribe technology. Previously based out of San Francisco Bay Area and London, Inna has 16 years on industry experience creating scalable Data Warehouse solutions with focus on DB performance optimization in transactional and reporting systems. Her experience spans across technologies starting from COBOL/DB2 to Oracle (8i – 11g), to SQL Server 2005-2012, to Vertica 6.1 working on datasets ranging from hundreds of megabytes to hundreds of terabytes. She has earned MS in Statistics in 2010.
</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Jan. 17, 2014</div></td><td><div class="eventInfor">SPEAKER: Phillip Sheu (Department of EECS)</div><div class="eventInfor">Semantic Computing and Applications</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99259");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99259" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Jan. 17, 2014 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Phillip Sheu (Department of EECS)</td></tr><tr><td><b>Title</b></td><td>Semantic Computing and Applications</td></tr><tr><td><b>Abstract</b></td><td>
Semantic Computing (SC) is an emerging field that addresses computing
technologies which allow users to search, create, manipulate and connect
computational resources (including data, documents, tools, people, agents,
devices, etc.) based on semantics.

Semantic Computing includes the computing technologies (e.g., artificial
intelligence, natural language, software engineering, data and knowledge
engineering, computer systems, signal processing, etc.), and their
interactions, that may be used to extract or process computational content
and descriptions. While some areas of Semantic Computing have appeared as
pieces in different disciplines, Semantic Computing glues these pieces
together into an integrated theme with synergetic interactions. It
addresses not only the analysis and transformation of signals (e.g.,
pixels, words) into useful information, but also how such information can
be accessed and used to synthesize new signals.

The National Science Foundation has approved the planning of an
Industry/University Cooperative Research Center (I/UCRC) for Semantic
Computing currently involving UCI, UCSD and UCLA. The missions of the
I/UCRC are to develop semantic technologies that may facilitate the
transition of the Internet into its next generation, and develop new
business models to stimulate, strengthen, and grow the economy.

An important outcome of this I/UCRC is a Semantic Problem Solving Network
(SPSN) which is a public consortium of resources from all domains
including data, documents, devices, products, services, and people. The
resources are interconnected and integrated with a service-oriented
architecture and a semantic layer to help the public to solve general
problems and professional users to solve domain specific problems (e.g.,
finance, IT, health, defense, entertainment, education, manufacturing).

This talk will introduce Semantic Computing and its applications, the
operations of the I/UCRC, the architecture of the SPSN, how companies and
academic researchers can join or affiliate with the I/UCRC and SPSN, and
how companies and academic researchers can benefit.
	     </td></tr><tr><td><b>Speaker Bio</b></td><td>
Phillip C.-Y. Sheu is a professor of EECS, Computer Science and Biomedical
Engineering at the University of California, Irvine. He received his B.S.
degree in EE from National Taiwan University, and MS and Ph.D degrees in
EECS from the University of California at Berkeley.

Dr. Sheu’s current research interests include semantic computing and
complex biomedical systems. He is a fellow of IEEE, a founder of the IEEE
Computer Society Technical Committee on Semantic Computing (TCSEM), IEEE
International Conference on Semantic Computing (ICSC), International
Journal of Semantic Computing (IJSC), the NSF I/UCRC (Industry University
Cooperative Research Center) for Semantic Computing (ISC) being planned,
and a main author of the book Semantic Computing (SC, eds. P. Sheu, H. Yu,
C.V. Ramamoorthy, A. Joshi and L.A. Zadeh, IEEE and Wiley, 2010).
</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Nov. 15, 2013</div></td><td><div class="eventInfor">SPEAKER: José A. Blakeley (Microsoft Corporation)</div><div class="eventInfor">Microsoft SQL Server Parallel Data Warehouse - Architecture Overview</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99252");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99252" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Nov. 15, 2013 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>José A. Blakeley (Microsoft Corporation)</td></tr><tr><td><b>Title</b></td><td>Microsoft SQL Server Parallel Data Warehouse - Architecture Overview</td></tr><tr><td><b>Abstract</b></td><td>
 In this talk I will present an architectural overview of the SQL Server Parallel Data Warehouse DBMS system. PDW is a massively parallel processing, share nothing, scale-out version of SQL Server for data warehouse and big data workloads. The product is packaged as a database appliance built on industry standard hardware.
	     </td></tr><tr><td><b>Speaker Bio</b></td><td>José Blakeley is Partner Architect in the Modern Data Warehousing Unit of the Server and Tools Division at Microsoft where he contributes to the development of the SQL Server Parallel Data Warehouse (PDW) DBMS product. José joined Microsoft in 1994. Some of his contributions at Microsoft include the development of the OLE DB data access interfaces, the integration of the .NET runtime with SQL Server 2005, the extensibility features in SQL Server, and the creation of the ADO.NET Entity Framework in Visual Studio 2008. José has authored many conference papers, book chapters and journal articles on design aspects of relational and object database management systems, and data access. Jose has 20 patents awarded and 22 patents pending. He became an ACM Fellow in 2009. Before joining Microsoft, José was a member of the technical staff with Texas Instruments where he was co-principal investigator of the DARPA Open-OODB system. He received a B. Eng from ITESM, Monterrey, Mexico, and a Ph.D. in computer science from University of Waterloo, Canada. </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Oct. 25, 2013</div></td><td><div class="eventInfor">SPEAKER: David Lomet (joint work with Justin Levandoski and Sudipta Sengupta)(MSR)</div><div class="eventInfor">LLAMA: A Cache/Storage Subsystem for Modern Hardware</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99248");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99248" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Oct. 25, 2013 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>David Lomet (joint work with Justin Levandoski and Sudipta Sengupta)(MSR)</td></tr><tr><td><b>Title</b></td><td>LLAMA: A Cache/Storage Subsystem for Modern Hardware</td></tr><tr><td><b>Abstract</b></td><td>
LLAMA is a subsystem designed for new hardware environments that supports an API for page-oriented access methods, providing both cache and storage management. Caching (CL) and storage (SL) layers use a common mapping table that separates a page’s logical and physical location. CL supports data updates and management updates (e.g., for index re-organization) via latch-free compare-and-swap atomic state changes on its mapping table. SL uses the same mapping table to cope with page location changes produced by log structuring on every page flush. To demonstrate LLAMA’s suitability, we tailored our latch-free Bw-tree implementation to use LLAMA. The Bw-tree is a B-tree style index. Layered on LLAMA, it has higher performance and scalability using real workloads compared with BerkeleyDB’s B-tree, which is known for good performance.
	     </td></tr><tr><td><b>Speaker Bio</b></td><td>
David Lomet (Ph.D from Penn) is a Principal Researcher and manager of the Database Group at Microsoft Research Redmond.  Earlier, he was at Digital's CRL, Wang Institute, and IBM Research.  Lomet has over 100 papers on databases, indexing, concurrency, and recovery, including two SIGMOD "best papers".  He is an inventor of transactions.  Lomet has served on SIGMOD, VLDB, and ICDE PCs, being co-chair of ICDE'2000 and VLDB'2006.  He won SIGMOD's Contributions Award for his service as Data Engineering Bulletin Editor-in-Chief since 1992. He has been editor of ACM TODS, VLDB Journal, and DAPD.  He has served on the VLDB Endowment and ICDE Steering Committee, has been IEEE TCDE Chair and is a Fellow of AAAS, ACM, and IEEE.
             </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Oct. 18, 2013</div></td><td><div class="eventInfor">SPEAKER: Tyson Condie (Microsoft and UCLA)</div><div class="eventInfor">Big Learning Systems</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99246");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99246" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Oct. 18, 2013 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Tyson Condie (Microsoft and UCLA)</td></tr><tr><td><b>Title</b></td><td>Big Learning Systems</td></tr><tr><td><b>Abstract</b></td><td>
A new wave of systems is emerging in the space of Big Data Analytics that open the door to programming models beyond Hadoop MapReduce (HMR). It is well understood that HMR is not ideal for applications in the domain of machine learning and graph processing. This realization is fueling a number of new (Big Data) system efforts: Berkeley Spark, Google Pregel, GraphLab (CMU), and AsterixDB (UC Irvine), to name a few. Each of these add unique capabilities, but form islands around key functionalities: fault-tolerance, resource allocation, and data caching. In this talk, I will provide an overview of some Big Data systems starting with Google's MapReduce, which defined the foundational architecture for processing large data sets. I will then identify a key limitation in this architecture; namely, its inability to efficiently support iterative workflows. I will then describe real-world examples of systems that aim to fill this computational void. I will conclude with a description of my own work on a layering that unifies key runtime functionalities (fault-tolerance, resource allocation, data caching, and more) for workflows (both iterative and acyclic) that process large data sets.
	     </td></tr><tr><td><b>Speaker Bio</b></td><td>
Tyson Condie is a principal scientist with the Cloud and Information Services Lab at Microsoft and an Assistant Professor at UCLA. He received his Ph.D. from Berkeley. His research focuses on data analytics, distributed systems, Internet-scale query processing and optimization, and declarative language design and implementation. His current work involves building a system software stack for large-scale data processing tasks on resource managers like Apache YARN, Berkeley Mesos, Google Omega, and Facebook Corona.             </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Oct. 11, 2013</div></td><td><div class="eventInfor">SPEAKER: Anhai Doan (University of Wisconsin and WalmartLabs)</div><div class="eventInfor">Toward Hands-Off Crowdsourcing: Crowdsourced Entity Matching for the Masses</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99245");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99245" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Oct. 11, 2013 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Anhai Doan (University of Wisconsin and WalmartLabs)</td></tr><tr><td><b>Title</b></td><td>Toward Hands-Off Crowdsourcing: Crowdsourced Entity Matching for the Masses</td></tr><tr><td><b>Abstract</b></td><td>Entity matching (EM) finds data records that refer to the same
real-world entity. Recent work has applied crowdsourcing to EM, and
has clearly established the promise of this approach. This work
however is limited in that it crowdsources only parts of the EM
workflow, requiring a developer who knows how to code to execute the
remaining parts. Consequently, this work does not scale to the growing
EM need at enterprises and crowdsourcing startups, and cannot handle
scenarios where ordinary users (i.e., the masses) want to leverage
crowdsourcing to match entities.

To address these problems, we propose the notion of hands-off
crowdsourcing (HOC), which crowdsources the entire workflow of a task,
thus requiring no developers. We show how HOC can represent a next
logical direction for crowdsourcing research, scale up EM at
enterprises and crowdsourcing startups, and open up crowdsourcing for
the masses. We describe Corleone, a HOC solution for EM. We show how
Corleone uses the crowd to generate blocking rules, applies active
learning to learn matchers, estimates accuracy given severe skew, and
identifies difficult-to-match pairs to which Corleone can apply more
complex matchers. Finally, we discuss the implications of our work to
executing crowdsourced RDBMS joins, cleaning learning models, and
soliciting complex information types from crowd workers.
	     </td></tr><tr><td><b>Speaker Bio</b></td><td>AnHai Doan is an Associate Professor in the database group at the
University of Wisconsin, Madison. His current interests include
crowdsourcing, knowledge bases, data integration, and information
extraction. He received the ACM Doctoral Dissertation Award in 2003
and a Sloan fellowship in 2007. AnHai was Chief Scientist of Kosmix, a
social media startup acquired by Walmart in 2011. Currently he also
works as Chief Scientist of WalmartLabs, a research and development
lab devoted to analyzing and integrating data for e-commerce. AnHai is
a co-author of “Principles of Data Integration” (with Alon Halevy and
Zack Ives), a textbook published by Morgan Kaufmann in 2012. </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Sept. 20, 2013</div></td><td><div class="eventInfor">SPEAKER: Li Xiong (Emory University)</div><div class="eventInfor">Real-Time Aggregate Monitoring with Differential Privacy</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99244");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99244" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Sept. 20, 2013 2 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Li Xiong (Emory University)</td></tr><tr><td><b>Title</b></td><td>Real-Time Aggregate Monitoring with Differential Privacy</td></tr><tr><td><b>Abstract</b></td><td>
While Big Data promises significant economic and social benefits, it also raises serious privacy concerns.  Real-time aggregate statistics of data collected from individuals can be shared to enable many applications such as disease surveillance and traffic monitoring.  However, it must be ensured that the privacy of individuals is not compromised.  While differential privacy has emerged as a de facto standard for private data analysis, directly applying the differential privacy mechanisms on time-series has limited utility due to high correlations between data values.  In this talk, I will present FAST, a novel Filtering and Adaptive Sampling based framework for monitoring aggregate Time-series under differential privacy.  FAST adaptively samples long time-series according to detected data dynamics and simultaneously uses filtering techniques to dynamically predict and correct released data values.  I will present experimental studies using real datasets demonstrating the feasibility and benefit of FAST and conclude with open questions. 
	     </td></tr><tr><td><b>Speaker Bio</b></td><td>
Li Xiong is an Associate Professor in the Department of Mathematics and Computer Science and the Department of Biomedical Informatics at Emory University where she directs the Assured Information Management and Sharing (AIMS) research group. She holds a PhD from Georgia Institute of Technology, an MS from Johns Hopkins University, and a BS from University of Science and Technology of China, all in Computer Science. She also worked as a software engineer in IT industry for several years prior to pursuing her doctorate. Her areas of research are in data privacy and security, distributed data management, and biomedical informatics. She is a recent recipient of the Career Enhancement Fellowship by Woodrow Wilson Foundation, a Cisco Research Award, and an IBM Faculty Innovation Award. Her current research is supported by NSF and AFOSR.
             </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Sep. 13, 2013</div></td><td><div class="eventInfor">SPEAKER: Raman Grover (ISG PhD candidate)</div><div class="eventInfor">Scalable Fault-tolerant Elastic Data Feeds</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99242");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99242" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Sep. 13, 2013 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Raman Grover (ISG PhD candidate)</td></tr><tr><td><b>Title</b></td><td>Scalable Fault-tolerant Elastic Data Feeds</td></tr><tr><td><b>Abstract</b></td><td>In this ISG talk / thesis proposal, I describe and study
the support for data feed ingestion in AsterixDB, a Big Data
Management System (BDMS) that provides a platform for the
scalable storage, searching, and analysis of very large volumes
of semi-structured data. Data feeds are a mechanism for having
continuous data arrive into a database system from external
sources that produce data continuously, and to have that data
incrementally populate a persisted dataset and associated indexes.
To my knowledge, this will be the first system to explore the
challenges involved in building a data ingestion facility that deals
with semi-structured data and employs partitioned parallelism to
scale the facility and couple it with high-volume and/or parallel
external data sources. I describe language-level support for
modeling/defining a feed and present the methodology for providing
tolerance to software/hardware failures. Mechanisms by
which a feed can dynamically adapt to different workloads for
optimum usage of resources are provided.</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">May 17, 2013</div></td><td><div class="eventInfor">SPEAKER: Charles Boicey (UCI Irvine Health and Information Services)</div><div class="eventInfor">Apache Hadoop in the Healthcare Setting</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99241");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99241" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>May 17, 2013 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Charles Boicey (UCI Irvine Health and Information Services)</td></tr><tr><td><b>Title</b></td><td>Apache Hadoop in the Healthcare Setting</td></tr><tr><td><b>Abstract</b></td><td>Apache Hadoop is open source software that enables distributed processing of large data sets across clusters of computers. Hadoop can scale up to thousands of computers, each able to store and process data. Hadoop is capable of ingesting and storing the types of data found in healthcare, structured, unstructured, image and video. Hadoop also has an advantage for healthcare in its ability to interoperate with other open source software. This interoperability combined with scalability makes Hadoop an ideal platform for the development of a software ecosystem that fills in the gaps left by the Electronic Medical Record and Enterprise Data Warehouse.</td></tr><tr><td><b>Speaker Bio</b></td><td>
Charles Boicey is the Informatics Solutions Architect for the UC Irvine Health. At UCI Charles is responsible for the development and implementation of the enterprise data warehouse, health information exchange, home health integration and UC Irvine Health’s “Big Data” initiative. Charles has 20 years of experience in the healthcare field.cope of clinical expertise encompasses trauma critical care nursing. Charles is Vice President of the American Nursing Informatics Association.
             </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">May 10, 2013</div></td><td><div class="eventInfor">SPEAKER: </div><div class="eventInfor">No Seminar</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99240");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99240" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>May 10, 2013 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td></td></tr><tr><td><b>Title</b></td><td>No Seminar</td></tr><tr><td><b>Abstract</b></td><td>
No ISG seminar.  Leaving this time free so that ISG affiliates can attend today's ICS Trends in Society and Information Technology talk (see www.ics.uci.edu/trends).             
             </td></tr><tr><td><b>Speaker Bio</b></td><td>
             </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">April 19, 2013</div></td><td><div class="eventInfor">SPEAKER: Michael J. Carey (with the AsterixDB dev team) </div><div class="eventInfor">Want To Kick My Asterix(DB)?</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99233");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99233" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>April 19, 2013 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Michael J. Carey (with the AsterixDB dev team) </td></tr><tr><td><b>Title</b></td><td>Want To Kick My Asterix(DB)?</td></tr><tr><td><b>Abstract</b></td><td>
		Due to several faculty traveling and thus being MIA this coming Friday, the planned ISG seminar by Teradata is being postponed until later in the quarter.  Instead, this week's Friday ISG seminar slot will be used to invite ISG (and ICS) community participation in the forthcoming Beta Release of a new open source BDMS (Big Data Management System) that members of UCI's ASTERIX project have been working on for nearly four years.  We want this new "product", to be called AsterixDB, to be very high quality - and we are hereby inviting interested helpers at UCI to come hear about it and then help us polish it by downloading it and playing with it - trying out its data model, query language, and API for apps - kicking its tires - this Friday!  Our goal is to get a handful of "outside the team" folks to join us in using the system ahead of the Beta Release and then filing any issues using the GoogleCode issue tracking infrastructure - so that when we release this publically, it's well-polished and well shaken out.  (To date we have only delivered an Alpha Release, and only very recently, to one of our industrial partners.)  So - if you like database technology and would like to help us deliver "Big Data 2.0" to the world in a month or so - and you have time/interest in playing a bit in the very near term - PLEASE COME FRIDAY and we will show you what AsterixDB is all about!  This will be an informal presentation, based on giving a tour of our Alpha documentation and the release info on the GoogleCode wiki, and then having the team give a demo and even help you get the system working in real time if you bring your favorite laptop when you come.  We hope to see some of you there! (Refreshments will be provided, as usual for the ISG seminar, but we might upgrade the refreshments a little for this event; we'll see. We will also surely do something nice later for any outside folks who do end up significantly contributing in this manner to the quality of the release.)  If you plan to come on Friday, please RSVP to the speaker (mjcarey@ics.uci.edu) so we know what to maybe expect in terms of potential turnout.  Thx!          
             </td></tr><tr><td><b>Speaker Bio</b></td><td>
		Mike Carey is a Professor in the ISG subgroup of the UCI CS department.  His goal is to eventually change the Big Data management landscape forever through the great work that our AsterixDB team has done and is now preparing to share.  :-)
             </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">April 12, 2013</div></td><td><div class="eventInfor">SPEAKER: Shahram Ghandeharizadeh (USC)</div><div class="eventInfor"></div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99232");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99232" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>April 12, 2013 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Shahram Ghandeharizadeh (USC)</td></tr><tr><td><b>Title</b></td><td></td></tr><tr><td><b>Abstract</b></td><td>          
             </td></tr><tr><td><b>Speaker Bio</b></td><td>
             </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">March 1, 2013</div></td><td><div class="eventInfor">SPEAKER: Pat Helland (Salesforce)</div><div class="eventInfor">Immutability Changes Everything</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99224");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99224" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>March 1, 2013 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Pat Helland (Salesforce)</td></tr><tr><td><b>Title</b></td><td>Immutability Changes Everything</td></tr><tr><td><b>Abstract</b></td><td>For a number of decades, I've been saying "Computing Is Like Hubble's Universe, Everything Is Getting Farther Away from Everything Else".   It
used to be that everything you cared about ran on a single database and the transaction system presented you the abstraction of a singularity;
your transaction happened at a single point in space (the database) and a single point in time (it looked like it was before or after all other transactions).

Now, we see a more complicated world.  Across the Internet, we put up HTML documents or send SOAP calls and these are not in a transaction.  Within a cluster, we typically write files in a file system and then read them later in a big map-reduce job that sucks up read-only files, crunches, and writes files as output.  Even inside the emerging many-core systems, we see high-performance computation on shared memory but increasing cost to using semaphores.  Indeed, it is clear that "Shared Memory Works Great as Long as You Don't Actually SHARE Memory".

There are emerging solutions which are based on immutable data.  It seems we need to look back to our grandparents and how they managed distributed work in the days before telephones.  We realize that "Accountants Don't Use Erasers" but rather accumulate immutable knowledge and then offer interpretations of their understanding based on the limited knowledge presented to them.  This talk will explore a number of the ways in which our new distributed systems leverage write-once and read-many immutable data.         
             </td></tr><tr><td><b>Speaker Bio</b></td><td>Pat Helland has been working on databases, transaction processing, messaging, and distributed systems for 34 years.  In the 1980s, he was chief architect of the Tandem NonStop's transaction system called TMF (Transaction Monitoring Facility).  From 1991 to 1994, he worked at HaL Computers (a subsidiary of Fujitsu) and designed and architected a CC-NUMA (Cache Coherent Non-Uniform Memory Architecture) multiprocessor which Fujitsu shipped.  Starting in 1994, Pat worked at Microsoft where he was the chief architect for MTS (Microsoft Transaction Server) and DTC (Distributed Transaction Coordinator).  Later, he built SQL Service Broker which offers high performance (&gt;100K msg/sec) transactional exactly-once messaging integrated with SQL Server.  From 2005 to 2007, Pat work at Amazon on the product catalog and then returned in 2007 to Microsoft.  By 2009, he was working on Cosmos, the multi-peta-byte storage and computational plumbing behind Bing.  This year, Pat moved to San Francisco to be by the grandkids and joined Salesforce.com working on database and filesystem technology.
             </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Feb 22, 2013</div></td><td><div class="eventInfor">SPEAKER: Swaroop Jagadish and Kapil Surlaker (LinkedIn)</div><div class="eventInfor">On Brewing Fresh Espresso: LinkedIn’s Distributed Data Serving Platform</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99223");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99223" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Feb 22, 2013 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Swaroop Jagadish and Kapil Surlaker (LinkedIn)</td></tr><tr><td><b>Title</b></td><td>On Brewing Fresh Espresso: LinkedIn’s Distributed Data Serving Platform</td></tr><tr><td><b>Abstract</b></td><td>As LinkedIn has grown, our core data sets and request processing requirements have grown as well. The development of Espresso was motivated by our desire to migrate LinkedIn’s online serving infrastructure from monolithic, commercial, RDBMS systems running on high cost specialized hardware to elastic clusters of commodity servers running free software; and to improve agility by enabling rapid development by simplifying the programming model, separating scalability, routing, caching from business logic. Espresso is a document-oriented distributed data serving platform that has been built to address LinkedIn’s requirements for a scalable, performant, source-of-truth primary store. It provides a hierarchical document model, transactional support for modifications to related documents, real- time secondary indexing, on-the-fly schema evolution and provides a timeline consistent change capture stream. 
This talk describes the motivation and design principles involved in building Espresso, its architecture and presents a set of experimental results that characterize the performance of the system along various dimensions.         
             </td></tr><tr><td><b>Speaker Bio</b></td><td>
Swaroop Jagadish is a member of the Data Infrastructure team at
Linkedin, where he works on distributed data systems
such as Databus, Helix and Espresso. Prior to that, he worked at Yahoo where he 
built one of the first real-time bidding engines in the display-ads industry. 
He holds B.E. from BMS College of Engineering and M.S. from University of California, Santa Barbara.
Kapil Surlaker is a member of the Data Infrastructure team at
Linkedin, where he works on distributed data systems
such as Databus, Helix and Espresso. Prior to that, he worked at
Kickfire (acquired by Teradata) where he built high-performance
Database systems. Earlier in his career, he worked on replication
technology at Oracle where he was part of the team that built Oracle
Streams. He holds B.Tech. (CS) from IIT Bombay and M.S. From University
of Minnesota.
             </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Feb 8, 2013</div></td><td><div class="eventInfor">SPEAKER: Ronen Vaisenberg (Google)</div><div class="eventInfor">Practice Talk: Scheduling Sensors for Monitoring Sentient Spaces using an Approximate POMDP Policy (Percom2013)</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99222");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99222" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Feb 8, 2013 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Ronen Vaisenberg (Google)</td></tr><tr><td><b>Title</b></td><td>Practice Talk: Scheduling Sensors for Monitoring Sentient Spaces using an Approximate POMDP Policy (Percom2013)</td></tr><tr><td><b>Abstract</b></td><td>We present a framework for sensor actuation and control in sentient spaces, in which sensors are used to observe a physical phenomena. Our framework utilizes the spatio-temporal statistical properties of an observed phenomena, with the goal of maximizing an application specified reward. Specifically, we define an observation of a phenomena by assigning it a discrete value (state) and we model its semantics as the transition between these values (states). This semantic model is used to predict the future states in which the phenomena is likely to be at, based on partially observed past states. To accomplish real-time agility, we designed an approximate, adaptive-grid solution for POMDPs that yields practically good results, and in some cases, guarantees on the quality of the approximation. We instantiate the framework in a camera network and use it perform real- time actuation of large-scale sensor networks. To the best of our knowledge, we are the first to address the problem of actuating a large scale sensor network based on an approximated POMDP formulation. Our semantic model is simple enough to be implemented in real-time, yet powerful enough to capture meaningful semantics of typical behavior. Our action selection process is as fast as a table lookup in real-time.          
             </td></tr><tr><td><b>Speaker Bio</b></td><td>
             </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Feb 15, 2013</div></td><td><div class="eventInfor">SPEAKER: Hongzhi Wang (ISG)</div><div class="eventInfor"></div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99222");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99222" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Feb 15, 2013 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Hongzhi Wang (ISG)</td></tr><tr><td><b>Title</b></td><td></td></tr><tr><td><b>Abstract</b></td><td>          
             </td></tr><tr><td><b>Speaker Bio</b></td><td>
             </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Feb. 1, 2013</div></td><td><div class="eventInfor">SPEAKER: Marco Sanvido (Pure Storage)</div><div class="eventInfor">The Why and How of an All-Flash Enterprise Storage Array</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99220");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99220" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Feb. 1, 2013 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Marco Sanvido (Pure Storage)</td></tr><tr><td><b>Title</b></td><td>The Why and How of an All-Flash Enterprise Storage Array</td></tr><tr><td><b>Abstract</b></td><td>Enterprise storage is an $30 billion a year industry dominated by spinning disks. Flash storage is poised to take a large
chunk of the market, having grown significantly in capacity and production, driven by consumer electronics.
Flash's technical advantages over disk promise storage arrays that are faster and easier to use while consuming less power and costing less.

The downsides of flash (inc. large erase blocks, limited overwrites, and higher price) mean that using flash as a drop-in
replacement for disk leads to increased price, volatile performance, and decreased reliability.
In this talk, we describe the design of the Pure FlashArray, an enterprise storage array built around consumer flash storage.
The array and its software, Purity, play to the advantages of flash while minimizing the downsides. Purity writes to flash
in multiples of the erase block size and stores its metadata in a key-value store that minimizes overwrites and stores approximate
answers, trading extra reads for fewer writes. And, Purity reduces data stored on flash through a range of techniques, including
compression, deduplication, and thin provisioning.

The net result is a flash array that deliver a sustained read-write workload of over 100,000 4kb I/O requests per second while
maintaining sub-millisecond latency. With many customers seeing 4x or greater data reduction, the Pure FlashArray ends up
being cheaper than disk too.
             </td></tr><tr><td><b>Speaker Bio</b></td><td>
Dr. Marco Sanvido holds a Dipl.-Ing. degree (1996) and a Dr.techn. degree (2002) in Computer
Science from the Swiss Federal Institute of Technology in Zürich, Switzerland (ETHZ).
He was a co-founder of weControl, an ETHZ spin-off, where he developed low-power and
real-time embedded systems for autonomous flying vehicles. He was a postdoctoral
researcher in Computer Science at the University of California at Berkeley from 2002 to 2004,
and thereafter he worked on virtualization at VMware. In 2005 he then became a researcher
at Hitachi Global Storage Technologies, where he worked on hard disk drive and solid state drive
architectures. Since 2010 Marco joined Pure Storage as a Principal Software Engineer.
             </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Jan. 25, 2013</div></td><td><div class="eventInfor">SPEAKER: Silvius Rus (Quantcast)</div><div class="eventInfor">Petabyte Scale Data Processing at Quantcast</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99211");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99211" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Jan. 25, 2013 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Silvius Rus (Quantcast)</td></tr><tr><td><b>Title</b></td><td>Petabyte Scale Data Processing at Quantcast</td></tr><tr><td><b>Abstract</b></td><td>The talk will present the big data storage, processing and query systems
in production at Quantcast.  We receive up to 50 TB of new data every day,
respond to 500,000 events per second, process up to 30 PB per day and
store tens of petabytes of data.  We have implemented our own MapReduce
software stack that scales better and has significantly lower resource






requirements than Hadoop. The QFS file system is available open source
at https://github.com/quantcast/qfs/wiki.
			</td></tr><tr><td><b>Speaker Bio</b></td><td>Silvius Rus leads Big Data Platforms at Quantcast.  He directs, manages
and participates in the development of cluster language runtimes (SQL,
Sawzall), petabyte scale map-reduce, interactive big data analytics,
cluster resource management, distributed file systems and large scale
realtime processing.  Before Quantcast he was at Google working on Gmail
load balancing across datacenters, parallel memory allocation performance,
server performance and C++ compiler and library optimization.
Silvius holds a PhD in computer science from Texas A and M University,
where he worked on full program optimization based on hybrid
(static and dynamic) analysis of memory reference patterns.</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Jan. 18, 2013 (Special Time)</div></td><td><div class="eventInfor">SPEAKER: Joe Hellerstein (UC Berkeley)</div><div class="eventInfor">Keep CALM and Query On</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99210");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99210" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Jan. 18, 2013 (Special Time) 11 am</td></tr><tr><td><b>Location</b></td><td>DBH 6011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Joe Hellerstein (UC Berkeley)</td></tr><tr><td><b>Title</b></td><td>Keep CALM and Query On</td></tr><tr><td><b>Abstract</b></td><td>

Any modern software system of note has two key characteristics: it is a distributed system, and it manages significant amounts of data.  As a result, the topic of distributed data consistency has become a key problem in the engineering of modern software systems.  Conventional distributed systems wisdom dictates that perfect consistency is too expensive to guarantee in general, and consistency mechanisms—if they are used at all—should be reserved for infrequent, small-scale, mission-critical tasks.   Like most design maxims, these ideas are not so easy to translate into practice; all kinds of unavoidable tactical questions pop up.  For example:

 • Exactly where in my multifaceted system is loose consistency “good enough” to meet application needs?
 • How do I know that my “mission-critical” software isn’t tainted by my “best effort” components?
 • How do I ensure that my design maxims are maintained as software and developer teams evolve?

Until recently, answers to these questions have been more a matter of folklore than mathematics.

In this talk, I will describe the CALM Theorem, which links Consistency And Logical Monotonicity, and discuss how it can inform distributed software development.  I'll also describe Bloom, a "disorderly" distributed programming language developed in my group.  Bloom admits a form of automated CALM analysis, which enables a compiler to answer questions like the ones above.    

Time permitting, I will also point out some additional results from my two main research projects: the BOOM project on large-scale system development, and the d^p project on human interaction in the data analysis lifecycle.

             </td></tr><tr><td><b>Speaker Bio</b></td><td>
Joseph M. Hellerstein is a Chancellor's Professor of Computer Science at the University of California, Berkeley, whose research focuses on data-centric systems and the way they drive computing. A Fellow of the ACM, his work has been recognized via awards including an Alfred P. Sloan Research Fellowship, MIT Technology Review's TR10 and TR100 lists, Fortune Magazine's "Smartest in Tech" list, and two ACM-SIGMOD "Test of Time" awards. He has led a number of influential open source projects, including Bloom, MADlib, Telegraph, and TinyDB.

In 2012, Joe co-founded Trifacta, Inc, which develops productivity software for data analysts.
             </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Dec. 14, 2012</div></td><td><div class="eventInfor">SPEAKER: Bijit Hore (UCI ISG)</div><div class="eventInfor">Hide-and-Seek in the cloud: How to securely store and query your data in untrusted environments</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99203");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99203" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Dec. 14, 2012 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Bijit Hore (UCI ISG)</td></tr><tr><td><b>Title</b></td><td>Hide-and-Seek in the cloud: How to securely store and query your data in untrusted environments</td></tr><tr><td><b>Abstract</b></td><td>Security and privacy of data is a major concern for organizations (and many individuals) that use cloud-based services to cater to their IT needs. This is cited as the central reason why many federal, healthcare, and financial organizations have  not embraced cloud computing in a major way in spite of its many benefits. In this talk we consider the central problem of "data confidentiality", that arises while storing sensitive data in the cloud. While data encryption is an obvious solution for ensuring confidentiality, standard algorithms like AES make the data unusable in the cloud. For example, keyword search or database queries cannot be evaluated against the encrypted data. Over the past decade, many new schemes have been developed, that admit a variety of computations directly on the encrypted representation.  We give a brief overview of some of the important techniques proposed in this arena, specifically, for evaluating keyword-match and range queries. Finally, we describe our own contributions to this area and conclude with a discussion about open problems and future directions.</td></tr><tr><td><b>Speaker Bio</b></td><td>
             </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Dec. 7, 2012</div></td><td><div class="eventInfor">SPEAKER: </div><div class="eventInfor">No Seminar</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99202");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99202" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Dec. 7, 2012 3 pm</td></tr><tr><td><b>Location</b></td><td></td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td></td></tr><tr><td><b>Title</b></td><td>No Seminar</td></tr><tr><td><b>Abstract</b></td><td>
			No ISG seminar.  Leaving this time free so that ISG affiliates can attend today's ICS Trends in Society and Information Technology talk (see www.ics.uci.edu/trends).             </td></tr><tr><td><b>Speaker Bio</b></td><td>
             </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Nov. 30, 2012</div></td><td><div class="eventInfor">SPEAKER: Peter Bailis (UC Berkeley)</div><div class="eventInfor">Probabilistically Bounded Staleness for Practical Partial Quorums</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99200");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99200" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Nov. 30, 2012 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Peter Bailis (UC Berkeley)</td></tr><tr><td><b>Title</b></td><td>Probabilistically Bounded Staleness for Practical Partial Quorums</td></tr><tr><td><b>Abstract</b></td><td>Data store replication results in a fundamental trade-off between operation latency and data consistency. In this talk, we examine this trade-off in the context of quorum-replicated data stores. Under partial, or non-strict quorum replication, a data store waits for responses from a subset of replicas before answering a query, without guaranteeing that read and write replica sets intersect. As deployed in practice, these configurations provide only basic eventual consistency guarantees, with no limit to the recency of data returned. However, anecdotally, partial quorums are often “good enough” for practitioners given their latency benefits.

We explain why partial quorums are regularly acceptable in practice, analyzing both the staleness of data they return and the latency benefits they offer. We introduce Probabilistically Bounded Staleness (PBS) consistency, which provides expected bounds on staleness with respect to both versions and wall clock time. We derive a closed-form solution for versioned staleness as well as model real-time staleness for representative Dynamo-style systems under internet-scale production workloads. Using PBS, we measure the latency-consistency trade-off for partial quorum systems. We quantitatively demonstrate how and why eventually consistent systems frequently return consistent data within tens of milliseconds while offering significant latency benefits.

This is joint work with Shivaram Venkataraman, Mike Franklin, Joe Hellerstein, and Ion Stoica at UC Berkeley. An earlier version of this work appeared at VLDB 2012 (selected for "Best of VLDB 2012"), and an implementation of PBS is slated for release in Cassandra 1.2.0. Demo: http://pbs.cs.berkeley.edu/#demo</td></tr><tr><td><b>Speaker Bio</b></td><td>Peter Bailis is a graduate student in Computer Science at UC Berkeley, where he works closely with Joe Hellerstein, Ion Stoica, and Ali Ghodsi. He currently studies distributed systems, with a particular focus on distributed consistency models. Peter received his A.B. from Harvard College in 2011, where he worked with Margo Seltzer and Matt Welsh and was a 2011 CRA Outstanding Undergraduate Researcher. He is the recipient of the NSF Graduate Research Fellowship and the Berkeley Fellowship for Graduate Study and is a co-founder of @TinyToCS, the premiere journal for Computer Science research of 140 characters or less.

             </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Nov. 9, 2012</div></td><td><div class="eventInfor">SPEAKER: Chaitan Baru (SDSC at UCSD)</div><div class="eventInfor">Data Initiatives at SDSC</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99197");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99197" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Nov. 9, 2012 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Chaitan Baru (SDSC at UCSD)</td></tr><tr><td><b>Title</b></td><td>Data Initiatives at SDSC</td></tr><tr><td><b>Abstract</b></td><td>
			As a data-oriented supercomputer center, SDSC is engaged in a variety of
activities that support data-intensive computing and big data, from
research and development to fielding production systems, and enabling end
applications. This talk will provide an overview of several data
activities at SDSC including the Gordon supercomputer; data intensive
applications on Gordon; the SDSC Cloud, with Globus Online interface for
OpenStack; and new initiatives such as the Center for Large-scale Data
Systems Research (CLDS). We will present two CLDS programs, one on
establishing industry standards for Big Data Benchmarking and another on
Data Growth and Data Value. A new initiative targeted at long-tail
scientific data, motivated partly by needs identified by the NSF EarthCube
initiative and by the challenges faced by a typical research university,
will also be presented. There are many opportunities for joint
collaborations and student projects across these initiatives.
			</td></tr><tr><td><b>Speaker Bio</b></td><td>
Chaitan Baru, is Associate Director for Data Initiatives at the San
Diego Supercomputer Center Director, UC San Diego, where he also directs
the Center for Large-scale Data Systems research (CLDS). His technical
interests are in the areas of scientific
data management, large-scale data systems, data integration, data
analytics, and parallel database systems. He has been involved in
cyberinfrastructure projects across a range of science disciplines, e.g.
earth sciences, ecological sciences, hydrology, earthquake engineering,
 biomedical sciences, and others. He is PI of the OpenTopography project;
coordinator of the Data Discovery, Mining, and Access community group for
the NSF EarthCube project; and Chair, Coordinating Committee for Big Data
Benchmarking. Before joining SDSC 16 years ago, Baru led one of the
development teams at IBM for DB2 Parallel Edition (a shared-nothing
database engine). Prior to that, he was on the faculty of the EECS Dept,
University of Michigan. Baru has a B.Tech. in Electronics Engineering from
IIT Madras, and an ME and PhD in Electrical Engineering from the
University of Florida, Gainesville.
             </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Nov. 2, 2012</div></td><td><div class="eventInfor">SPEAKER: </div><div class="eventInfor">No Seminar</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99196");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99196" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Nov. 2, 2012 3 pm</td></tr><tr><td><b>Location</b></td><td></td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td></td></tr><tr><td><b>Title</b></td><td>No Seminar</td></tr><tr><td><b>Abstract</b></td><td>
			No ISG seminar.  Leaving this time free so that ISG affiliates can attend today's ICS Trends in Society and Information Technology talk (see www.ics.uci.edu/trends).             </td></tr><tr><td><b>Speaker Bio</b></td><td>
             </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Oct. 26, 2012</div></td><td><div class="eventInfor">SPEAKER: Yingyi Bu (ISG PhD student)</div><div class="eventInfor">Pregelix: Think Like a Vertex, Scale like Spandex</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99195");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99195" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Oct. 26, 2012 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Yingyi Bu (ISG PhD student)</td></tr><tr><td><b>Title</b></td><td>Pregelix: Think Like a Vertex, Scale like Spandex</td></tr><tr><td><b>Abstract</b></td><td>
Recently, there are more and more demands for analyzing Big Graph Data.  For example,  the scale of the world wide web keeps expanding to billions of web pages and hyper-links, the key social network sites like Facebook, LinkedIn, Twitter all have a rapidly growing gigantic social graph, and the biology science people assemble genomes from huge de Bruijn graphs.  To analyze such Big graphs requires a system which can not only scale out to hundreds or thousands of machines, but also do the computation very efficiently.  In this talk, I will introduce the Pregelix system,  which supports easy programming and scales to large commodity machine clusters. I will first illustrate the programming model -- application programmers need zero knowledge of the parallel/distributed system,  but just "think like a vertex" and write a couple of functions that encapsulate the logic for what one graph vertex does.  After that, I will detail the shining internals of Pregelix,  including the system architecture,  the scalable dataflow runtime,  the execution strategies, and the out-of-core support.  Then,  I will walk through a few examples built on top of Pregelix, such as PageRank and connected components.  Finally I will demonstrate our performance numbers and conclude the talk. (Truth in lending disclosure: the programming model and API were shamelessly borrowed from Google's Pregel graph analytics platform, hence the name:-))
             </td></tr><tr><td><b>Speaker Bio</b></td><td>
 Yingyi Bu is a PhD student in the ISG group of UC Irvine.  He is working on the ASTERIX project that aims at an open source data-intensive computing platform, with new technologies for ingesting, storing, managing, indexing, querying, analyzing, and subscribing intensive semi-structured data.  Within the project, Yingyi has been working on the data-model independent algebra/optimization layer,  the ASTERIX query optimizer, and the Pregelix system.
             </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Oct. 19, 2012</div></td><td><div class="eventInfor">SPEAKER: David Lomet (Microsoft Research)</div><div class="eventInfor">The Bw-Tree: A B-tree for New Hardware Platforms</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99194");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99194" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Oct. 19, 2012 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>David Lomet (Microsoft Research)</td></tr><tr><td><b>Title</b></td><td>The Bw-Tree: A B-tree for New Hardware Platforms</td></tr><tr><td><b>Abstract</b></td><td>The emergence of new hardware and platforms has led to reconsideration of how data management 
            	systems are designed. However, certain basic functions such as key indexed access to records remain essential. 
            	While we exploit the common architectural layering of prior systems, we make radically new design decisions about each layer. 
            	Our new form of B-tree, called the Bw-tree achieves its very high performance via a latch-free approach that effectively 
            	exploits the processor caches of modern multi-core chips. Our storage manager uses a unique form of log structuring that 
            	blurs the distinction between a page and a record store and works well with flash storage. This paper describes the architecture 
            	and algorithms for the Bw-tree, focusing on the main memory aspects. The paper includes results of our experiments that demonstrate that this fresh approach produces outstanding performance.
            </td></tr><tr><td><b>Speaker Bio</b></td><td>
            	David Lomet has been a principal researcher managing the Microsoft Research Database Group at Microsoft Research since 1995. Earlier, he spent seven and a half years at Digital Equipment Corporation. He has been at IBM Research in Yorktown and a Professor at Wang Institute. Dr. Lomet spent a sabbatical at University of Newcastle-upon-Tyne working with Brian Randell. He has a Computer Science Ph.D from the University of Pennsylvania.
Dr. Lomet has done research and product development in architecture, programming languages, and distributed systems. His primary interest is database systems, focusing on access methods, concurrency control, and recovery. He is one of the inventors of the transaction concept and is an author of over 100 papers and 45 patents. Two papers won SIGMOD "best paper" awards. He received the 2010 SIGMOD Contributions Award for his work as editor-in-chief of the Data Engineering Bulletin since 1992.
Dr. Lomet has served on program committees, including SIGMOD, PODS, VLDB, and ICDE. He was ICDE'2000 PC co-chair and VLDB 2006 PC core chair. He is a member of the ICDE Steering Committee and VLDB Board. He is a past editor of ACM TODS and the VLDB Journal. Dr. Lomet is IEEE Golden Core Member and has received IEEE Outstanding Contribution and Meritorious Service Awards. Dr. Lomet is a Fellow of the ACM, IEEE, and AAAS.
            </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Oct. 5, 2012</div></td><td><div class="eventInfor">SPEAKER: Thomas Bodner (TU Berlin)</div><div class="eventInfor">A Taxonomy of Platforms for Analytics on Big Data (Stratosphere talk series 5)</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99193");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99193" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Oct. 5, 2012 4:30 pm - 5pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Thomas Bodner (TU Berlin)</td></tr><tr><td><b>Title</b></td><td>A Taxonomy of Platforms for Analytics on Big Data (Stratosphere talk series 5)</td></tr><tr><td><b>Abstract</b></td><td>
Within the past few years, industrial and academic organizations designed a wealth of systems for data-intensive analytics including MapReduce, SCOPE/Dryad, ASTERIX, Stratosphere, Spark, and many others. These systems are being applied to new applications from diverse domains other than (traditional) relational OLAP, making it difficult to understand the tradeoffs between them and the workloads for which they were built. We present a taxonomy of existing system stacks based on their architectural components and the design choices made related to data processing and programmability to sort this space. We further demonstrate a web repository for sharing Big Data analytics platform information and use cases. The repository enables researchers and practitioners to store and retrieve data and queries for their use case, and to easily reproduce experiments from others on different platforms, simplifying comparisons.
             </td></tr><tr><td><b>Speaker Bio</b></td><td>
Thomas Bodner is a second year Master's student in the computer science department at the Technische Universität Berlin working in the Database Systems and Information Management (DIMA) group on the Stratosphere project. He received his B.S. from the University of Cooperative Education at Stuttgart. In the course of his studies, Thomas Bodner studied abroad at University of California, Irvine and Royal Melbourne Institute of Technology. He worked as an intern at the IBM Almaden Research Center and the IBM Böblingen Laboratory. His research interests include benchmarking of and query optimization for Big Data analytics systems.
             </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Oct. 5, 2012</div></td><td><div class="eventInfor">SPEAKER: Alexander Alexandrov</div><div class="eventInfor">Generating a Myriad of Atoms in the Blink of an Eye (Stratosphere talk series 4)</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99192");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99192" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Oct. 5, 2012 4 pm - 4:30pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Alexander Alexandrov</td></tr><tr><td><b>Title</b></td><td>Generating a Myriad of Atoms in the Blink of an Eye (Stratosphere talk series 4)</td></tr><tr><td><b>Abstract</b></td><td>
 Data from real-world applications is regarded as the golden standard for database systems evaluation. Unfortunately, finding appropriate real-world datasets is often hard due to various privacy-related constraints. To overcome this problem, we developed the Myriad Parallel Data Generator Toolkit - a generic toolkit for declarative specification of synthetic data generators that provides built-in parallelization support for the specified data generation programs. In this talk, I will motivate and present the main technical challenges solved by the highly-parallel execution model of the Myriad Toolkit. In addition, to demonstrate the usability of the toolkit, I will also give a brief overview of the supported data generator specification syntax and explain how different statistical constraints for the generated data can be implemented using the appropriate combination of specification routines.
             </td></tr><tr><td><b>Speaker Bio</b></td><td>
Alexander Alexandrov is a research associate at the Database Systems and Information Management research group at the Technische Universität Berlin. Before moving to Berlin for a Master in Computer Science at TU Berlin, he received his Bachelor of Science in Software and Internet Technologies at the University of Mannheim. Alexander has been working on the Stratosphere project both as student and research assistant since 2009. His research interests include data generation, evaluation, and query optimization for large-scale parallel batch processing systems with partial operator semantics.
             </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Oct. 5, 2012</div></td><td><div class="eventInfor">SPEAKER: Stephan Ewen (TU Berlin)</div><div class="eventInfor">Spinning Fast Iterative Data Flows (Stratosphere talk series 3)</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99191");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99191" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Oct. 5, 2012 3:30 pm - 4pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Stephan Ewen (TU Berlin)</td></tr><tr><td><b>Title</b></td><td>Spinning Fast Iterative Data Flows (Stratosphere talk series 3)</td></tr><tr><td><b>Abstract</b></td><td>
 Parallel data flow systems are a central part of most analytic pipelines for big data. The iterative nature of many analysis and machine learning algorithms, however, is still a challenge for current systems. While certain types of bulk iterative algorithms are supported by novel data flow frameworks, these systems cannot exploit computational dependencies present in many algorithms, such as graph algorithms. As a result, these algorithms are inefficiently executed and have led to specialized systems based on other paradigms, such as message passing or shared memory. We propose a method to integrate "incremental iterations", a form of workset iterations, with parallel data flows. After showing how to integrate bulk iterations into a dataflow system and its optimizer, we present an extension to the programming model for incremental iterations. The extension alleviates for the lack of mutable state in dataflows and allows for exploiting the "sparse computational dependencies" inherent in many iterative algorithms. The evaluation of a prototypical implementation shows that those aspects lead to up to two orders of magnitude speedup in algorithm runtime, when exploited. In our experiments, the improved dataflow system is highly competitive with specialized systems while maintaining a transparent and unified data flow abstraction.
             </td></tr><tr><td><b>Speaker Bio</b></td><td>
Stephan Ewen is a research associate at the department for Database Systems and Information Management (DIMA) at the Technische Universität Berlin. He is working on the Stratosphere Project that aims at creating a versatile and efficient analytics engine for deep analysis of Big Data on cloud platforms. Within the project, Stephan works on the system's data flow programming abstraction, the data flow optimization and the parallel runtime system. Prior to joining the DIMA group, Stephan completed the "Applied Computer Science" program at the University of Cooperative Education Stuttgart jointly with IBM Germany and got his Diploma from the University of Stuttgart. In the course of his studies, Stephan Ewen worked, among others, for the IBM Almaden Research Centre and the IBM Development Laboratory Böblingen.
             </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Oct. 5, 2012</div></td><td><div class="eventInfor">SPEAKER: Kostas Tzoumas (TU Berlin)</div><div class="eventInfor">Query Optimization with MapReduce Functions (Stratosphere talk series 2)</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99190");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99190" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Oct. 5, 2012 3 pm - 3:30 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Kostas Tzoumas (TU Berlin)</td></tr><tr><td><b>Title</b></td><td>Query Optimization with MapReduce Functions (Stratosphere talk series 2)</td></tr><tr><td><b>Abstract</b></td><td>
Many systems for big data analytics employ a data flow programming abstraction to define parallel data processing tasks. In this setting, custom operations expressed as user-defined functions are very common. We address the problem of performing data flow optimization at this level of abstraction, where the semantics of operators are not known. Traditionally, query optimization is applied to queries with known algebraic semantics. In this work, we find that a handful of properties, rather than a full algebraic specification, suffice to establish reordering conditions for data processing operators. We show that these properties can be accurately estimated for black box operators using a shallow static code analysis pass based on reverse data and control flow analysis over the general-purpose code of their user-defined functions. We design and implement an optimizer for parallel data flows that does not assume knowledge of semantics or algebraic properties of operators. Our evaluation confirms that the optimizer can apply common rewritings such as selection reordering, bushy join order enumeration, and limited forms of aggregation push-down, hence yielding similar rewriting power as modern relational DBMS optimizers. Moreover, it can optimize the operator order of non-relational data flows, a unique feature among today's systems.
             </td></tr><tr><td><b>Speaker Bio</b></td><td>
 Kostas Tzoumas is a postdoctoral researcher co-leading the Stratosphere research project at the Technische Universität Berlin. He received his PhD from Aalborg University in 2011 with a thesis on discovering and exploiting correlations for query optimization. He was a visiting researcher at the University of Maryland, College Park, and an intern at Microsoft Research. He received a Diploma in Electrical and Computer Engineering from the National Technical University of Athens in 2007. His research interests are centered around systems for data analytics, including query processing and optimization in massively parallel environments.
             </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Oct. 5, 2012</div></td><td><div class="eventInfor">SPEAKER: Volker Markl (TU Berlin)</div><div class="eventInfor">The Current State of the Stratosphere (Stratosphere talk series 1)</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99189");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99189" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Oct. 5, 2012 3 pm - 5pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Volker Markl (TU Berlin)</td></tr><tr><td><b>Title</b></td><td>The Current State of the Stratosphere (Stratosphere talk series 1)</td></tr><tr><td><b>Abstract</b></td><td>
            Introduction to the Stratosphere system.
			</td></tr><tr><td><b>Speaker Bio</b></td><td>
            Volker Markl is a Full Professor and Chair of the Database Systems and Information Management (DIMA) group at the Technische Universität Berlin (TU-Berlin). Prior to joining TU-Berlin, Dr. Markl lead a research group at FORWISS, the Bavarian Research Center for Knowledge-based Systems in Munich, Germany, and was a Research Staff member and Project Leader at the IBM Almaden Research Center in San Jose, California, USA. His research interests include: information as a service, new hardware architectures for information management, information integration, autonomic computing, query processing, query optimization, data warehousing, electronic commerce, and pervasive computing. Volker has presented over 100 invited talks in numerous industrial settings and at major conferences and research institutions worldwide. He has authored and published more than 50 research papers at world-class scientific venues. Volker regularly serves as member and chair for program committees of major international database conferences. He also is a member of the Board of Trustees of the VLDB Endowment. Volker has 5 patent awards, and he has submitted over 20 invention disclosures to date. Over the course of his career, he has garnered many prestigious awards, including the European Information Society and Technology Prize, an IBM Outstanding Technological Achievement Award, an IBM Shared University Research Grant, an HP Open Innovation Award, and the Pat Goldberg Memorial Best Paper Award.
            </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Jun. 8, 2012</div></td><td><div class="eventInfor">SPEAKER:  Kerim Yasin Oktay and Bijit Hore (ISG)</div><div class="eventInfor">CloudProtecti and Risk-Aware Workload Distribution in Hybrid Clouds</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99162");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99162" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Jun. 8, 2012 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td> Kerim Yasin Oktay and Bijit Hore (ISG)</td></tr><tr><td><b>Title</b></td><td>CloudProtecti and Risk-Aware Workload Distribution in Hybrid Clouds</td></tr><tr><td><b>Abstract</b></td><td>
            In this talk, we describe the CloudProtect system from the recently accepted paper in IEEE Cloud 2012. The CloudProtect middleware empowers users to encrypt sensitive data stored within various cloud applications. However, most web applications require data in plaintext for implementing the various functionalities and in general, do not support encrypted data management. Therefore, CloudProtect strives to carry out the data transformations (encryption/decryption) in a manner that is transparent to the application, i.e., preserves all functionalities of the application, including those that require data to be in plaintext. Additionally, CloudProtect allows users flexibility in trading off performance for security in order to let them optimally balance their privacy needs and usage-experience.
            This paper explores an efficient and secure mechanism
            to partition computations across public and private
            machines in a hybrid cloud setting. We propose a principled
            framework for distributing data and processing in a hybrid
            cloud that meets the conflicting goals of performance, sensitive
            data disclosure risk and resource allocation costs. The proposed
            solution is implemented as an add-on tool for a Hadoop and
            Hive based cloud computing infrastructure. Our experiments
            demonstrate that the developed mechanism can lead to a
            major performance gain by exploiting both the hybrid cloud
            components without violating any pre-determined public cloud
            usage constraints.
            </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Jun. 1, 2012</div></td><td><div class="eventInfor">SPEAKER: Ken Slocum (UCSD)</div><div class="eventInfor">Scalable Lineage Capture for DISC</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99161");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99161" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Jun. 1, 2012 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Ken Slocum (UCSD)</td></tr><tr><td><b>Title</b></td><td>Scalable Lineage Capture for DISC</td></tr><tr><td><b>Abstract</b></td><td>
            	Scale-out data processing architectures enable sophisticated ``big data''
analytics, but understanding and debugging multi-step dataflows that ingest
large volumes of data remains a fundamental challenge.  We are building a
system called Newt, a scalable architecture for capturing fine-grain,
record-level provenance from these data-intensive scalable compute (DISC)
systems in a generic manner.  Developers leverage a unique API to
instrument these systems, actively capturing fine-grain lineage across
multi-step, perhaps non-relational, transformations.   We report on our 
experiences instrumenting Hyracks and Hadoop, and find that Newt's capture 
incurs 16-26% time overheads for the PigMix benchmark and a 14% overhead 
on a complex 145-stage de novo genomic assembler.
            </td></tr><tr><td><b>Speaker Bio</b></td><td>
            	Ken Yocum is an associate research scientist in the Department of Computer
Science at UC San Diego where he runs the Synoptic Systems Lab.  
While he once worked on high-speed networking (briefly holding the land-speed record for gigabit TCP), he has since become enamored with the myriad systems challenges of "big data" processing and software-defined networks.  He received his Ph.D. from Duke University, and
his B.S from Stanford.  When he's not working, he enjoys his children,
cycling, and going to the race track.
            </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">May. 29, 2012 (Special Time)</div></td><td><div class="eventInfor">SPEAKER: Murali Mani (University of Michigan, Flint)</div><div class="eventInfor"> Algebraic Manipulation of Encrypted Databases</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99160");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99160" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>May. 29, 2012 (Special Time) 12 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Murali Mani (University of Michigan, Flint)</td></tr><tr><td><b>Title</b></td><td> Algebraic Manipulation of Encrypted Databases</td></tr><tr><td><b>Abstract</b></td><td>
            	Can we improve on the work that received the 10 year ACM SIGMOD test of time award? In this talk, we will outline our preliminary approach at doing the entire query processing on the server/cloud, while the client is involved only with encryption, and decryption. Our work is based on Craig Gentry's revolutionary recent work on fully homomorphic encryption (first such scheme was published in 2009). We utilize Craig Gentry's scheme for query processing, while maintaining the algebraic framework that is a key aspect of database systems. There are several avenues for future investigation: exploring physical implementations for algebraic operators beyond what we have investigated; exploring query optimization and utilization of indexes; exploring feasibility of Craig Gentry's fully homomorphic encryption in the context of databases as some aspects of his scheme are very time consuming.
            </td></tr><tr><td><b>Speaker Bio</b></td><td>
            	Murali Mani finished his PhD in Computer Science from UCLA in 2003. Since then, he has worked at WPI, and is currently an assistant professor at University of Michigan, Flint. His areas of interest are database systems, and his significant projects have been on event stream processing, processing of XML streaming data, provenance metadata management, and data modeling using XML schemas. His research on XML stream processing and provenance have been supported by NSF.
            </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">May. 25, 2012</div></td><td><div class="eventInfor">SPEAKER: Jimmy Lin (Twitter)</div><div class="eventInfor">Flexibility without Anarchy: Analytics Infrastructure at Twitter</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99159");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99159" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>May. 25, 2012 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Jimmy Lin (Twitter)</td></tr><tr><td><b>Title</b></td><td>Flexibility without Anarchy: Analytics Infrastructure at Twitter</td></tr><tr><td><b>Abstract</b></td><td>The data analytics infrastructure at Twitter supports a myriad of technologies: Hadoop, Pig (with Python and JRuby), 
            	Cascading/Scalding, HBase, MySQL, Vertica, and ZooKeeper. Our philosophy is to let developers and data scientists use whatever 
            	tools they are most comfortable with, while allowing individual components to be weaved together into complex analytic tapestries. 
            	Managing complex workflows that cross language boundaries (e.g. Java vs. Pig vs. Scala) as well as architectures with significant 
            	impedance mismatches (e.g., Hadoop vs. Vertica) has been and continues to remain a significant challenge. 
            	In this talk, I'll detail some of these issues and our present solutions.
            </td></tr><tr><td><b>Speaker Bio</b></td><td>
            	 Jimmy Lin is a visiting scientist at Twitter, currently on leave from the University of Maryland. His current research focuses on scalable algorithms for data analytics, particularly on text and graph data. At Twitter, he works on services designed to surface relevant content for users and the distributed infrastructure that supports mining relevance signals from massive amounts of data.
            </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">May. 18, 2012</div></td><td><div class="eventInfor">SPEAKER: Raman Grover (ISG Ph.D. student)</div><div class="eventInfor">ASTERIX: Scalable Warehouse-Style Web Data Integration</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99151");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99151" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>May. 18, 2012 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Raman Grover (ISG Ph.D. student)</td></tr><tr><td><b>Title</b></td><td>ASTERIX: Scalable Warehouse-Style Web Data Integration</td></tr><tr><td><b>Abstract</b></td><td>
            		A growing wealth of digital information is being generated on a
daily basis in social networks, blogs, online communities, etc. Organizations
and researchers in a wide variety of domains recognize
that there is tremendous value and insight to be gained by
warehousing this emerging data and making it available for querying,
analysis, and other purposes. This new breed of “Big Data”
applications poses challenging requirements against data management
platforms in terms of scalability, flexibility, manageability,
and analysis capabilities. At UC Irvine, we are building a nextgeneration
database system, called ASTERIX, in response to these
trends. We present ongoing work that approaches the following
questions: How does data get into the system? What primitives
should we provide to better cope with dirty/noisy data? How can
we support efficient data analysis on spatial data? Using real examples,
we show the capabilities of ASTERIX for ingesting data via
feeds, supporting set-similarity predicates for fuzzy matching, and
answering spatial aggregation queries.
            </td></tr><tr><td><b>Speaker Bio</b></td><td>
            </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">May. 11, 2012</div></td><td><div class="eventInfor">SPEAKER: Inci Cetindil (ISG Ph.D. student)</div><div class="eventInfor">Analysis of Instant Search Query Logs</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99150");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99150" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>May. 11, 2012 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Inci Cetindil (ISG Ph.D. student)</td></tr><tr><td><b>Title</b></td><td>Analysis of Instant Search Query Logs</td></tr><tr><td><b>Abstract</b></td><td>Instant search is a new search paradigm that shows results as
a user types in a query. It has become increasingly popular
in recent years due to its simplicity and power. In an instant-
search system, every keystroke from a user triggers a new
request to the server. Therefore, its log has a richer content
than that of a traditional search system, and previous log
analysis research is not applicable to this type of log. In
this study, we present the problem of analyzing the query
log of an instant-search system. We propose a classification
scheme for user typing behaviors. We also compare the log
of an instant-search system and that of a traditional search
system on the same data. The results show that on a people
directory search system, instant search can typically save
2 seconds per search, reduce the typing effort by showing
the results with fewer characters entered, and increase the
success rate.
            </td></tr><tr><td><b>Speaker Bio</b></td><td>
            </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">April. 27, 2012</div></td><td><div class="eventInfor">SPEAKER: Afsin Akdogan (University of Southern California)</div><div class="eventInfor">Voronoi-based Geospatial Query Processing with MapReduce</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99144");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99144" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>April. 27, 2012 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Afsin Akdogan (University of Southern California)</td></tr><tr><td><b>Title</b></td><td>Voronoi-based Geospatial Query Processing with MapReduce</td></tr><tr><td><b>Abstract</b></td><td>
            		Geospatial queries (GQ) have been used in a wide variety of applications such as decision support systems, profile-based marketing, bioinformatics and GIS. Most of the existing query-answering approaches assume non parallel processing on a single machine although GQs are intrinsically parallelizable. There are some approaches that have been designed for parallel databases and cluster systems; however, these only apply to the systems with limited parallel processing capability, far from that of cloud-based platforms. In this study, I present the problem of parallel geospatial query processing with MapReduce programming model.  Our approach creates a spatial index, Voronoi diagram, for given data points in 2D space and enables efficient processing of GQs. We evaluated the performance of our proposed techniques and correspondingly compared them with their closest related work while varying the number of employed nodes.
        		</td></tr><tr><td><b>Speaker Bio</b></td><td>Afsin Akdogan received his master’s degree in computer science from Cornell University in 2009. He received a best paper award in IEEE Cloud Computing Technology and Science conference in 2010. He has also interned at Yahoo. He is currently working towards his Ph.D. degree in computer science at the University of Southern California and his research focuses on cloud computing, parallel data processing languages and geo-spatial databases.</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">April. 20, 2012</div></td><td><div class="eventInfor">SPEAKER: Leila Jalali (Ph.D. student in ISG)</div><div class="eventInfor">A Reflective Approach to Synchronization for Consistent Multisimulations</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99143");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99143" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>April. 20, 2012 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Leila Jalali (Ph.D. student in ISG)</td></tr><tr><td><b>Title</b></td><td>A Reflective Approach to Synchronization for Consistent Multisimulations</td></tr><tr><td><b>Abstract</b></td><td>In this talk, I consider the challenge of designing a framework that supports the integration of multiple existing autonomous simulation models into an integrated simulation environment (multisimulation). In particular, I focus on solutions for synchronization problem in multisimulation to orchestrate consistent information flow through multiple simulator: (1) a transaction-based approach to modeling the synchronization problem in multisimulations by mapping it to a problem similar to multidatabase concurrency; we express multisimulation synchronization as a scheduling problem where the goal is to generate “correct schedules” for time advancement and data exchange across simulators that meets the dependencies without loss of concurrency, (2) a hybrid scheduling strategy which adapts itself to the “right” level of pessimism/optimism based on the state of the execution and underlying dependencies, and (3) relaxation model for dependencies which guarantee bounded violation of consistency to support higher levels of concurrency. We also develop two key optimizations: (a) efficient checkpointing/rollback techniques, and (b) relaxation model for dependencies which guarantee bounded violation of consistency to support higher levels of concurrency. We evaluate our proposed techniques via a detailed case study from the emergency response domain by integrating three disparate simulators – a fire simulator (CFAST), an evacuation simulator (Drillsim) and a communication simulator (LTEsim).
						</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">April. 13, 2012 (Special Time\Place)</div></td><td><div class="eventInfor">SPEAKER: Jennifer Widom (Stanford)</div><div class="eventInfor">Data-Centric Human Computation + From 100 Students to 100,000</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99142");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99142" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>April. 13, 2012 (Special Time\Place) 11 am</td></tr><tr><td><b>Location</b></td><td>DBH 6011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Jennifer Widom (Stanford)</td></tr><tr><td><b>Title</b></td><td>Data-Centric Human Computation + From 100 Students to 100,000</td></tr><tr><td><b>Abstract</b></td><td>This talk will have two completely independent parts -- one related to research and the other to education.
In the first part of the talk, I'll describe our ongoing research in leveraging human computation for tasks related to data. Human computation ("crowdsourcing") augments traditional computation with the use of human abilities to solve sub-problems that are difficult for computers, e.g., object or image comparisons, information extraction, relevance judgements, and data gathering. We are addressing two different types of data-centric human computation: (1) Fundamental algorithms, such as sorting, clustering, and data cleaning, in which the basic operations (e.g., compare, filter) are performed by humans. (2) A database-system like platform in which declarative queries are posed by users, and the system orchestrates a combination of stored and crowdsourced data to answer them. Common to both areas is the need to formalize and optimize new tradeoffs among latency (humans are much slower than computers), cost (humans require real money to perform tasks), and quality (humans are inaccurate and inconsistent).
In the second part of the talk, I'll describe my recent experience teaching introductory databases to 60,000 students. Admittedly only 25,000 of them submitted their homework, and a mere 6500 achieved a strong final score. But even with 6500 students, I more than quadrupled the total number of students I've taught in my entire 18-year academic career. I began by "flipping" the way I teach my Stanford course and, as a side-effect, making all components of the course freely available online. But the big inflection point came when I offered the online course in a structured fashion with a schedule, automatically-graded assignments and exams, and most importantly a worldwide community of students. I'll cover a variety of topics related to the massive online course, both logistical and social, while avoiding speculation on the future of higher education.
</td></tr><tr><td><b>Speaker Bio</b></td><td>Jennifer Widom is the Fletcher Jones Professor and Chair of the Computer Science Department at Stanford University. She received her Bachelor's degree from the Indiana University School of Music in 1982 and her Computer Science Ph.D. from Cornell University in 1987. She was a Research Staff Member at the IBM Almaden Research Center before joining the Stanford faculty in 1993. Her research interests span many aspects of nontraditional data management. She is an ACM Fellow and a member of the National Academy of Engineering and the American Academy of Arts and Sciences; she received the ACM SIGMOD Edgar F. Codd Innovations Award in 2007 and was a Guggenheim Fellow in 2000; she has served on a variety of program committees, advisory boards, and editorial boards.</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">March. 16, 2012 (Special Time\Place)</div></td><td><div class="eventInfor">SPEAKER: Cyrus Shahabi (USC)</div><div class="eventInfor">TransDec:
A Data-Driven Framework for Decision-Making in Transportation Systems</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99128");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99128" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>March. 16, 2012 (Special Time\Place) 11 am</td></tr><tr><td><b>Location</b></td><td>DBH 6011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Cyrus Shahabi (USC)</td></tr><tr><td><b>Title</b></td><td>TransDec:
A Data-Driven Framework for Decision-Making in Transportation Systems</td></tr><tr><td><b>Abstract</b></td><td>The vast amounts of transportation datasets (traffic flow, incidents, etc.) collected by various federal and
state agencies are extremely valuable in 1) real-time decision-making, planning, and management of the
transportation systems, and 2) conducting research to develop new policies to enhance the efficacy of the
transportation systems. In this talk, I will present our data-driven framework, dubbed TransDec (short
for Transportation Decision-Making), which enables real-time integration, visualization, querying, and
analysis of dynamic and archived transportation data. I will show that considering the large size of the
transportation data, variety of the data (different modalities and resolutions), and frequent changes of the
data, implementation of such a scalable system that allows for effective querying and analysis of both
archived and real-time data is an intrinsically challenging data management task. Subsequently, I will
focus on a route-planning problem where the weights on the road-network edges vary as a function of
time due to the variability of traffic congestion. I will show that naïve approaches to address this problem
are either inaccurate or slow, motivating the need for new solutions. Consequently, I will discuss our
initial approach to this problem and demonstrate its implementation within the TransDec framework.</td></tr><tr><td><b>Speaker Bio</b></td><td>Cyrus Shahabi is a Professor and the Director of the Information Laboratory (InfoLAB) at the Computer
Science Department and also the Director of the NSF's Integrated Media
Systems Center (IMSC) at the University of Southern California. He is also the
CTO and co-founder of a USC spin-off, Geosemble Technologies. He
received his B.S. in Computer Engineering from Sharif University of
Technology in 1989 and then his M.S. and Ph.D. Degrees in Computer
Science from the University of Southern California in May 1993 and
August 1996, respectively. He authored two books and more than hundred-
fifty research papers in the areas of databases, GIS and multimedia. Dr. Shahabi has received funding from several agencies such as NIJ, NSF, NASA, NIH, DARPA, AFRL,
and DHS as well as several industries such as Google, Microsoft, NCR, NGC, and Chevron. He was an
Associate Editor of IEEE Transactions on Parallel and Distributed Systems (TPDS) from 2004 to 2009.
He is currently on the editorial board of the VLDB Journal, IEEE Transactions on Knowledge and Data
Engineering (TKDE), ACM Computers in Entertainment and Journal of Spatial Information Science. He
is the founding chair of IEEE NetDB workshop and also the general co-chair of ACM GIS 2007, 2008
and 2009. He chaired the nomination committee of ACM SIGSPATIAL for the 2011-2014 terms. He
regularly serves on the program committee of major conferences such as VLDB, ACM SIGMOD, IEEE
ICDE, ACM SIGKDD, and ACM Multimedia. Dr. Shahabi is a recipient of the ACM Distinguished
Scientist award in 2009, the 2003 U.S. Presidential Early Career Awards for Scientists and Engineers
(PECASE), the NSF CAREER award in 2002, and the 2001 Okawa Foundation Research Grant for
Information and Telecommunications. He was the recipient of US Vietnam Education Foundation (VEF)
faculty fellowship award in 2011, an organizer of the 2011 National Academy of Engineering “Japan-
America Frontiers of Engineering” program, an invited speaker in the 2010 National Research Council
(of the National Academies) Committee on New Research Directions for the National Geospatial-
Intelligence Agency, and a participant in the 2005 National Academy of Engineering “Frontiers of
Engineering” program.</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">March. 9, 2012</div></td><td><div class="eventInfor">SPEAKER: Nga Dang (Ph.D. student in ISG)</div><div class="eventInfor">QuARES: A Quality-Aware Renewable Energy-driven Sensing Framework</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99127");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99127" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>March. 9, 2012 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Nga Dang (Ph.D. student in ISG)</td></tr><tr><td><b>Title</b></td><td>QuARES: A Quality-Aware Renewable Energy-driven Sensing Framework</td></tr><tr><td><b>Abstract</b></td><td> Mobile devices, such as smartphones and tablets, are getting increasingly popular, and continue to generate record-high amount of mobile data traffic. For example a recent Cisco report indicates that mobile data traffic will increase 39 times by 2015, while 66% of such boost is due to video traffic. Network capacity issue may be partially coped by deploying more cellular base stations, installing dedicated broadcast networks, or upgrading the cellular base stations to support 4G. However, these approaches all result in additional costs on new network infrastructure, and might not be fully compatible with existing
obile devices. Also, according to the report, the network capacity provided by cellular network providers is predicted to be only 10 time increasing by 2015, which implies that the above methods do not still meet the requirement for increasing mobile traffic. A better way is moving data to other networks to reduce heavy traffic in cellular networks. In our research, we study motivations and methods to offload part of mobile traffic from cellular networks to other networks such as WiFi or Ad Hoc, which are available in most modern smartphones. Such these methods are cheap, practical, and easily implemented.</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">March. 1, 2012 (Special Time\Place)</div></td><td><div class="eventInfor">SPEAKER: Archan Misra (Singapore Management University) </div><div class="eventInfor">Real-time Mobile Sensing/Analytics and the LiveLabs Experimentation Platform</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99126");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99126" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>March. 1, 2012 (Special Time\Place) 11 am</td></tr><tr><td><b>Location</b></td><td>DBH 4011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Archan Misra (Singapore Management University) </td></tr><tr><td><b>Title</b></td><td>Real-time Mobile Sensing/Analytics and the LiveLabs Experimentation Platform</td></tr><tr><td><b>Abstract</b></td><td>
            This talk explores the ongoing transformation of the mobile device into a combined “sensing and
analytics” platform, distinguished by two key features: a) efficient localized processing of sensor data
streams and b) localized coordination and distributed computation among a set of proximal mobile
nodes. I will first introduce the LiveLabs Experimentation Platform, a unique “urban behavioral testbed”
that combines innovations in wireless networks, mobile sensing and App deployment to enable an
ecosystem of industry partners to test next-generation context-based applications on approx. 30,000 real-
life users in urban environments, such as the SMU campus, 2 major shopping malls and a resort theme
park. I will then describe ongoing research on offline and near-real time energy-efficient, continuous
smartphone-based human context estimation or “activity mining”, with a special focus on how such
analytics can utilize proximity-driven social interactions. I will then briefly cover two ongoing projects
that exploit such context-sensing to: a) optimize the delivery of mobile advertising and b) perform real-
time adaptation of femtocellular indoor networks.
            </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Feb. 17, 2012</div></td><td><div class="eventInfor">SPEAKER: Russell Sears (Yahoo! Research)</div><div class="eventInfor">A general purpose Log Structured Merge Tree</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99124");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99124" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Feb. 17, 2012 3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Russell Sears (Yahoo! Research)</td></tr><tr><td><b>Title</b></td><td>A general purpose Log Structured Merge Tree</td></tr><tr><td><b>Abstract</b></td><td>
            Data management workloads are increasingly write-intensive and subject
to strict latency SLAs.  This presents a dilemma: Traditional update
in place systems have unmatched latency properties but poor write
throughput.  In contrast, existing log structured techniques
significantly improve write throughput but generally sacrifice read
performance and exhibit unacceptable latency spikes.

We begin by presenting a new performance metric: read fanout, and
argue that, along with read amplification and write amplification, it
better characterizes the real-world performance of index algorithms than
existing approaches such as asymptotic analysis and price/performance.

We then present a Log Structured Merge (LSM) tree implementation that
combines the best properties of B-Trees and log structured approaches:
(1) Unlike existing log structured trees, our implementation has
near-optimal read and scan performance, and (2) we present merge
algorithms that bound write latencies without impacting write
throughput or allowing merges to block application writes for extended
periods of time.  We do this by introducing a new ``spring and gear''
scheduler that ensures merges at each level of the tree make steady
progress.  This allows us to avoid blocking application writes without
resorting to techniques that degrade read performance.

We use Bloom filters to improve index performance, and find that a
number of subtleties arise.  First, it is important to ensure that
reads can safely stop after finding the first version of a record.
Otherwise, frequently written items will incur multiple disk
lookups.  Second, many applications and data management architectures
check for preexisting values at insertion time.  Avoiding the disk
seek performed by the check is crucial for such applications.

This work will appear in Sigmod 2012.
			</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Feb. 10, 2012 (Special Time\Place)</div></td><td><div class="eventInfor">SPEAKER: Anhai Doan (U.  Wisconsin and Walmart Labs - ex Kosmix)</div><div class="eventInfor">Social Media, Data Integration, and Human Computation</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99122");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99122" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Feb. 10, 2012 (Special Time\Place) 11 am</td></tr><tr><td><b>Location</b></td><td>DBH 6011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Anhai Doan (U.  Wisconsin and Walmart Labs - ex Kosmix)</td></tr><tr><td><b>Title</b></td><td>Social Media, Data Integration, and Human Computation</td></tr><tr><td><b>Abstract</b></td><td>Social media has emerged as a major frontier on the World-Wide Web, with applications ranging from helping teenagers track Justin Bieber to e-commerce to fostering revolutions. In this talk I will discuss our work in this area, as carried out at Wisconsin, Kosmix, and @WalmartLabs. I describe how we integrate data from 'traditional' Web sources to build a global taxonomy, greatly expand it with social-media data, then leverage it to build consumer-facing applications. Example applications include building topic pages, detecting Twitter events, and monitoring these events. I discuss the critical role of data integration and human computation in processing social media. Finally, I discuss how all of these can help the emerging area of social commerce, and why Walmart recently acquired Kosmix to make inroads into this new and exciting area.</td></tr><tr><td><b>Speaker Bio</b></td><td>AnHai Doan is an Associate Professor at the University of Wisconsin-Madison. His interests cover databases, AI, and Web, with a current focus on data integration, large-scale knowledge bases, social media, crowdsourcing, human computation, and information extraction. He received the ACM Doctoral Dissertation Award in 2003, a CAREER Award in 2004, and a Sloan Fellowship in 2007. AnHai was Chief Scientist of Kosmix, a social media startup acquired by Walmart in 2011. Currently he also works as Chief Scientist of @WalmartLabs, a research and development lab devoted to integrating social and mobile data for e-commerce.</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Feb. 3, 2012 (Special Time\Place)</div></td><td><div class="eventInfor">SPEAKER: Yannis Papakonstantinou (UCSD)</div><div class="eventInfor">Declarative, optimizable data-driven specifications of web and mobile applications</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99121");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99121" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Feb. 3, 2012 (Special Time\Place) 11 am</td></tr><tr><td><b>Location</b></td><td>DBH 6011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Yannis Papakonstantinou (UCSD)</td></tr><tr><td><b>Title</b></td><td>Declarative, optimizable data-driven specifications of web and mobile applications</td></tr><tr><td><b>Abstract</b></td><td>Developers of web and mobile application development write too much low level "plumbing" code to efficiently access, integrate and coordinate application state that resides on multiple sub-systems of the architecture, and is accessed using different languages: SQL at the database server; HTML and Javascript at the browser, which in HTML5 includes its own database state; Java or other programming languages at the application server.
The FORWARD project replaces such low level code with declarative specifications. Its cornerstones are 
(i) the unified application state virtual database, which enables modeling and manipulating the entire application state in an extension of SQL, named SQL++ 
(ii) specification of Ajax pages as essentially rendered views over the unified application state.
Consequently the following three problems are resolved by appropriate reduction to data management problems, where prior database research literature is leveraged and extended.
1. The partial change of Ajax pages, in response to application state changes, is reduced to an incremental view maintenance problem. Id's that retain the provenance of the page data play an instrumental efficiency role.
2. Efficient data access is reduced to semistructured query processing over an integrated view that involves large database(s) and small main memory-based sources.
3. The inherent location transparency of the specifications is exploited in order to perform computation at the appropriate location (browser vs server). More broadly, the talk discusses ongoing and future work in utilizing the increased abilities of HTML5 clients towards achieving low latency mobile web applications applications, while location transparency of the specifications is retained.</td></tr><tr><td><b>Speaker Bio</b></td><td>
Yannis Papakonstantinou is a Professor of Computer Science and Engineering at the University of California, San Diego. His research is in the intersection of data management technologies and the web, where he has published over eighty research articles. He has given multiple tutorials and invited talks, has served on journal editorial boards and has chaired and participated in program committees for many international conferences and workshops.
Yannis was the CEO and Chief Scientist of Enosys Software, which built and commercialized an early XML-based Enterprise Information Integration platform. Enosys Software was acquired in 2003 by BEA Systems. He was the CEO and is the Chief Scientist of app2you, which has commercialized UCSD R and D on rapid development of web applications for data-driven analytics and business process management. He is the Chief Computer Scientist of a pharmaceutical spin-off startup in the area of data analytics for the pharmaceutical industry. He has been in the technical advisory board of multiple startups, currently including Brightscope Inc.
Yannis holds a Diploma of Electrical Engineering from the National Technical University of Athens, MS and Ph.D. in Computer Science from Stanford University (1997) and an NSF CAREER award for his work on data integration.</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Jan. 27, 2012 </div></td><td><div class="eventInfor">SPEAKER: Kurt Brown (EMC/Greenplum)</div><div class="eventInfor">The Future of Big Data Analytics</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99120");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99120" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Jan. 27, 2012  3 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Kurt Brown (EMC/Greenplum)</td></tr><tr><td><b>Title</b></td><td>The Future of Big Data Analytics</td></tr><tr><td><b>Abstract</b></td><td>"Big Data" and analytics have both existed in some form for as long as computing itself, but only now has technology advanced to the point that, together, they are starting to qualitatively change the way organizations and individuals perceive, understand, and predict the world around them.  In this talk, I'll set Big Data Analytics in a historical context to help sort out what aspects of current technologies (hardware, software, and programming models) are simply transient artifacts or long-term trends, and to project where Big Data Analytics is possibly headed (from the perspective of Greenplum and EMC).</td></tr><tr><td><b>Speaker Bio</b></td><td>Kurt Brown is currently Director of Advanced R and D at Greenplum/EMC.  
            	Prior to EMC, he co-directed Intel's Berkeley Research Lab, spent 13 years with IBM in operating systems 
            	and database R and D on the East and West coasts, and co-founded three startups in database middleware, 
            	small business marketing services, and residential energy management.  
            	He received his PhD in 1995 from the University of Wisconsin for work in automated database performance  tuning.
            </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Jan. 13, 2011</div></td><td><div class="eventInfor">SPEAKER: Thomas Bodner</div><div class="eventInfor">The Stratosphere Parallel Analysis Framework, Present and Future</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99115");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99115" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Jan. 13, 2011 3:00 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Thomas Bodner</td></tr><tr><td><b>Title</b></td><td>The Stratosphere Parallel Analysis Framework, Present and Future</td></tr><tr><td><b>Abstract</b></td><td>Data-intensive computing is a much investigated topic in current research. 
            	Next to parallel databases, new flavors of data processors have established themselves - most prominently the MapReduce programming and execution model. 
            	The new systems provide key features that current parallel databases lack, such as flexibility in the data models, the ability to 
            	parallelize custom functions, and fault tolerance that enables them to scale out to thousands of machines. 
            	This talk presents the current state of Stratosphere system, a cloud data and query processor that has been released as open-source in spring 2011. 
            	The system consists of the parallel data programming model PACT, an extension of the MapReduce programming model for the specification of complex data-intensive tasks in the cloud, 
            	and the elastic, massively parallel execution engine Nephele, a Dryad-like parallel data processor. Furthermore, I give a demo of the most recent Stratosphere release. 
            	And finally, I report on future enhancements for Stratosphere, particularly, for the compilation, optimization and parallel execution of data-intensive operations in the system.
</td></tr><tr><td><b>Speaker Bio</b></td><td>  Since October 2010, Thomas Bodner is a Master's student at the department for Database Systems and Information Management (DIMA) at the Technical University of Berlin. 
        	Between 2007 and 2010, Thomas Bodner completed the Applied Computer Science program at the University of Cooperative Education, Stuttgart, jointly with IBM Germany as partner. 
        	In the course of his undergraduate studies, he studied abroad for one semester at the Royal Melbourne Institute of Technology, Australia and worked as an intern at the IBM Almaden Research Center, 
        	California, USA and the IBM Böblingen Laboratory in Germany, exploring query optimization and in-memory technologies for database management systems. His research interests include architectures 
        	for information management, query processing and optimization, benchmarking and machine learning.
        </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Dec. 9, 2011 (Special Time\Place)</div></td><td><div class="eventInfor">SPEAKER: Pat Helland (Microsoft)</div><div class="eventInfor">If You Have Too Much Data, then "Good Enough" Is Good Enough</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99114");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99114" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Dec. 9, 2011 (Special Time\Place) 11 am</td></tr><tr><td><b>Location</b></td><td>DBH 6011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Pat Helland (Microsoft)</td></tr><tr><td><b>Title</b></td><td>If You Have Too Much Data, then "Good Enough" Is Good Enough</td></tr><tr><td><b>Abstract</b></td><td>Classic database systems offer crisp answers for a relatively small amount of data. These systems hold their data in one or a relatively small number of computers. With a tightly defined schema and transactional consistency, the results returned from queries are crisp and accurate.
New systems have humongous amounts of data content, change rates, and querying rates and take lots of computers to hold and process. The data quality and meaning are fuzzy. The schema, if present, is likely to vary across the data. The origin of the data may be suspect, and its staleness may vary.
Today's data systems coalesce data from many sources. The Internet, B2B, and enterprise application integration (EAI) combine data from different places. No computer is an island. This large amount of interconnectivity and interdependency has led to a relaxation of many database principles. 
In this talk, consider the some of the ways in which today's answers differ from what we used to expect.
					</td></tr><tr><td><b>Speaker Bio</b></td><td>
						Pat Helland has been working in distributed systems, transaction processing, databases, and similar areas since 1978. 
						For most of the 1980s, he was the chief architect of Tandem Computers' TMF (Transaction Monitoring Facility), which provided distributed transactions for the NonStop System. 
						With the exception of a two-year stint at Amazon, Helland has worked at Microsoft Corporation since 1994 where he was the architect for Microsoft Transaction Server and SQL Service Broker. 
						Until September, 2011, he was working on Cosmos, a distributed computation and storage system that provides back-end support for Bing.  
						Pat recently relocated to San Francisco with his wife to be close to the grandchildren and to explore new opportunities in "Big Data" and/or "Cloud Computing".
					</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Nov. 18, 2011</div></td><td><div class="eventInfor">SPEAKER: Yi Pan and Masood Mortazavi (Yahoo!)</div><div class="eventInfor">Scalability and Programming Model in Serving Storage Systems</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99111");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99111" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Nov. 18, 2011 3pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Yi Pan and Masood Mortazavi (Yahoo!)</td></tr><tr><td><b>Title</b></td><td>Scalability and Programming Model in Serving Storage Systems</td></tr><tr><td><b>Abstract</b></td><td>We will review some of the storage technologies Yahoo applications use in Yahoo's cloud platform. These serving storage systems can scale to extremely large numbers of records. After discussing overall architecture of these scalable storage systems, we will focus on Sherpa (PNUTS). Sherpa is a multi-tenant, distributed, highly elastic key-value store with a well-defined transaction semantics that serves data for 100s of Yahoo applications. To exemplify the type of scalability challenges we face, we will describe how we're evolving Sherpa along various dimensions. We will then focus on the programmability dimension and explain how we have implemented a highly scalable, eventually consistent indexing system for Sherpa. Design decisions we have made to balance concerns related to consistency and availability will be discussed, 
            	and we hope to elucidate the basic questions that come up, repeatedly, when evolving such massively scalable systems while they are in operation.</td></tr><tr><td><b>Speaker Bio</b></td><td>Dr. Masood Mortazavi works as a senior principal architect at Yahoo's serving storage systems group. His interests include distributed systems, scalability, multi-tenancy and cloud serving systems. Masood has also worked for Huawei Technologies, Sun Microsystems, Tecknowledge and Hughes Aircrafts.  Masood's LinkedIn profile can be found here: http://www.linkedin.com/in/mortazavi . . . At Yahoo, he helps advance cloud platform and storage technologies.
						Dr. Yi Pan graduated with a Ph.D. degree in computer science from University of California at Irvine. He got his B.S. and M.S. Degree from Fudan University in Shanghai, China. His main interests expand across many areas in large scale distributed computer networks and applications.  Currently, he works as a principal software engineer in Yahoo!’s Cloud Platform Group. His main goal is to push forward Yahoo!’s state-of-art cloud storage systems with innovative features.</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Nov. 4, 2011</div></td><td><div class="eventInfor">SPEAKER: Thomas Bodner</div><div class="eventInfor">Myriad - Parallel Data Generation on Shared-Nothing Architectures</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99110");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99110" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Nov. 4, 2011 3:30 pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Thomas Bodner</td></tr><tr><td><b>Title</b></td><td>Myriad - Parallel Data Generation on Shared-Nothing Architectures</td></tr><tr><td><b>Abstract</b></td><td>
The need for efficient data generation for the purposes of testing and benchmarking newly developed data-intensive computing systems has increased with the emergence of
big data problems. As synthetic data model specifications evolve over time the data generator programs implementing these models have to be continuously adapted –
 a task that might become complex as the set of model constraints grows. This talk presents Myriad - a new parallel data generation toolkit. Data generators created
  with the toolkit can produce very large datasets by exploiting a completely parallel execution model, while at the same time maintain cross-partition dependencies, correlations and distributions in the generated data. 
In addition, I report on our efforts towards a benchmark suite for large-scale parallel analysis systems that uses Myriad for the generation of large social network graphs and OLAP-style relational datasets.
						</td></tr><tr><td><b>Speaker Bio</b></td><td>  Since October 2010, Thomas Bodner is a Master's student at the department for Database Systems and Information Management (DIMA) at the Technical University of Berlin. 
        	Between 2007 and 2010, Thomas Bodner completed the Applied Computer Science program at the University of Cooperative Education, Stuttgart, jointly with IBM Germany as partner. 
        	In the course of his undergraduate studies, he studied abroad for one semester at the Royal Melbourne Institute of Technology, Australia and worked as an intern at the IBM Almaden Research Center, 
        	California, USA and the IBM Böblingen Laboratory in Germany, exploring query optimization and in-memory technologies for database management systems. His research interests include architectures 
        	for information management, query processing and optimization, benchmarking and machine learning.
        </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Oct. 21, 2011</div></td><td><div class="eventInfor">SPEAKER: David Lomet (Microsoft Research)</div><div class="eventInfor">Deuteronomy: Transaction Support for Cloud Data</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99109");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99109" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Oct. 21, 2011 3pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>David Lomet (Microsoft Research)</td></tr><tr><td><b>Title</b></td><td>Deuteronomy: Transaction Support for Cloud Data</td></tr><tr><td><b>Abstract</b></td><td>The Deuteronomy system supports efficient and scalable ACID transactions in the cloud by decomposing the storage engine
            	 into: (a) a transactional component (TC) that manages transactions and their ``logical" concurrency control and undo/redo recovery, 
            	 and (b) a data component (DC) that knows about the access methods and supports a record-oriented interface with atomic operations, 
            	 but knows nothing about transactions.  The Deuteronomy TC can be applied to data anywhere, in the cloud, local, etc. with a variety 
            	 of deployments for both the TC and DC components.  In this talk, we first describe the architecture of our TC, and the considerations 
            	 that led to it.  We next describe the contract between TC and DC, how we changed the operation protocol to simplify it and make it more efficient.   
            	 We have implemented both TC and multiple DCs, and will describe our TC implementation in detail.  
            	 We will end a few words about observed performance and scalability.</td></tr><tr><td><b>Speaker Bio</b></td><td>
            	David Lomet is a principal researcher managing the Microsoft Research Database Group.  Earlier, he worked at Digital, IBM Research, and Wang Institute.  
            	He has a CS Ph.D from the University of Pennsylvania.  He is author of over 100 papers (two SIGMOD "best papers") and has 45 patents.  
            	He has served on program committees (SIGMOD, PODS, VLDB, ICDE...), was ICDE'2000 PC co-chair, VLDB'2006 PC core chair, and is on the ICDE Steering Committee, 
            	the VLDB Board, is TCDE Chair and has been an editor for TODS, VLDBJ, and JDPD. He is the Data Engineering Bulletin EIC, for which he received the SIGMOD Contributions Award.  
            	He received IEEE Golden Core, Outstanding, and Meritorious Service Awards and is a Fellow of IEEE, ACM, and AAAS.
            </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Oct. 21, 2011 (Special Time\Place)</div></td><td><div class="eventInfor">SPEAKER: Danny Sullivan (Editor In Chief, Search Engine Land)</div><div class="eventInfor">From Search 1.0 to Search 4.0</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99108");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99108" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Oct. 21, 2011 (Special Time\Place) 11am</td></tr><tr><td><b>Location</b></td><td>DBH 6011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Danny Sullivan (Editor In Chief, Search Engine Land)</td></tr><tr><td><b>Title</b></td><td>From Search 1.0 to Search 4.0</td></tr><tr><td><b>Abstract</b></td><td>When search engines first began, they focused on crawling web pages
and "words on the page" ranking analysis. That system quickly failed,
being far too easy to game. Search 2.0 gave us ranking where links
were used as votes; Search 3.0, a third generational system,
introduced blending vertical search results with web matches.
Currently underway, the fourth generational trend of Search 4.0 taps
into human signals, from social networks and personalization, to
refine search results. The "how and why" of this evolution has
unfolded.</td></tr><tr><td><b>Speaker Bio</b></td><td>Widely considered a leading "search engine guru," Danny Sullivan has
been helping webmasters, marketers and everyday web users understand
how search engines work for over a decade. Danny's expertise about
search engines is often sought by the media, and he has been quoted in
places like The Wall St. Journal, USA Today, The Los Angeles Times,
Forbes, The New Yorker and Newsweek and ABC's Nightline. Danny began
covering search engines in late 1995, when he undertook a study of how
they indexed web pages. The results were published online as "A
Webmaster's Guide To Search Engines," a pioneering effort to answer
the many questions site designers and Internet publicists had about
search engines. Danny currently heads up Search Engine Land as
editor-in-chief, which covers all aspects of search marketing and
search engine news. Danny also serves as Third Door Media's chief
content officer, which owns Search Engine Land and the SMX: Search
Marketing Expo conference series. Danny also maintains a personal blog
called Daggle and microblogs on Twitter: @dannysullivan.</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Oct. 14, 2011</div></td><td><div class="eventInfor">SPEAKER: Tyson Condie (Yahoo! Research)</div><div class="eventInfor">Scal(a)ing up Machine Learning and Graph-based Analytics</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99107");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99107" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Oct. 14, 2011 3pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Tyson Condie (Yahoo! Research)</td></tr><tr><td><b>Title</b></td><td>Scal(a)ing up Machine Learning and Graph-based Analytics</td></tr><tr><td><b>Abstract</b></td><td>
Machine learning practitioners are increasingly interested in applying their algorithms to Big Data. Unfortunately, current high-level 
languages for data analytics (e.g., Hive, Pig, Sawzall, Scope) do not fully cover this domain. One key missing ingredient is the means to 
efficiently support iteration over the data. Zaharia et al., were the first to answer this call from a systems perspective with Spark. 
Spark adds the notion of a working set to data-parallel workflows and has published speed-ups of 30x over Hadoop MapReduce for many machine learning and graph algorithms.

Unfortunately, Spark does cover the whole pipeline of Big Data analytics; at Yahoo!, it is common to compose Pig, MPI and direct MapReduce program modules into workflows. 
This fractioning of individual processing steps can be a major pain e.g., for optimization, debugging, and code readability. Our prescription to this dilemma is a new DSL 
for data analytics called ScalOps. Like Pig, ScalOps combines the declarative style of SQL and the low-level procedural style of MapReduce. Like Spark, ScalOps can optimize 
its runtime—the Hyracks parallel-database engine—for repeated access to data collections. ScalOps is part of a broader research agenda to explore new abstractions
 for machine learning and graph-based analytics. In this talk, I will present example workflows from the machine learning domain expressed in ScalOps and their translation to Hyracks recursive query plans. </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Sept. 30, 2011</div></td><td><div class="eventInfor">SPEAKER: Grad. students</div><div class="eventInfor">System Demo</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99106");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99106" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Sept. 30, 2011 3pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Grad. students</td></tr><tr><td><b>Title</b></td><td>System Demo</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Sept. 23, 2011</div></td><td><div class="eventInfor">SPEAKER: ISG memebers</div><div class="eventInfor">ISG Gathering</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99105");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99105" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Sept. 23, 2011 3pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>ISG memebers</td></tr><tr><td><b>Title</b></td><td>ISG Gathering</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">June 3, 2011</div></td><td><div class="eventInfor">SPEAKER: Donald Kossman</div><div class="eventInfor">Predictable Performance for Unpredictable Workloads</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99104");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99104" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>June 3, 2011 2pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Donald Kossman</td></tr><tr><td><b>Title</b></td><td>Predictable Performance for Unpredictable Workloads</td></tr><tr><td><b>Abstract</b></td><td>This talk presents the design of SwissBox. 
            	SwissBox is a database appliance designed to process thousands of concurrent queries and updates with bounded 
            	query response times and strict data freshness guarantees. The system was designed to aggressively share operations 
            	between concurrent queries and updates. This talk shows the design of the storage manager (called Crescando)
            	 and the design of the query processor (called SharedDB). Furthermore, the talk presents the results of 
            	 performance experiments with workloads from an airline reservation system.</td></tr><tr><td><b>Speaker Bio</b></td><td>Donald Kossmann is a professor for Computer Science at ETH Zurich (Switzerland). He received his MS from the University of Karlsruhe and completed his PhD at the University of Aachen. After that, he held positions at the University of Maryland, the IBM Almaden Research Center, the University of Passau, the University of Munich, and the University of Heidelberg. He is an ACM fellow, member of the board of trustees of the VLDB endowment, and was the program committee chair of the ACM SIGMOD Conf., 2009. He is a co-founder of i-TV-T (1998), XQRL Inc. 
            	(acquired by BEA in 2002), and 28msec Inc. (2007). 
            	His research interests lie in the area of databases and information systems.</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">May 20, 2011</div></td><td><div class="eventInfor">SPEAKER: Ronen Vaisenberg</div><div class="eventInfor">Scheduling and Actuating Camera Networks to Maximize Event Detection</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99103");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99103" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>May 20, 2011 2pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Ronen Vaisenberg</td></tr><tr><td><b>Title</b></td><td>Scheduling and Actuating Camera Networks to Maximize Event Detection</td></tr><tr><td><b>Abstract</b></td><td>A distributed camera network allows for many compelling applications, such as large-scale tracking, face recognition, occupancy monitoring or event detection. 
					In most practical systems, resources are either constrained or mutually exclusive. Constraints arise from network bandwidth restrictions, I/O and disk usage from writing images, 
					and CPU usage needed to extract features from the images. Detecting events in real time requires dynamically choosing a subset of the available sensors for processing at any given time. 
Furthermore, certain camera configurations are not feasible. For example, a camera cannot zoom into two different regions in its field of view.
Zooming into a specific area in the field of view of a camera would generate a high resolution image of the region in the expense of a wider field of view. 
Thus, the field of view needs to be changed dynamically to get a higher resolution images of certain regions of the space at the expanse other regions. 
In order to illustrate the complexity of this problem, consider a face recognition application, which is only interested in high resolution (by means of optical zoom) 
facial images. If we always zoom into a region to look for a high res face, we might miss presence of a person in different region and hence opportunity for zooming later to get the face in next time step.
In this talk we examine the problem of scheduling sensors for data collection and actuating them on real time to maximize some user-specified objective - e.g., 
detecting as much motion as possible or collect as many high resolution facial images. 
The main idea behind our approach is the use of sensor semantics to guide the scheduling process. We learn a dynamic probabilistic model of motion correlations 
between cameras, and use the model to guide resource allocation for our sensor network.
Although previous work has leveraged probabilistic models for sensor-scheduling, our work is distinct in its focus on real-time building-monitoring using a camera network.  
We validate our approach using a sensor network of a dozen cameras spread throughout a university building, recording measurements of unscripted human activity over a two week period. 
We automatically learn a semantic model of typical behaviors, and show that one can significantly improve efficiency of resource allocation and actuation by exploiting this model.
</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">May 13, 2011 (Special)</div></td><td><div class="eventInfor">SPEAKER: Prof. John Ousterhout (Stanford)</div><div class="eventInfor">RAMCloud: Scalable High-Performance Storage Entirely in DRAM</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99102");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99102" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>May 13, 2011 (Special) 11am</td></tr><tr><td><b>Location</b></td><td>DBH 6011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Prof. John Ousterhout (Stanford)</td></tr><tr><td><b>Title</b></td><td>RAMCloud: Scalable High-Performance Storage Entirely in DRAM</td></tr><tr><td><b>Abstract</b></td><td>Disk-oriented approaches to online storage are becoming increasingly problematic: they do not scale gracefully to meet the needs of new large-scale Web applications, and improvements in disk capacity have out-stripped improvements in access speed.  In this talk I will describe a new approach to datacenter storage called RAMCloud, where information is kept entirely in DRAM and large-scale systems are created by aggregating the main memories of thousands of commodity servers.  A RAMCloud can provide durable and available storage with 100-1000x the throughput of disk-based systems and 100-1000x lower access latency.  By combining low latency and large scale, RAMClouds will enable a new class of applications that manipulate large datasets more intensively than has ever been possible.
</td></tr><tr><td><b>Speaker Bio</b></td><td>John Ousterhout is Professor (Research) of Computer Science at Stanford University.  His current research focuses on infrastructure
for Web applications and cloud computing.  Ousterhout's prior positions include 14 years in industry where he founded two companies (Scriptics and Electric Cloud), preceded by 14 years as Professor of Computer Science at U.C. Berkeley.  He is the creator of the Tcl scripting language and is also well known for his work in distributed operating systems and file systems.  Ousterhout received a BS degree in Physics from Yale University and a PhD in Computer Science from Carnegie Mellon University.  He is a member of the National Academy of Engineering and has received numerous awards, including the ACM Software System Award, the ACM Grace Murray Hopper Award, the National Science Foundation Presidential Young Investigator Award, and the U.C. Berkeley Distinguished Teaching Award.
</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">May 9, 2011 (Special)</div></td><td><div class="eventInfor">SPEAKER: <a xmlns="" href="http://pages.cs.wisc.edu/~bart/">Prof. Barton P. Miller</a></div><div class="eventInfor">Scaling Up to Large (Really Large) Systems</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99101");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99101" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>May 9, 2011 (Special) 11am</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td><a xmlns="" href="http://pages.cs.wisc.edu/~bart/">Prof. Barton P. Miller</a></td></tr><tr><td><b>Title</b></td><td>Scaling Up to Large (Really Large) Systems</td></tr><tr><td><b>Abstract</b></td><td>I will discuss the problem of developing tools and middleware for large scale
parallel environments.  We are especially interested in systems, both leadership
class parallel computers and clusters that have 100,000's or even millions
of processors.  The infrastructure that we have developed to address this
problem is called MRNet, the Multicast/Reduction Network. MRNet's approach
to scale is to structure control and data flow in a tree-based overlay
network (TBON) that allows for efficient request distribution and flexible
data reductions.

I will then present an overview of the MRNet design, architecture, and
computational model and then discuss several of the applications of MRNet.
The applications include scalable automated performance analysis, a vision
clustering application and, most recently, an effort to develop our first
petascale debugging tool, STAT, a scalable stack trace analyzer running
currently on 100,000's of processors on both the Cray XT and IBM BlueGene.</td></tr><tr><td><b>Speaker Bio</b></td><td>Prof. Barton Miller is a Professor of Computer Sciences at the University of Wisconsin.
Bart is a product of the UC System: he received his BA degree from UC San Diego in 1977
and his MS and PhD in Computer Science from UC Berkeley in 1980 and 1984, respectively.
His research interests include distributed and parallel program performance and tools,
binary code analysis and instrumentation, computer security, scalable systems, operating
systems, and software testing. Bart is a Fellow of the ACM.</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">May 6, 2011</div></td><td><div class="eventInfor">SPEAKER: Matthias Nicola, IBM</div><div class="eventInfor"> A Matter of Time: Temporal Data Management in DB2 for z/OS </div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99100");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99100" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>May 6, 2011 2pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Matthias Nicola, IBM</td></tr><tr><td><b>Title</b></td><td> A Matter of Time: Temporal Data Management in DB2 for z/OS </td></tr><tr><td><b>Abstract</b></td><td>Time is a critical dimension in data management. For many enterprises
		 it is useful or even required to have the ability to go back in time and look at a 
		 past state of the database. Many applications also need to manage time in their 
		 business records, such as contract start and end dates, expiration dates, or 
		 "effective dates" to indicate that information is valid for a certain period in the past, presence, or future. This presentation
		 describes typical use cases for temporal data management and describes 
		  the temporal capabilities in DB2, including system time, business time, and bitemporal support. 
</td></tr><tr><td><b>Speaker Bio</b></td><td>Matthias Nicola is a senior software engineer at IBM's Silicon Valley Lab, in 
	San Jose, CA, USA. He focuses on DB2 performance and benchmarking, XML, temporal data 
	management, in-database analytics, and other emerging technologies. Matthias also works 
	closely with customers and business partners to help them design, optimize and implement 
	DB2 solutions. Previously Matthias worked on data warehouse performance at Informix Software. 
	Matthias received his PhD in computer science from the Technical University of Aachen, Germany. 
</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">April 25, 2011 (Special)</div></td><td><div class="eventInfor">SPEAKER: <a xmlns="" href="http://www.cs.cmu.edu/~christos/">Prof. Christos Faloutsos</a>, CMU</div><div class="eventInfor">Mining Billion-node Graphs</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99029");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99029" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>April 25, 2011 (Special) 11am</td></tr><tr><td><b>Location</b></td><td>DBH 6011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td><a xmlns="" href="http://www.cs.cmu.edu/~christos/">Prof. Christos Faloutsos</a>, CMU</td></tr><tr><td><b>Title</b></td><td>Mining Billion-node Graphs</td></tr><tr><td><b>Abstract</b></td><td>What do graphs look like? How do they evolve over time? How to handle a 
graph with a billion nodes? We present a comprehensive list of static 
and temporal laws, and some recent observations on real graphs (like, 
e.g., ``eigenSpokes''). We present tools, and specifically ``oddBall'' 
for discovering anomalies and patterns, as well as fast algorithms for 
immunization. Finally, we present an overview of the PEGASUS system 
which is designed to handle billion-node graphs, running on top of the 
"hadoop" system.</td></tr><tr><td><b>Speaker Bio</b></td><td>Christos Faloutsos is a Professor at Carnegie Mellon University. He has 
received the Presidential Young Investigator Award by the National 
Science Foundation (1989), the Research Contributions Award in ICDM 
2006, the SIGKDD Innovations Award (2010), seventeen ``best paper'' 
awards, (including two ``test of time'') and four teaching awards. He 
has served as a member of the executive committee of SIGKDD; he is an 
ACM Fellow; he has published over 200 refereed articles, 11 book 
chapters and one monograph.  He holds five patents and he has given over 
30 tutorials and over 10 invited distinguished lectures. His research 
interests include data mining for graphs and streams, fractals, database 
performance, and indexing for multimedia and bio-informatics data.
</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">April 22, 2011</div></td><td><div class="eventInfor">SPEAKER: <a xmlns="" href="https://researcher.ibm.com/researcher/view.php?person=us-simeon">Jerome Simeon</a>, IBM Research T.J. Watson</div><div class="eventInfor">Algebraic Comprehensions (Database Optimization for Web 2.0 Queries)</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99028");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99028" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>April 22, 2011 2pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td><a xmlns="" href="https://researcher.ibm.com/researcher/view.php?person=us-simeon">Jerome Simeon</a>, IBM Research T.J. Watson</td></tr><tr><td><b>Title</b></td><td>Algebraic Comprehensions (Database Optimization for Web 2.0 Queries)</td></tr><tr><td><b>Abstract</b></td><td>Direct support for querying is becoming a "must have" for programming languages targeting Web 2.0 and Cloud development. Most of those languages (Microsoft's Linq, University of Edinburgh's Links, EPFL's Scala, Yahoo!'s Pig Latin, IBM's Thorn, etc) rely on the classic notion of comprehensions over collections. At the language level, comprehensions are a perfect choice, being well understood programming constructs, and capturing the expressive power of SQL iterators. At the compiler level, however, they are at odds with database optimizers which mostly rely on relational (or nested-relational) algebras. That mismatch was clearly on display during the design of XQuery, whose semantics is based on comprehensions, and for which most implementations target relational backends. We propose a alternative functional semantic formulation of XQuery to the one proposed by W3C, which is also based on comprehensions but has the benefit of corresponding precisely to compilation into a typed algebra that supports traditional database optimizations. First, this provides a formal foundation for XQuery implementations that want to ensure semantics integrity with the standard, along with modern database optimization techniques. Also, it provides key insights into the nature of database compilers that we believe is essential for the integration of database and programming languages technology. We notably discover that type systems for database algebras require an original solution to the old problem of subtyping with record concatenation, and that such a type system can eliminate the need for complex side conditions used in query language optimization.
	</td></tr><tr><td><b>Speaker Bio</b></td><td>Jerome Simeon is a Researcher for the Scalable XML Infrastructure Group at IBM T.J. Watson. He holds a degree in Engineering from EcolePolytechnique, and a Ph.D. from Universite d'Orsay. Previously, Jerome worked at INRIA from 1995 to 1999, and Bell Laboratories from 1999 to 2004. His research interests include databases, programming languages, compilers, and semantics, with a focus on Web development. He has put his work into practice in areas ranging from telecommunication infrastructure, to music. He is a co-editor for five of the W3C XML Query specifications, and has published more than 50 papers in scientific journals and international conferences. He is also a project lead for the Galax open-source XQuery implementation, and a co-author of "XQuery from the Experts" (Addison Wesley, 2004).
        </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">April 15, 2011 </div></td><td><div class="eventInfor">SPEAKER: Tyson Condie, Yahoo! Research</div><div class="eventInfor">RubySky: Exploring Big Data with Transparency and Adjustability</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99027");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99027" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>April 15, 2011  2pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Tyson Condie, Yahoo! Research</td></tr><tr><td><b>Title</b></td><td>RubySky: Exploring Big Data with Transparency and Adjustability</td></tr><tr><td><b>Abstract</b></td><td>In this talk, I will introduce a new scripting language for ad-hoc exploration of large data sets, called RubySky.  
            	As with several prior efforts, RubySky scripts execute either in a local environment or in the cloud (Hadoop). 
            	Typically, cloud-based execution is highly opaque and hands-off, rendering debugging and iterative code development 
            	very difficult. RubySky, on the other hand, aims for a more transparent and adjustable paradigm.  
            	It includes the ability to ``peek into'' intermediate cloud execution pathways, integrated as a first-class language construct.  
            	Also integrated into the language is a way for the user to make last-minute code revisions, at any point at which troublesome
            	 data is encountered in the cloud. 
            	 Combined, these features aim to improve usability for users who develop and run single-use scripts that
            	  explore new data sets. This is joint work with Christopher Olston at Yahoo! Research.
            </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">April 1, 2011 </div></td><td><div class="eventInfor">SPEAKER: <a xmlns="" href="http://research.microsoft.com/en-us/people/terry/">Doug Terry</a>, Microsoft Research</div><div class="eventInfor">Replicated Data Consistency Explained through Baseball</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99026");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99026" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>April 1, 2011  2pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td><a xmlns="" href="http://research.microsoft.com/en-us/people/terry/">Doug Terry</a>, Microsoft Research</td></tr><tr><td><b>Title</b></td><td>Replicated Data Consistency Explained through Baseball</td></tr><tr><td><b>Abstract</b></td><td> A variety of relaxed consistency models for replicated data have
been proposed and studied as an alternative to one-copy serializability, and
some of these are being used in cloud storage systems.  The designers of
such systems particularly avoid two-phase commit for updates to
geo-replicated data that spans multiple data centers on different
continents.  Instead, many cloud services, including systems from Amazon,
Yahoo, and Microsoft, have adopted techniques that provide eventual
consistency.  This talk explores the hows and whys of different consistency
models.  The discussion will be driven by a simple example: maintaining the
score of a baseball game. We'll see that people with various roles in the
game can tolerate and benefit from different types of consistency when
accessing the score.
</td></tr><tr><td><b>Speaker Bio</b></td><td>Doug Terry is a Principal Researcher in the Microsoft Research Silicon
Valley lab.  His research focuses on the design and implementation of novel
distributed systems including mobile and cloud services.  He currently is
serving as Chair of ACM's Special Interest Group on Operating Systems
(SIGOPS) and as a member of the ACM Council.  Prior to joining Microsoft,
Doug was the co-founder and CTO of a start-up company named Cogenia, Chief
Scientist of the Computer Science Laboratory at Xerox PARC, and an Adjunct
Professor in the Computer Science Division at U. C. Berkeley, where he still
occasionally teaches a graduate course on distributed systems.  Doug has a
Ph.D. in Computer Science from U.C. Berkeley and is an ACM Fellow.</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Mar 31, 2011 </div></td><td><div class="eventInfor">SPEAKER: <a xmlns="" href="http://research.microsoft.com/en-us/people/terry/">Doug Terry</a>, Microsoft Research</div><div class="eventInfor">Cimbiosys: Content-based Replication for Mobile Devices and the Cloud</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99025");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99025" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Mar 31, 2011  11am</td></tr><tr><td><b>Location</b></td><td>DBH 6011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td><a xmlns="" href="http://research.microsoft.com/en-us/people/terry/">Doug Terry</a>, Microsoft Research</td></tr><tr><td><b>Title</b></td><td>Cimbiosys: Content-based Replication for Mobile Devices and the Cloud</td></tr><tr><td><b>Abstract</b></td><td> As people increasingly use mobile devices and cloud services to
share large data collections, exploiting communication proximity and
selectively replicating content is essential.  Cimbiosys is a replicated
storage platform that permits each device to define its own content-based
filtering criteria and to exchange data directly with other devices.  This
talk focuses on the key challenge of ensuring eventual consistency in the
face of fluid network connectivity, redefinable content filters, and
arbitrary updates.  Notably, Cimbiosys guarantees that each device
eventually stores precisely those items whose latest version matches its
custom filter and represents its replication-specific metadata in a compact
form, resulting in low data synchronization overhead.  This permits ad hoc
replication between newly encountered devices and frequent synchronization
between established partners, even over low bandwidth wireless networks or
across geo-distributed data centers.  (This talk will be a Ted and Janice Smith
Distinguished lecture, and not at the normal time or place for ISG Seminars.)
</td></tr><tr><td><b>Speaker Bio</b></td><td>Doug Terry is a Principal Researcher in the Microsoft Research Silicon
Valley lab.  His research focuses on the design and implementation of novel
distributed systems including mobile and cloud services.  He currently is
serving as Chair of ACM's Special Interest Group on Operating Systems
(SIGOPS) and as a member of the ACM Council.  Prior to joining Microsoft,
Doug was the co-founder and CTO of a start-up company named Cogenia, Chief
Scientist of the Computer Science Laboratory at Xerox PARC, and an Adjunct
Professor in the Computer Science Division at U. C. Berkeley, where he still
occasionally teaches a graduate course on distributed systems.  Doug has a
Ph.D. in Computer Science from U.C. Berkeley and is an ACM Fellow.
</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Mar 25, 2011 </div></td><td><div class="eventInfor">SPEAKER: <a xmlns="" href="http://www.ics.uci.edu/~abehm/">Alexander Behm</a>, UCI PhD student</div><div class="eventInfor">Answering Approximate String Queries on Large Data Sets Using External Memory</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99023");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99023" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Mar 25, 2011  2pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td><a xmlns="" href="http://www.ics.uci.edu/~abehm/">Alexander Behm</a>, UCI PhD student</td></tr><tr><td><b>Title</b></td><td>Answering Approximate String Queries on Large Data Sets Using External Memory</td></tr><tr><td><b>Abstract</b></td><td>An approximate string query is to find from a
collection of strings those that are similar to a given query string.
Answering such queries is important in many applications such
as data cleaning and record linkage, where errors could occur
in queries as well as the data. Many existing algorithms have
focused on in-memory indexes. In this paper we investigate how
to efficiently answer such queries in a disk-based setting, by
systematically studying the effects of storing data and indexes
on disk. We devise a novel physical layout for an inverted
index to answer queries and we study how to construct it with
limited buffer space. To answer queries, we develop a cost-based,
adaptive algorithm that balances the I/O costs of retrieving
candidate matches and accessing inverted lists. Experiments
on large, real datasets verify that simply adapting existing
algorithms to a disk-based setting does not work well and that our
new techniques answer queries efficiently. Further, our solutions
significantly outperform a recent tree-based index, BED-tree.
This talk is a ICDE practice talk.
            </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Mar 18, 2011 </div></td><td><div class="eventInfor">SPEAKER: Pinaki Sinha</div><div class="eventInfor">Summarization of  Personal Photo Collections</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99022");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99022" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Mar 18, 2011  2pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Pinaki Sinha</td></tr><tr><td><b>Title</b></td><td>Summarization of  Personal Photo Collections</td></tr><tr><td><b>Abstract</b></td><td>The volume of personal photos hosted on photo archives and social sharing platforms 
            	has been increasing exponentially. According to recent estimates, 6 Billion photos are uploaded
            	 on Facebook per month. It is difficult to get an overview of a large collection of personal 
            	 photos without browsing though the entire database manually. In this talk, I will discuss a 
            	 framework to generate representative subset summaries from photo collections present on personal 
            	 archives or social networks. I will define salient properties of an effective photo summary 
            	 and model summarization as an optimization of these properties, given the size constraints. 
            	 Computer vision, and IR  based techniques will be used to generate summaries that "look good" as well as 
            	 are informative. I will also introduce information theory based metrics for evaluating photo 
            	 summaries based on their information content and the ability to satisfy user's information needs. 
            	 I will also discuss the manual evaluation experiments that were done to evaluate summaries.
            </td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Mar 11, 2011 </div></td><td><div class="eventInfor">SPEAKER: <a xmlns="" href="http://www.ics.uci.edu/~dvk"> Dmitri V. Kalashniknov </a></div><div class="eventInfor">Entity resolution</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99021");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99021" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Mar 11, 2011  2pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td><a xmlns="" href="http://www.ics.uci.edu/~dvk"> Dmitri V. Kalashniknov </a></td></tr><tr><td><b>Title</b></td><td>Entity resolution</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Mar 4, 2011 </div></td><td><div class="eventInfor">SPEAKER: Rares Vernica</div><div class="eventInfor"> Efficient Processing of Set-Similarity Joins on Large Clusters</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99019");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99019" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Mar 4, 2011  2pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td>Rares Vernica</td></tr><tr><td><b>Title</b></td><td> Efficient Processing of Set-Similarity Joins on Large Clusters</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Feb 23, 2011 </div></td><td><div class="eventInfor">SPEAKER:  Dr. Terence Sim</div><div class="eventInfor">Getting More From Fisher</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99018");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99018" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Feb 23, 2011  3pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td> Dr. Terence Sim</td></tr><tr><td><b>Title</b></td><td>Getting More From Fisher</td></tr><tr><td><b>Abstract</b></td><td>The Fisher Linear Discriminant (FLD) is commonly used
in classification to find a subspace that maximally separates
class patterns according to the Fisher Criterion.   It was
previously proven that a pre-whitening step can be used to
truly optimize the Fisher Criterion. In this talk, we show that
more insight and more applications may be derived from this
classical technique.

First, we explore the subspaces induced by this whitened FLD.
In particular, we show how the Identity Space and Variation
Space are useful for decomposing and representing data.
We give sufficient conditions for these spaces to exist.  Through
experiments we also show how these spaces may
be used for classification and image synthesis.

Second, we further extend classical Fisher to handle data exhibiting
multiple factors (modes), e.g. face images that exhibit personal
identity, illumination, and pose.  We call our method Multimodal
Discriminant Analysis (MMDA), which is useful for decomposing
a dataset into independent modes. For face images, MMDA
effectively separates identity, illumination and pose into mutually
orthogonal subspaces.  MMDA is based on maximizing the
Fisher Criterion on all modes simultaneously, and is therefore
well-suited for multimodal and mode-invariant pattern recognition.
We also show that MMDA may be used for dimension reduction,
and for synthesizing face images under novel illumination, and
even novel personal identity.</td></tr><tr><td><b>Speaker Bio</b></td><td>Terence Sim is an Asst. Prof. at the School of Computing, National University of Singapore.  He teaches an undergraduate course in computer vision, as well as a graduate course in multimedia fundamentals.  For research, he works primarily in these areas:  face recognition, biometrics, and computational photography.   He is also interested in computer vision problems in general, such as shape-from-shading, photometric stereo, object recognition.  On the side, he dabbles with some aspects of music processing, such as polyphonic music transcription.  Dr. Sim serves as Vice-Chairman of the Biometrics Technical Committee (BTC), Singapore, and Chairman of the Cross-Jurisdictional and Societal Aspects Working Group (WG6) within the BTC.  The interesting issues here are the legal and privacy aspects of using biometrics.   He also serves as Vice-President of the Pattern Recognition and Machine Intelligence Association (PREMIA), a national professional body for pattern recognition.  Dr. Sim obtained his PhD from Carnegie Mellon in 2002, his MSc from Stanford University in 1991, and his SB from MIT in 1990.</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Feb 16, 2011</div></td><td><div class="eventInfor">SPEAKER: <a xmlns="" href="https://researcher.ibm.com/researcher/view.php?person=almaden-laura"> Laura Haas (IBM)</a></div><div class="eventInfor">New Principles for Information Integration </div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99016");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99016" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Feb 16, 2011 11am</td></tr><tr><td><b>Location</b></td><td>DBH 4011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td><a xmlns="" href="https://researcher.ibm.com/researcher/view.php?person=almaden-laura"> Laura Haas (IBM)</a></td></tr><tr><td><b>Title</b></td><td>New Principles for Information Integration </td></tr><tr><td><b>Abstract</b></td><td> Ten years ago, Clio introduced nonprocedural schema mappings to describe the relationship between data in heterogeneous schemas. This enabled powerful tools for mapping discovery and integration code generation, greatly simplifying the integration process.  However, further progress is needed. We see an opportunity to raise the level of abstraction further, and propose two new principles that the next generation of integration systems should embody. Holistic information integration supports iteration across the various integration tasks, leveraging information about both schema and data to improve the integrated result. Integration independence allows applications to be independent of how, when, and where information integration takes place, making materialization and the timing of transformations an optimization decision that is transparent to applications. This talk introduces these principles and describes some promising recent work in these directions. </td></tr><tr><td><b>Speaker Bio</b></td><td> Laura Haas is an IBM Fellow and has been director of computer science at IBM Almaden Research Center since 2005.  Previously, Dr. Haas was responsible for Information Integration Solutions (IIS) architecture in IBM's Software Group after leading the IIS development team through its first two years.  She joined the development team in 2001 as manager of DB2 UDB Query Compiler development.  Before that, Dr. Haas was a research staff member and manager at the Almaden lab for nearly twenty years.   In IBM Research, she worked on and managed a number of exploratory projects in distributed database systems.  Dr. Haas is best known for her work on the Starburst query processor (from which DB2 UDB was developed); on Garlic, a system which allowed federation of heterogeneous data sources; and on Clio, the first semi-automatic tool for heterogeneous schema mapping.  Garlic technology, married with DB2 UDB query processing, is the basis for the IBM WebSphere Information Server's federation capabilities, while Clio capabilities are a core differentiator in IBM’s Rational Data Architect.  Dr. Haas has received several IBM awards for Outstanding Technical Achievement and Outstanding Innovation, and an IBM Corporate Award for her work on federated database technology. In 2010 she was recognized with the Anita Borg Institute Technical Leadership Award. She is a member of the National Academy of Engineering and the IBM Academy of Technology, an ACM Fellow, and Vice Chair of the board of the Computing Research Association.  Dr. Haas received her PhD from the University of Texas at Austin, and her bachelor degree from Harvard University.</td></tr></table></div><div style="height: 5px;"></div></td></tr><tr><td><div class="eventDate">Feb 4, 2011 (POSTPONED)</div></td><td><div class="eventInfor">SPEAKER: <a xmlns="" href="http://www.ics.uci.edu/~amyvoida/Site/News_%26_Updates/News_%26_Updates.html"> Amy Voida</a></div><div class="eventInfor">Homebrew Databases</div></td></tr><tr><td></td><td><a href="#" onclick='var x=getElementById("e99015");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>Details</a></td></tr><tr class="detr"><td colspan="2"><div id="e99015" style="display:none;border:solid grey;border-width:1px;"><table><tr><td style="width:100px;"><b>Date and Time</b></td><td>Feb 4, 2011 (POSTPONED) 2pm</td></tr><tr><td><b>Location</b></td><td>DBH 3011</td></tr></table><br></br><table><tr><td style="width:100px;"><b>Speaker</b></td><td><a xmlns="" href="http://www.ics.uci.edu/~amyvoida/Site/News_%26_Updates/News_%26_Updates.html"> Amy Voida</a></td></tr><tr><td><b>Title</b></td><td>Homebrew Databases</td></tr></table></div><div style="height: 5px;"></div></td></tr></table></div><div class="content"><p></p><p> For more information on CS distinguished lectures, please visit <a href="http://www.ics.uci.edu/computerscience/research/seminarseries/">Computer Science Department Seminar Series</a>.</p><p></p><div class="gotop"><a href="#header">^ top</a></div></div><div id="footer"><p>Last Updated on January 07, 2011</p></div></div></body></html>

