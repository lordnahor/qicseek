<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html>
<head>
    <meta name="description" content="Web Crawling Introduction to Information Retrieval CS 221 Donald J. Patterson Content adapted from Hinrich Schütze http://www.informationretrieval.org  A Robust Crawl Architecture Robust Crawling  Processing Steps in Crawling Robust Crawling Pick a URL from the frontier (how to prioritize?) Fetch the document (DNS lookup) Parse the URL Extract Links Check for duplicate content If not add to index For each extracted link Make sure it passes filter (robots.txt) Make sure it isn’t in the URL frontier  Domain Name Server DNS A lookup service on the internet Given a URL, retrieve its IP address www.djp3.net -&gt; 69.17.116.124 This service is provided by a distributed set of servers Latency can be high Even seconds Common OS implementations of DNS lookup are blocking One request at a time Solution: Caching Batch requests  DNS dig +trace www.djp3.net  DNS What really happens  Class Exercise DNS Calculate how long it would take to completely fill a DNS cache. How many active hosts are there? What is an average lookup time? Do the math. http://www.flickr.com/photos/lurie/298967218/  DNS http://www.flickr.com/photos/lurie/298967218/ Why run a DNS lookup service? A public good It helps your other business You can make money on bad queries Mobile servers need special attention  A Robust Crawl Architecture Robust Crawling  Parsing: URL normalization Parsing When a fetched document is parsed some outlink URLs are relative For example: http://en.wikipedia.org/wiki/Main_Page has a link to “/wiki/Special:Statistics” which is the same as http://en.wikipedia.org/wiki/Special:Statistics Parsing involves normalizing (expanding) relative URLs  A Robust Crawl Architecture Robust Crawling  Content Seen? Duplication Duplication is widespread on the web If a page just fetched is already in the index, don’t process it any further This can be done by using document fingerprints/shingles A type of approximate hashing scheme Similar to watermarking, SIFT features, etc.  A Robust Crawl Architecture Robust Crawling  Compliance with webmasters wishes... Filters  Robots.txt Filters is a regular expression for a URL to be excluded How often do you check robots.txt? Cache to avoid using bandwidth and loading web server Sitemaps A mechanism to better manage the URL frontier  A Robust Crawl Architecture Robust Crawling  Duplicate Elimination  For a one-time crawl Test to see if an extracted,parsed, filtered URL has already been sent to the frontier. has already been indexed. For a continuous crawl See full frontier implementation: Update the URL’s priority Based on staleness Based on quality Based on politeness  Distributing the crawl The key goal for the architecture of a distributed crawl is cache locality We want multiple crawl threads in multiple processes at multiple nodes for robustness Geographically distributed for speed Partition the hosts being crawled across nodes Hash typically used for partition How do the nodes communicate?  Robust Crawling The output of the URL Filter at each node is sent to the Duplicate Eliminator at all other nodes  URL Frontier Freshness Crawl some pages more often than others Keep track of change rate of sites Incorporate sitemap info Quality High quality pages should be prioritized Based on link-analysis, popularity, heuristics on content Politeness When was the last time you hit a server?  URL Frontier Freshness, Quality and Politeness These goals will conflict with each other A simple priority queue will fail because links are bursty Many sites have lots of links pointing to themselves creating bursty references Time influences the priority Politeness Challenges Even if only one thread is assigned to hit a particular host it can hit it repeatedly Heuristic : insert a time gap between successive requests  Magnitude of the crawl To fetch 1,000,000,000 pages in one month... a small fraction of the web we need to fetch 400 pages per second ! Since many fetches will be duplicates, unfetchable, filtered, etc. 400 pages per second isn’t fast enough  Introduction URL Frontier Robust Crawling DNS Various parts of architecture URL Frontier Index Distributed Indices Connectivity Servers Overview Web Crawling Outline  Robust Crawling The output of the URL Filter at each node is sent to the Duplicate Eliminator at all other nodes  URL Frontier Implementation - Mercator  URLs flow from top to bottom Front queues manage priority Back queue manage politeness Each queue is FIFO http://research.microsoft.com/~najork/mercator.pdf  URL Frontier Implementation - Mercator Prioritizer takes URLS and assigns a priority Integer between 1 and F Appends URL to appropriate queue Priority Based on rate of change Based on quality (spam) Based on application Front queues  URL Frontier Implementation - Mercator Selection from front queues is initiated from back queues Pick a front queue, how? Round robin Randomly Monte Carlo Biased toward high priority Back queues  URL Frontier Implementation - Mercator Each back queue is non-empty while crawling Each back queue has URLs from one host only Maintain a table of URL to back queues (mapping) to help Back queues  URL Frontier Implementation - Mercator Timing Heap One entry per queue Has earliest time that a host can be hit again Earliest time based on Last access to that host Plus any appropriate heuristic robots.txt “crawl-delay” sitemaps instruction Back queues  URL Frontier Implementation - Mercator A crawler thread needs a URL It gets the timing heap root It gets the next eligible queue based on time, b. It gets a URL from b If b is empty Pull a URL v from front queue If back queue for v exists place it in that queue, repeat. Else add v to b - update heap. Back queues  URL Frontier Implementation - Mercator How many queues? Keep all threads busy ~3 times as many back queues as crawler threads Web-scale issues This won’t fit in memory Solution Keep queues on disk and keep a portion in memory. Back queues"/>
    <title></title>
    <script type="text/javascript" language="javascript">
//      <![CDATA[
            var images = new Array (31);
            images[0] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.001.png";
            images[1] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.002.png";
            images[2] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.003.png";
            images[3] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.004.png";
            images[4] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.005.png";
            images[5] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.006.png";
            images[6] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.007.png";
            images[7] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.008.png";
            images[8] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.009.png";
            images[9] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.010.png";
            images[10] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.011.png";
            images[11] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.012.png";
            images[12] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.013.png";
            images[13] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.014.png";
            images[14] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.015.png";
            images[15] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.016.png";
            images[16] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.017.png";
            images[17] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.018.png";
            images[18] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.019.png";
            images[19] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.020.png";
            images[20] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.021.png";
            images[21] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.022.png";
            images[22] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.023.png";
            images[23] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.024.png";
            images[24] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.025.png";
            images[25] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.026.png";
            images[26] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.027.png";
            images[27] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.028.png";
            images[28] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.029.png";
            images[29] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.030.png";
            images[30] = "Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.031.png";
            var index = 0;
            function WindowLoaded(evt)
            {
                document.body.onselectstart = function () { return false; };
            }
            function Step(i)
            {
                GoTo(index + i)
            }
            function GoTo(newIndex)
            {
                if(newIndex >= 0 && newIndex < images.length)
                {
                    index = newIndex;
                    document.Slideshow.src = images[index];
                }
            }
//      ]]>
    </script>
</head>
<body bgcolor="black" onload='WindowLoaded(event);'>
    <p align="center">
        <br/>
        <br/>
        <img name="Slideshow" alt="" src="Lecture05_Slides_CS221_files/Lecture05_Slides_CS221.001.png" onclick="Step(1)"/>
        <br/>
        <br/>
        <input type="image" src="Lecture05_Slides_CS221_files/home.png" onclick="GoTo(0)"/>
        &nbsp;&nbsp;&nbsp;
        <input type="image" src="Lecture05_Slides_CS221_files/prev.png" onclick="Step(-1)"/>
        <input type="image" src="Lecture05_Slides_CS221_files/next.png" onclick="Step(1)"/>
    </p>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-15170336-1");
pageTracker._trackPageview();
} catch(err) {}</script>

</body>
</html>

