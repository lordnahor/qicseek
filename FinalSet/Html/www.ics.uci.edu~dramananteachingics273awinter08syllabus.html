<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<html> <head>
<title></title>
</head>

<body>
<h1>Syllabus</h1>

This list is very much in flux - and in particular, is overly ambitious.<br> We probably will not get to the last couple of topics, so please check back often.

<ol type = I>
  <li> Linear regression
  <ol>
    <li> LMS algorithm
    <li> Normal equations
    <ol type = 1>
      <li> Matrix derivatives
      <li> Least squares
    </ol>          
    <li> Probablistic motivation
    <li> Locally weighted linear regression
    <ol type = 1>
      <li> Nearest neighbors
      <li> Overfitting
    </ol>
  </ol>
  <li> Classification and logistic regression
  <ol>
    <li> Sigmoid loss function
    <li> Perceptron
    <li> Iteratively weighted least squares
  </ol>
  <li> Generalized linear models
  <ol>
    <li> Exponential family
    <ol>
      <li> Bernoulli
      <li> Gaussian
    </ol>
    <li> Recap with GLM models
    <ol>
      <li> Linear regression
      <li> Logistic regression
      <li> Softmax regression
    </ol>
  </ol>
  <li> Generative models
  <ol>
    <li> Guassian/Quadratic discriminant analysis
    <ol>
      <li> Multi-variate gaussian
      <li> Probablistic model
      <li> Comparison with logistic regression
    </ol>
    <li> Naive Bayes
    <ol>
      <li> Laplace smoothing
    </ol>
  </ol>
  <li> Decision Trees
  <ol>
    <li> Entropy gain
  </ol>
  <li> Subspace methods
  <ol>
    <li> Principle Component Analysis (PCA)
    <ol>
      <li> Singular Value Decomposition (SVD)
    </ol>
    <li> Linear Discriminant Analysis (LDA)
    <li> Canonical Correlation Analysis (CCA)
    <li> Independant Componant Analysis (ICA)
  </ol>
  <li> Neural nets
  <ol>
    <li> Seperating hyperplanes
    <li> Hidden layer models
    <li> Backpropogation
  </ol>
  <li> Support vector machines
  <ol>
    <li> Functional and geometric margins
    <li> Quadratic Program (QP) Primal forumaiton
    <li> Lagrange duality
    <li> Support vectors
    <li> Kernals
    <li> Non-separability
    <li> Sequential Minimal Optimization (SMO)
    <ol>
      <li> Coordinate ascent
      <li> SMO
    </ol>
    <li> Kernalized subspace methods
  </ol>
  <li> Boosting
  <ol>
    <li> Exponential loss function
    <li> Adaboost
    <li> Viola Jones face detection
  </ol>
    
  <li> Learning Theory
  <ol>
    <li> Bias/Variance
    <ol>
      <li> Consistency
    </ol>
    <li> Bounds
    <ol>
      <li> Union bound
      <li> Chernoff bound
    </ol>
    <li> Provably Approximately Correct (PAC) models
    <li> Loss functions
    <li> Empircal versus structural risk
    <li> Sample complexity
    <li> VC dimension
  </ol>
  <li> Regularization and model selection
  <ol>
    <li> Cross validation
    <li> Feature selection
    <li> Bayes statistics for regularization
    <ol>
      <li> Maximum likelihood (ML) versus Maximum a-Posteriori (MAP)
    </ol>
  </ol>
  <li> Structured prediction
  <ol>
    <li> Multi-class generalization
    <li> Vitterbi optimization of markov models
    <li> Margin-based training
    <li> Conditional Random Feilds
  </ol>
  <li> Expectation maximization
  <ol>
    <li> K-means clustering
    <li> Guassian mixture models
    <li> Expected complete log-likelihood
  </ol>
</ol>
  


<hr>
<address></address>
</body> </html>

