<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
<title>Winter 2009: Informatics 141 : Information Retrieval : Assignment 03</title>
<link href="../styleSheet.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div class="container">
  <table>
    <tr>
      <td width="30%"><p class=" style7 ">Informatics   141: Information Retrieval: </p>
        <p class=" style7 ">Assignment 03 </p>
        <p>Winter 2009 </p>
        <p class="style6"><a href="http://www.ics.uci.edu/informatics">Department of Informatics</a></p>
        <p class="style6"><a href="http://www.ics.uci.edu">Donald Bren School of Information and Computer Sciences </a></p>
        <p class="style6"><a href="http://www.uci.edu">University of California, Irvine </a></p></td>
      <td><div id="gsfn_list_widget"> <a href="http://getsatisfaction.com/luci" class="widget_title"> Discussions about this class</a>
  <div id="gsfn_content"><center><img src="../ajax-loader.gif"/></center></div>
        </div>
        <script src="http://getsatisfaction.com/luci/widgets/javascripts/db439d8/widgets.js" type="text/javascript"></script>
        <script src="http://getsatisfaction.com/luci/topics.widget?callback=gsfnTopicsCallback&amp;length=120&amp;limit=10&amp;product=luci_winter_2009_inf_141cs_141&amp;sort=recently_active" type="text/javascript"></script>
      </td>
    </tr>
  </table>
</div>
<div class="container"><a href="../index.html">Home</a> | <a href="../admin.html">Administrative Policies </a> | <a href="../structure.html">Course Structure</a> |<a href="../materials.html"> Resources &amp; Materials</a> | <a href="../calendar.html#CurrentWeek">Calendar </a></div>
<div class="container">
  <p class="somethingDue">Due 2/2/2009</p>
</div>
<div class="container">
  <ol>
    <li><span class="style5"><strong>Goals</strong></span><strong>:
      </strong>
      <ol>
        <li>This assignment is
          designed to:
          <ol>
            <li>To teach you how difficult crawling the web is in terms of scale and temporal requirements. The software itself is not the hard part of this assignment, it is managing all the data. This is only one web site (wikipedia) after all. </li>
            <li>To teach you how difficult it is to process text that is not written for computers to consume. Again this is a very structured domain as far as text goes.</li>
            <li>To make the point that web-scale web-centric activities do not lend themselves to &quot;completeness&quot;. In some sense you are never done. So thinking about web algorithms in terms of &quot;finishing&quot; doesn't make sense. You have to change your mindset to &quot;best possible&quot; given resources. </li>
          </ol>
        </li>
      </ol>
    </li>
    <li><span class="style5"><strong>Administration:</strong></span>
      <ol>
        <li>You may work in teams of 1, 2 or 3. </li>
      </ol>
    </li>
    <li><span class="style5"><strong>Write a Java Program</strong></span><ol><li><span class="style5">Write a program to crawl the web.</span></li>
        <li>Suggested approach
          <ol>
            <li>Use <a href="http://code.google.com/p/crawler4j/">crawler4j</a> as your crawling engine. It is a java library. </li>
            <li>Follow the instructions at http://code.google.com/p/crawler4j/ and create your MyCrawler and Controller classes.</li>
            <li>Remember to set the maximum heap of java to an acceptable value. For example, if your machine has 2GB RAM. You may want to assign 1GB RAM to your crawlers. You can add -Xmx1024M to your java command line parameters.</li>
            <li>Make sure that you dump your partial results in a permanent storage as you crawl. For example, you can write palindromes longer than 20 characters in a file and after finishing the crawl, report 10 longest one. It is always possible that your crawl crashes in the mean time or you pass the deadline. So, it's a good practice to have your partial results in a file.  Also make sure to flush the file whenever you write in it.</li>
            <li>If you're crawling on a remote linux machine like openlab machines, make sure to use the nohup command. Otherwise your crawling which may take several days will be stopped if your connection is disconnected for even a second. Search the web for how to use this command.</li>
            <li>Attend the discussion class. All of the above suggestions and other hints will be covered there.</li>
          </ol>
        </li>
        <li>Inputs
          <ol>
            <li>A URL start Page (the seed set) </li>
            <li>A regular expression
              <ol>
                <li>Pages are only crawled if the url matches this regular expression.</li>
              </ol>
            </li>
          </ol>
        </li>
        <li><span class="style5">Outputs</span>
              <ol>
                <li>Find the 10 longest Palindromes in  english content pages of wikipedia that is not on a page about palindromes. Present them in a table with the source page from which they came. </li>
                <li>Find the10 longest Lipograms (letter &quot;E&quot;/&quot;e&quot;) in  english content pages of wikipedia that is not on a page about lipograms. Present them in a table with the source page from which they came. </li>
                <li>Find the  10 Rhopalics with the most number of words in  english content pages of wikipedia. Present them in a table with the source page from which they came. </li>
                <li>Number of Pages Crawled.</li>
                <li>Number of Links found in these pages.</li>
                <li>Total Size of the Downloaded Content.</li>
                <li>Total Size of the Extracted Text of the Pages.</li>
                <li>What was the docid of the Page with this URL in your crawl: http://en.wikipedia.org/wiki/Barack_Obama</li>
              </ol>
        </li>
        </ol>
    </li>
    <li><strong>Submitting your assignment </strong>              
      <ol>
        <li>We are going to use <a href="http://checkmate.ics.uci.edu">checkmate.ics.uci.edu</a> to submit this assignment.        </li>
        <li>Make the file name &lt;StudentID&gt;-&lt;StudentIID&gt;-&lt;StudentID&gt;-Assignment03.pdf</li>
      </ol>
    </li>
    <li><span class="style5"><strong>Evaluation</strong></span>:
      <ol>
        <li>(90%) Produce the palindrome, lipogram and rhopalic   
          and source URLs from part 3.
          <ol>
            <li>Grades will be assigned according to the length of the sequence compared to the longest known sequence. </li>
          </ol>
        </li>
        <li>(10%) Adhering to administrative requirements</li>
      </ol>
    </li>
    <li><span class="style5"><strong>Train your group </strong></span>    
      <ol>
        <li>Each member of your group must be able to run your architecture <span class="style5">on their own</span> for Assignment 04. </li>
      </ol>
    </li>
  </ol>
</div>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-15170336-1");
pageTracker._trackPageview();
} catch(err) {}</script>

</body>
</html>

